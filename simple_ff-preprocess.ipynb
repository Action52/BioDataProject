{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HwIe3gAKWIy",
    "outputId": "a55c1a5d-f4b2-472d-ebee-f753e1fc5415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iopNSsqKebZ",
    "outputId": "ae0f0a09-7cac-4756-d927-4fc086a89b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Uni/UniPD/BioData/project/biological_data_pfp\n"
     ]
    }
   ],
   "source": [
    "cd 'drive/MyDrive/Uni/UniPD/BioData/project/biological_data_pfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r8NK2V60KguN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 15:37:47.990527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 15:37:47.990581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 15:37:47.991473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 15:37:47.997077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-26 15:37:48.655713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx\n",
    "import hashlib\n",
    "import obonet\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hbjhqs_MKkM6"
   },
   "outputs": [],
   "source": [
    "def readh5_to_dict(file_path):\n",
    "  # Create an empty dictionary to store the data\n",
    "  p_embeddings_data = {}\n",
    "\n",
    "  # Open the HDF5 file\n",
    "  with h5py.File(file_path, 'r') as p_embeddings:\n",
    "    # Store the data in the dictionary\n",
    "    for key in p_embeddings.keys():\n",
    "      p_embeddings_data[key] = p_embeddings[key][...]\n",
    "\n",
    "  return p_embeddings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RjTDFHv5K2j4"
   },
   "outputs": [],
   "source": [
    "def sample_protein_ids(file_path,percentage):\n",
    "\n",
    "  # Read the IDs from the text file\n",
    "  with open(file_path, 'r') as file:\n",
    "    ids = [line.strip() for line in file]\n",
    "\n",
    "  # Calculate the index to get the first 30% of IDs\n",
    "  split_index = int(len(ids) * percentage)\n",
    "\n",
    "  # Select the first 30% of IDs\n",
    "  selected_ids = ids[:split_index]\n",
    "\n",
    "  return selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y1i9yPHqLdUh"
   },
   "outputs": [],
   "source": [
    "def read_tsv(tsv_file_path):\n",
    "  # Read the TSV file into a Pandas DataFrame\n",
    "  df_train_set = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "  # Display the DataFrame\n",
    "  return df_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "62-TWs4OOTCJ"
   },
   "outputs": [],
   "source": [
    "def read_dat(file_path):\n",
    "  column_names = ['Protein_ID', 'IPR_ID', 'description', 'domain','dc1','dc2']\n",
    "  df = pd.read_csv(file_path, delimiter='\\t',names=column_names)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y71QKlmMMHuo"
   },
   "outputs": [],
   "source": [
    "def filter_train_data(df, selected_ids, category):\n",
    "  filtered_df = df[df['Protein_ID'].isin(selected_ids)]\n",
    "  filtered_df = filtered_df[filtered_df['aspect'] == category]\n",
    "\n",
    "  return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-HglxHVZN2dy"
   },
   "outputs": [],
   "source": [
    "def encode_go_terms(train_df):\n",
    "  one_hot_encoding = pd.get_dummies(train_df['GO_term'])\n",
    "\n",
    "  # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "  df_encoded = pd.concat([train_df, one_hot_encoding], axis=1)\n",
    "  df_encoded_grouped = df_encoded.groupby('Protein_ID').sum().reset_index()\n",
    "\n",
    "  return df_encoded_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_go_terms_sparse(train_df, chunk_size=10000):\n",
    "    # Unique GO terms and Protein IDs\n",
    "    go_terms = train_df['GO_term'].unique()\n",
    "    protein_ids = train_df['Protein_ID'].unique()\n",
    "    \n",
    "    # Mapping of GO terms and Protein IDs to integer indices\n",
    "    go_term_to_index = {go_term: i for i, go_term in enumerate(go_terms)}\n",
    "    protein_id_to_index = {protein_id: i for i, protein_id in enumerate(protein_ids)}\n",
    "    \n",
    "    # Initialize a sparse matrix\n",
    "    encoded_matrix = sparse.lil_matrix((len(protein_ids), len(go_terms)), dtype=np.int8)\n",
    "    \n",
    "    # Process in chunks using tqdm for progress bar\n",
    "    total_rows = train_df.shape[0]\n",
    "    for start in tqdm(range(0, total_rows, chunk_size), desc=\"Encoding\", total=total_rows // chunk_size + 1):\n",
    "        end = min(start + chunk_size, total_rows)\n",
    "        chunk = train_df.iloc[start:end]\n",
    "        \n",
    "        rows = chunk['Protein_ID'].map(protein_id_to_index)\n",
    "        cols = chunk['GO_term'].map(go_term_to_index)\n",
    "        data = np.ones(len(chunk), dtype=np.int8)\n",
    "        \n",
    "        # Create a sparse matrix for the chunk\n",
    "        chunk_matrix = sparse.coo_matrix((data, (rows, cols)), shape=encoded_matrix.shape, dtype=np.int8)\n",
    "        \n",
    "        # Add the chunk matrix to the main matrix\n",
    "        encoded_matrix += chunk_matrix\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_encoded = pd.DataFrame.sparse.from_spmatrix(encoded_matrix, index=protein_ids, columns=go_terms)\n",
    "    df_encoded = df_encoded.reset_index()\n",
    "    df_encoded.rename(columns={'index': 'Protein_ID'}, inplace=True)\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VL4CJoenW_G6"
   },
   "outputs": [],
   "source": [
    "def encode_ipr_domain(df_ipr):\n",
    "    df_ipr = df_ipr.drop(columns=['IPR_ID', 'description','dc1','dc2'])\n",
    "    one_hot_encoding = pd.get_dummies(df_ipr['domain'],sparse=True)\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    df_encoded = pd.concat([df_ipr, one_hot_encoding], axis=1)\n",
    "    df_encoded_grouped = df_encoded.groupby('Protein_ID').sum().reset_index()\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0hnJggsuL3XU"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(df, embeddings_dict):\n",
    "  df['embedding'] = df['Protein_ID'].map(embeddings_dict)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mSuC-w8ITSSG"
   },
   "outputs": [],
   "source": [
    "def get_ipr(df_ipr,df_train):\n",
    "   isp_dict = df_ipr.set_index('Protein_ID')['domain'].to_dict()\n",
    "   df_train['ipr'] = df_train['Protein_ID'].map(isp_dict)\n",
    "\n",
    "   return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qcA4jvHAbkQp"
   },
   "outputs": [],
   "source": [
    "def create_y(df):\n",
    "  y = df.to_numpy()\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z2pra8kYb30E"
   },
   "outputs": [],
   "source": [
    "def create_X(df,variables):\n",
    "  X = np.array(df[variables])\n",
    "  X = np.vstack(X)\n",
    "\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_freq(column, freq):\n",
    "    top_freq = []\n",
    "    for col in column:\n",
    "        if col not in freq:\n",
    "            continue\n",
    "        top_freq.append(col)\n",
    "    return top_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_ids(selected_ids, train_ids, percentage):\n",
    "    ids = sample_protein_ids(train_ids, percentage)\n",
    "    new_ids = sample_protein_ids(selected_ids, 1.0)\n",
    "    difference = list(set(ids) - set(new_ids))\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_test_data(test_embeddings_data, test_ids):\n",
    "    X_test = []\n",
    "    for id in test_ids:\n",
    "        X_test.append(test_embeddings_data[id])\n",
    "    X_test = np.array(X_test)\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    precision = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / (K.sum(K.round(K.clip(y_pred, 0, 1))) + K.epsilon())\n",
    "    recall = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / (K.sum(K.round(K.clip(y_true, 0, 1))) + K.epsilon())\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(X, y, model, n_splits):\n",
    "    histories = []\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Iterate over each fold\n",
    "    fold_no = 1\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        # Splitting the data into training and testing sets for this fold\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        # Fit the model\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "        history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "        histories.append(history)\n",
    "        \n",
    "        # Here, you can evaluate the model on the test set, e.g., calculate metrics\n",
    "        results = model.evaluate(X_test, y_test)\n",
    "        print(f\"Test results - Fold {fold_no}: {model.metrics_names[0]} of {results[0]}; {model.metrics_names[1]} of {results[1]*100}%\")\n",
    "    \n",
    "        fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Propagating the probability of the children to the parent, in this case if the parent has several children, we will take the max probabilty of the children\n",
    "def post_processing(y_pred, pred_scolumns, graph):\n",
    "    new_preds = []\n",
    "\n",
    "    for pred in y_pred:\n",
    "        ### Build prediction dict\n",
    "        preds = {k: 0 for k in pred_columns}\n",
    "        new_pred = [0 for i in range(len(pred))]\n",
    "        for i in range(len(pred)):\n",
    "            term = pred_columns[i]\n",
    "            preds[term] = pred[i]\n",
    "\n",
    "        ### Search the probabilty for the parent\n",
    "        pool = set()\n",
    "        for term, prob in preds.items():\n",
    "            for parent, child, key in graph.in_edges(term, keys=True):\n",
    "                if key not in {'is_a', 'part_of'} or parent not in preds:\n",
    "                    continue\n",
    "\n",
    "                probability = max(prob, preds[parent])\n",
    "                preds[parent] = probability\n",
    "                    \n",
    "        ### Build the array for the new preds\n",
    "        for term, prob in preds.items():\n",
    "            idx = pred_columns.index(term)\n",
    "            new_pred[idx] = prob\n",
    "        new_preds.append(new_pred)\n",
    "    return np.array(new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_df(y_pred, test_ids, pred_columns):\n",
    "    # assert that the length of y_pred must be same as test_ids\n",
    "    assert len(y_pred) == len(test_ids)\n",
    "    \n",
    "    # Group by the result and then sort by score id\n",
    "    out = {'id': [], 'term': [], 'score': []}\n",
    "    for i in range(len(y_pred)):\n",
    "        for j in range(len(y_pred[i])):\n",
    "            out['id'].append(test_ids[i])\n",
    "            out['term'].append(pred_columns[j])\n",
    "            out['score'].append(y_pred[i][j])\n",
    "    \n",
    "    out_df = pd.DataFrame(out).reset_index(drop=True)\n",
    "    \n",
    "    out_df = out_df.groupby('id', group_keys=False)\n",
    "    out_df = out_df.apply(lambda x: x.sort_values(by='score', ascending=False))\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    out_df = out_df[out_df['id'].isin(test_ids)]\n",
    "    \n",
    "    # Convert the 'ID' column to a Categorical with the order defined in filter_array\n",
    "    out_df['id'] = pd.Categorical(out_df['id'], categories=test_ids, ordered=True)\n",
    "    \n",
    "    # Sort by the 'ID' column\n",
    "    out_df = out_df.sort_values('id')\n",
    "    out_df['id'] = out_df['id'].astype(str)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, name_path=None):\n",
    "    '''\n",
    "        Train the model.\n",
    "    '''\n",
    "    embedding_size = len(X_train[1]) \n",
    "    num_classes = len(y_train[1])\n",
    "    \n",
    "    final_model = keras.Sequential([\n",
    "        layers.Input(shape=(embedding_size,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_score])\n",
    "    final_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "    if name_path:\n",
    "        final_model.save(f'{name_path}.h5')\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_predictions(bp_path, mf_path, cc_path):\n",
    "    # Read the files into DataFrames\n",
    "    bp_df = pd.read_csv(bp, sep='\\t', header=None, names=['Protein_ID', 'GO_term', 'score'])\n",
    "    mf_df = pd.read_csv(mf, sep='\\t', header=None, names=['Protein_ID', 'GO_term', 'score'])\n",
    "    cc_df = pd.read_csv(cc, sep='\\t', header=None, names=['Protein_ID', 'GO_term', 'score'])\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    concatenated_df = pd.concat([bp_df, mf_df, cc_df])\n",
    "    \n",
    "    # Create a custom sorting order based on the external list\n",
    "    sorting_order = {id: index for index, id in enumerate(test_ids)}\n",
    "    concatenated_df['sort_order'] = concatenated_df['Protein_ID'].map(sorting_order)\n",
    "    \n",
    "    # Sort by custom order and then by probability within each group\n",
    "    sorted_df = concatenated_df.sort_values(by=['sort_order', 'score'], ascending=[True, False])\n",
    "    \n",
    "    # Limit to 1500 rows per ID\n",
    "    limited_df = sorted_df.groupby('Protein_ID').head(1500)\n",
    "    \n",
    "    # Drop the auxiliary 'sort_order' column\n",
    "    limited_df = limited_df.drop(columns=['sort_order'])\n",
    "    \n",
    "    return limited_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set_all = read_tsv('./dataset/train/train_set.tsv')\n",
    "# Desired aspects\n",
    "desired_aspects = {'cellular_component', 'biological_process', 'molecular_function'}\n",
    "\n",
    "# Function to check if all aspects are present\n",
    "def check_aspects(group):\n",
    "    return desired_aspects == set(group['aspect'])\n",
    "\n",
    "# Apply the function to each group\n",
    "result = df_train_set_all.groupby('Protein_ID').filter(check_aspects)['Protein_ID'].unique()\n",
    "np.savetxt('./dataset/train/sampled_train.txt', result, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset containing 1000 data of the proteins in the result, this is created to help validate our modell using cafa evaluator\n",
    "selected_test_ids = np.array(sample_protein_ids('./dataset/train/sampled_train.txt', 0.03140))\n",
    "np.savetxt('./dataset/test/sampled_test.txt', selected_test_ids, fmt='%s')\n",
    "\n",
    "# Create ground truth file\n",
    "ground_truth_df = df_train_set_all[df_train_set_all['Protein_ID'].isin(selected_test_ids)]\n",
    "ground_truth_df = ground_truth_df[['Protein_ID', 'GO_term']]\n",
    "ground_truth_df.to_csv('./dataset/test/sampled_gt.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_embeddings_data = readh5_to_dict('./dataset/train/train_embeddings.h5')\n",
    "test_embeddings_data = readh5_to_dict('./dataset/test/test_embeddings.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 28.5 ms, total: 4.08 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph = obonet.read_obo('./dataset/taxonomy/go-basic.obo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Celullar component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "uxHe-ugrM2m1"
   },
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "selected_ids = create_training_ids('./dataset/test/sampled_test.txt', './dataset/train/train_ids.txt', 1.0)\n",
    "df_train_set = filter_train_data(df_train_set_all, selected_ids,'cellular_component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_ids = sample_protein_ids('./dataset/test/sampled_test.txt', 1.0)\n",
    "X_test_gt = build_test_data(p_embeddings_data, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "d0Y7tqbPNgU2",
    "outputId": "680b45a5-a649-454f-a1df-a8ba2f6702e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|███████████████████████████████| 110/110 [01:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein_ID</th>\n",
       "      <th>GO:0005575</th>\n",
       "      <th>GO:0110165</th>\n",
       "      <th>GO:0005622</th>\n",
       "      <th>GO:0043226</th>\n",
       "      <th>GO:0030139</th>\n",
       "      <th>GO:0097708</th>\n",
       "      <th>GO:0005737</th>\n",
       "      <th>GO:0045335</th>\n",
       "      <th>GO:0031982</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0005637</th>\n",
       "      <th>GO:0099092</th>\n",
       "      <th>GO:0099091</th>\n",
       "      <th>GO:0030140</th>\n",
       "      <th>GO:0005703</th>\n",
       "      <th>GO:0097386</th>\n",
       "      <th>GO:0000235</th>\n",
       "      <th>GO:0070822</th>\n",
       "      <th>GO:0010287</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q55DL5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1418, 0.06207, 0.07367, -0.0712, 0.0703, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O81027</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0491, 0.0389, -0.0178, 0.02779, -0.00568, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q04418</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.022, -0.06964, -0.007042, 0.0544, -0.04633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q7ZT12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.04028, -0.03357, 0.1046, 0.0669, -0.07935, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q07627</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.013565, 0.1422, 0.1249, 0.05283, 0.00569, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Protein_ID  GO:0005575  GO:0110165  GO:0005622  GO:0043226  GO:0030139  \\\n",
       "0     Q55DL5           1           1           1           1           1   \n",
       "1     O81027           1           1           1           1           0   \n",
       "2     Q04418           1           1           1           0           0   \n",
       "3     Q7ZT12           1           1           0           0           0   \n",
       "4     Q07627           1           1           1           0           0   \n",
       "\n",
       "   GO:0097708  GO:0005737  GO:0045335  GO:0031982  ...  GO:0005637  \\\n",
       "0           1           1           1           1  ...           0   \n",
       "1           0           1           0           0  ...           0   \n",
       "2           0           1           0           0  ...           0   \n",
       "3           0           0           0           0  ...           0   \n",
       "4           0           1           0           0  ...           0   \n",
       "\n",
       "   GO:0099092  GO:0099091  GO:0030140  GO:0005703  GO:0097386  GO:0000235  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   GO:0070822  GO:0010287                                          embedding  \n",
       "0           0           0  [0.1418, 0.06207, 0.07367, -0.0712, 0.0703, -0...  \n",
       "1           0           0  [0.0491, 0.0389, -0.0178, 0.02779, -0.00568, 0...  \n",
       "2           0           0  [-0.022, -0.06964, -0.007042, 0.0544, -0.04633...  \n",
       "3           0           0  [0.04028, -0.03357, 0.1046, 0.0669, -0.07935, ...  \n",
       "4           0           0  [0.013565, 0.1422, 0.1249, 0.05283, 0.00569, -...  \n",
       "\n",
       "[5 rows x 680 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = encode_go_terms_sparse(df_train_set)\n",
    "df_encoded = get_embeddings(df_encoded, p_embeddings_data)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKNV2HygPBYy",
    "outputId": "67a2bf91-619a-41c0-b74e-7a0ddac93a29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15083/2827862180.py:1: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  df_encoded.isna().sum().sum()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N labels, for celullar component we select top 300\n",
    "freq_df = pd.read_csv('./dataset/train/cellular_component_freq.csv')[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "Ru3RD7uTbfVi"
   },
   "outputs": [],
   "source": [
    "y_columns = df_encoded.iloc[:, 1:-1]\n",
    "pred_columns = get_top_freq(y_columns.columns.tolist(), set(freq_df['id']))\n",
    "\n",
    "y_columns = y_columns[pred_columns]\n",
    "y = create_y(y_columns)\n",
    "X = create_X(df_encoded,'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_B42EKPecoCD",
    "outputId": "d617883a-e559-4423-e96d-3c8fe35b9a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0882 - f1_score: 0.6166\n",
      "Epoch 2/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0712 - f1_score: 0.6567\n",
      "Epoch 3/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0684 - f1_score: 0.6682\n",
      "Epoch 4/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0667 - f1_score: 0.6755\n",
      "Epoch 5/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0652 - f1_score: 0.6814\n",
      "Epoch 6/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0640 - f1_score: 0.6867\n",
      "Epoch 7/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0631 - f1_score: 0.6909\n",
      "Epoch 8/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0621 - f1_score: 0.6958\n",
      "Epoch 9/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0613 - f1_score: 0.7000\n",
      "Epoch 10/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0605 - f1_score: 0.7031\n",
      "Epoch 11/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0598 - f1_score: 0.7067\n",
      "Epoch 12/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0592 - f1_score: 0.7096\n",
      "Epoch 13/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0586 - f1_score: 0.7125\n",
      "Epoch 14/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0580 - f1_score: 0.7153\n",
      "Epoch 15/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0575 - f1_score: 0.7178\n",
      "Epoch 16/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0571 - f1_score: 0.7202\n",
      "Epoch 17/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0566 - f1_score: 0.7227\n",
      "Epoch 18/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0562 - f1_score: 0.7248\n",
      "Epoch 19/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0558 - f1_score: 0.7266\n",
      "Epoch 20/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0554 - f1_score: 0.7285\n",
      "Epoch 21/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0550 - f1_score: 0.7307\n",
      "Epoch 22/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0546 - f1_score: 0.7323\n",
      "Epoch 23/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0543 - f1_score: 0.7342\n",
      "Epoch 24/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0540 - f1_score: 0.7355\n",
      "Epoch 25/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0536 - f1_score: 0.7375\n",
      "Epoch 26/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0533 - f1_score: 0.7389\n",
      "Epoch 27/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0530 - f1_score: 0.7408\n",
      "Epoch 28/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0527 - f1_score: 0.7422\n",
      "Epoch 29/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0525 - f1_score: 0.7437\n",
      "Epoch 30/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0522 - f1_score: 0.7444\n",
      "Epoch 31/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0519 - f1_score: 0.7462\n",
      "Epoch 32/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0517 - f1_score: 0.7473\n",
      "Epoch 33/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0515 - f1_score: 0.7482\n",
      "Epoch 34/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0513 - f1_score: 0.7501\n",
      "Epoch 35/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0510 - f1_score: 0.7512\n",
      "Epoch 36/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0508 - f1_score: 0.7522\n",
      "Epoch 37/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0506 - f1_score: 0.7532\n",
      "Epoch 38/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0504 - f1_score: 0.7539\n",
      "Epoch 39/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0502 - f1_score: 0.7550\n",
      "Epoch 40/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0500 - f1_score: 0.7557\n",
      "Epoch 41/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0498 - f1_score: 0.7575\n",
      "Epoch 42/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0497 - f1_score: 0.7578\n",
      "Epoch 43/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0495 - f1_score: 0.7587\n",
      "Epoch 44/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0493 - f1_score: 0.7603\n",
      "Epoch 45/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0491 - f1_score: 0.7607\n",
      "Epoch 46/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0490 - f1_score: 0.7613\n",
      "Epoch 47/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0488 - f1_score: 0.7624\n",
      "Epoch 48/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0486 - f1_score: 0.7636\n",
      "Epoch 49/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0485 - f1_score: 0.7636\n",
      "Epoch 50/50\n",
      "2222/2222 [==============================] - 3s 1ms/step - loss: 0.0483 - f1_score: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "model = train(X_train, y_train, './model/cc_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnDbWCBVcv7X",
    "outputId": "9f95b107-7edf-49fa-8576-4e48c7d55549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393/393 [==============================] - 0s 559us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing - Cellular Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_y_pred = post_processing(y_pred, pred_columns, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Cellular Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     12546\n",
      "           1       0.99      1.00      1.00     12438\n",
      "           2       0.89      0.96      0.92      9942\n",
      "           3       0.81      0.90      0.85      8531\n",
      "           4       0.41      0.13      0.20       138\n",
      "           5       0.49      0.28      0.36       768\n",
      "           6       0.76      0.86      0.81      7405\n",
      "           7       0.39      0.11      0.18        96\n",
      "           8       0.51      0.22      0.31      1045\n",
      "           9       0.77      0.86      0.81      7343\n",
      "          10       0.79      0.86      0.82      7814\n",
      "          11       0.80      0.90      0.85      8154\n",
      "          12       0.49      0.28      0.35       766\n",
      "          13       0.83      0.60      0.69      1329\n",
      "          14       0.76      0.36      0.49       968\n",
      "          15       0.73      0.32      0.44       728\n",
      "          16       0.58      0.38      0.46      2544\n",
      "          17       0.69      0.22      0.33       116\n",
      "          18       0.69      0.22      0.33       116\n",
      "          19       0.49      0.22      0.30       587\n",
      "          20       0.49      0.26      0.34      1084\n",
      "          21       0.73      0.60      0.66      3538\n",
      "          22       0.64      0.45      0.53       769\n",
      "          23       0.59      0.52      0.55      1824\n",
      "          24       0.77      0.73      0.75      3986\n",
      "          25       0.58      0.57      0.58      2218\n",
      "          26       0.60      0.56      0.58      1486\n",
      "          27       0.58      0.57      0.58      2218\n",
      "          28       0.61      0.60      0.60      1894\n",
      "          29       0.58      0.57      0.58      2218\n",
      "          30       0.64      0.46      0.54      2237\n",
      "          31       0.33      0.02      0.03        65\n",
      "          32       0.65      0.46      0.54      2235\n",
      "          33       0.66      0.35      0.46       792\n",
      "          34       0.65      0.29      0.40       491\n",
      "          35       0.48      0.11      0.17       133\n",
      "          36       0.63      0.33      0.43      2392\n",
      "          37       0.55      0.35      0.42        49\n",
      "          38       0.68      0.44      0.53       389\n",
      "          39       0.61      0.33      0.43        83\n",
      "          40       0.66      0.25      0.36       592\n",
      "          41       0.61      0.26      0.37       834\n",
      "          42       0.55      0.24      0.34        45\n",
      "          43       0.52      0.24      0.33       144\n",
      "          44       0.56      0.26      0.36       137\n",
      "          45       0.75      0.60      0.67      2902\n",
      "          46       0.51      0.16      0.24       190\n",
      "          47       0.76      0.57      0.65      2371\n",
      "          48       0.50      0.23      0.31       461\n",
      "          49       0.32      0.13      0.19       226\n",
      "          50       0.31      0.12      0.18       235\n",
      "          51       0.47      0.20      0.28        40\n",
      "          52       0.48      0.17      0.25        84\n",
      "          53       0.62      0.31      0.41       602\n",
      "          54       0.84      0.61      0.71       442\n",
      "          55       0.63      0.26      0.36       438\n",
      "          56       0.61      0.31      0.41        36\n",
      "          57       0.61      0.33      0.43       678\n",
      "          58       0.51      0.21      0.29       208\n",
      "          59       0.59      0.31      0.41       617\n",
      "          60       0.53      0.25      0.34       297\n",
      "          61       0.50      0.25      0.33       276\n",
      "          62       0.49      0.22      0.31       219\n",
      "          63       0.73      0.08      0.14       105\n",
      "          64       0.62      0.06      0.11       215\n",
      "          65       0.43      0.28      0.34       108\n",
      "          66       0.59      0.33      0.42       839\n",
      "          67       0.50      0.40      0.44        68\n",
      "          68       0.56      0.22      0.32       212\n",
      "          69       0.56      0.24      0.34       450\n",
      "          70       0.42      0.16      0.24       250\n",
      "          71       0.51      0.39      0.44        72\n",
      "          72       0.84      0.64      0.73       295\n",
      "          73       0.64      0.42      0.50        84\n",
      "          74       0.60      0.49      0.54       114\n",
      "          75       0.58      0.36      0.44       469\n",
      "          76       0.69      0.28      0.40       236\n",
      "          77       0.00      0.00      0.00        36\n",
      "          78       0.47      0.15      0.23       122\n",
      "          79       0.48      0.26      0.34       447\n",
      "          80       0.64      0.13      0.21        55\n",
      "          81       0.52      0.36      0.42       210\n",
      "          82       0.55      0.27      0.36       382\n",
      "          83       0.54      0.25      0.34       301\n",
      "          84       0.30      0.17      0.22       124\n",
      "          85       0.30      0.17      0.22       124\n",
      "          86       0.73      0.66      0.70       119\n",
      "          87       0.71      0.57      0.64       209\n",
      "          88       0.57      0.06      0.11       132\n",
      "          89       0.67      0.09      0.15       185\n",
      "          90       0.71      0.15      0.25       443\n",
      "          91       0.61      0.31      0.41        65\n",
      "          92       0.50      0.28      0.36        86\n",
      "          93       0.52      0.19      0.28        85\n",
      "          94       0.59      0.28      0.38        67\n",
      "          95       0.53      0.19      0.28        43\n",
      "          96       0.61      0.23      0.34        60\n",
      "          97       0.56      0.29      0.38       190\n",
      "          98       0.52      0.27      0.35        52\n",
      "          99       0.63      0.26      0.37       209\n",
      "         100       0.37      0.12      0.18        82\n",
      "         101       0.48      0.27      0.35       362\n",
      "         102       0.50      0.12      0.19        52\n",
      "         103       0.42      0.11      0.18        45\n",
      "         104       0.68      0.16      0.26       120\n",
      "         105       0.54      0.09      0.16       150\n",
      "         106       0.41      0.27      0.32       260\n",
      "         107       0.45      0.21      0.28       169\n",
      "         108       0.56      0.12      0.20       150\n",
      "         109       0.62      0.23      0.34       415\n",
      "         110       0.41      0.34      0.37        56\n",
      "         111       0.55      0.34      0.42       344\n",
      "         112       0.36      0.20      0.26        91\n",
      "         113       0.31      0.08      0.12        65\n",
      "         114       0.33      0.08      0.12        65\n",
      "         115       0.00      0.00      0.00        34\n",
      "         116       0.62      0.32      0.42       441\n",
      "         117       0.60      0.22      0.32       980\n",
      "         118       0.59      0.22      0.33       970\n",
      "         119       0.56      0.38      0.45        48\n",
      "         120       0.65      0.37      0.47       586\n",
      "         121       0.57      0.38      0.46        60\n",
      "         122       0.39      0.25      0.31       178\n",
      "         123       0.38      0.17      0.23       248\n",
      "         124       0.40      0.22      0.29       187\n",
      "         125       0.80      0.20      0.32       183\n",
      "         126       0.49      0.23      0.32       172\n",
      "         127       0.82      0.72      0.77        57\n",
      "         128       0.74      0.54      0.63        79\n",
      "         129       0.80      0.72      0.76        50\n",
      "         130       0.47      0.18      0.26       102\n",
      "         131       0.53      0.27      0.36       441\n",
      "         132       0.51      0.20      0.29        96\n",
      "         133       0.40      0.42      0.41        99\n",
      "         134       0.56      0.28      0.37       176\n",
      "         135       0.66      0.29      0.41       339\n",
      "         136       0.70      0.29      0.41       388\n",
      "         137       0.27      0.07      0.11        44\n",
      "         138       0.77      0.39      0.52        51\n",
      "         139       0.55      0.17      0.27        63\n",
      "         140       0.59      0.21      0.31        48\n",
      "         141       0.20      0.01      0.02       108\n",
      "         142       0.50      0.03      0.05        72\n",
      "         143       0.46      0.31      0.37        75\n",
      "         144       0.38      0.08      0.13        77\n",
      "         145       0.57      0.20      0.30       286\n",
      "         146       0.25      0.02      0.04        41\n",
      "         147       0.70      0.74      0.72        61\n",
      "         148       0.60      0.55      0.58        38\n",
      "         149       0.57      0.20      0.30       229\n",
      "         150       0.46      0.19      0.27       130\n",
      "         151       0.47      0.16      0.24        86\n",
      "         152       0.33      0.09      0.14        54\n",
      "         153       0.61      0.20      0.30       226\n",
      "         154       0.22      0.03      0.05       158\n",
      "         155       0.63      0.50      0.55       109\n",
      "         156       0.64      0.50      0.56       110\n",
      "         157       0.39      0.11      0.17       305\n",
      "         158       0.30      0.03      0.06        97\n",
      "         159       0.27      0.04      0.07        70\n",
      "         160       0.61      0.46      0.53        65\n",
      "         161       0.68      0.43      0.52        89\n",
      "         162       0.62      0.46      0.53        65\n",
      "         163       0.49      0.37      0.42        46\n",
      "         164       0.42      0.25      0.32       119\n",
      "         165       0.51      0.33      0.40        60\n",
      "         166       0.47      0.37      0.41        46\n",
      "         167       0.50      0.36      0.42       150\n",
      "         168       0.53      0.39      0.45        54\n",
      "         169       0.61      0.23      0.33        88\n",
      "         170       0.87      0.25      0.39        52\n",
      "         171       0.81      0.42      0.56        40\n",
      "         172       0.34      0.31      0.32        90\n",
      "         173       0.30      0.20      0.24        44\n",
      "         174       0.34      0.30      0.32        94\n",
      "         175       0.46      0.14      0.21        96\n",
      "         176       0.65      0.18      0.28        62\n",
      "         177       0.88      0.27      0.42        51\n",
      "         178       0.53      0.21      0.30        77\n",
      "         179       0.63      0.15      0.25       170\n",
      "         180       0.77      0.09      0.17       107\n",
      "         181       0.81      0.15      0.26        84\n",
      "         182       0.78      0.14      0.24        98\n",
      "         183       0.42      0.25      0.31        85\n",
      "         184       0.74      0.69      0.71       178\n",
      "         185       0.66      0.51      0.57        91\n",
      "         186       0.66      0.71      0.69        49\n",
      "         187       0.00      0.00      0.00       160\n",
      "         188       0.53      0.34      0.42       238\n",
      "         189       0.50      0.34      0.41       250\n",
      "         190       0.52      0.34      0.41       248\n",
      "         191       0.00      0.00      0.00        42\n",
      "         192       0.57      0.43      0.49       114\n",
      "         193       0.56      0.33      0.41       270\n",
      "         194       0.57      0.31      0.40       267\n",
      "         195       0.61      0.40      0.48        77\n",
      "         196       0.58      0.42      0.49        93\n",
      "         197       0.52      0.05      0.10       262\n",
      "         198       0.33      0.02      0.04       151\n",
      "         199       0.51      0.12      0.19       403\n",
      "         200       0.40      0.03      0.05       151\n",
      "         201       0.67      0.54      0.60       111\n",
      "         202       0.70      0.40      0.50        81\n",
      "         203       0.65      0.31      0.42        65\n",
      "         204       0.67      0.54      0.60       112\n",
      "         205       0.62      0.33      0.43        46\n",
      "         206       0.70      0.41      0.52        81\n",
      "         207       0.67      0.52      0.58       110\n",
      "         208       0.81      0.14      0.25        90\n",
      "         209       0.44      0.15      0.23        79\n",
      "         210       0.44      0.16      0.24        74\n",
      "         211       0.31      0.03      0.06       307\n",
      "         212       0.30      0.03      0.06       337\n",
      "         213       0.32      0.03      0.06       322\n",
      "         214       0.32      0.04      0.06       337\n",
      "         215       0.67      0.21      0.32        38\n",
      "         216       0.25      0.02      0.04        45\n",
      "         217       0.25      0.02      0.04        45\n",
      "         218       0.25      0.02      0.04        45\n",
      "         219       0.44      0.18      0.26        60\n",
      "         220       0.60      0.06      0.11       252\n",
      "         221       0.43      0.07      0.13        81\n",
      "         222       0.70      0.56      0.62        71\n",
      "         223       0.70      0.56      0.62        71\n",
      "         224       0.56      0.19      0.28       159\n",
      "         225       0.47      0.18      0.26       113\n",
      "         226       0.64      0.11      0.19       126\n",
      "         227       0.52      0.19      0.28       125\n",
      "         228       0.85      0.28      0.42        40\n",
      "         229       0.50      0.03      0.06        34\n",
      "         230       0.33      0.02      0.05       161\n",
      "         231       0.50      0.03      0.06        32\n",
      "         232       0.77      0.12      0.20        85\n",
      "         233       0.70      0.09      0.16        79\n",
      "         234       1.00      0.10      0.18        41\n",
      "         235       0.65      0.46      0.54       123\n",
      "         236       0.53      0.31      0.39        32\n",
      "         237       0.64      0.38      0.48        65\n",
      "         238       0.71      0.34      0.47        58\n",
      "         239       0.62      0.23      0.33        88\n",
      "         240       0.43      0.02      0.03       184\n",
      "         241       0.60      0.03      0.05       212\n",
      "         242       0.78      0.13      0.23       105\n",
      "         243       0.35      0.21      0.26        57\n",
      "         244       0.00      0.00      0.00        64\n",
      "         245       0.46      0.20      0.28       147\n",
      "         246       0.46      0.19      0.27       101\n",
      "         247       0.54      0.21      0.31        70\n",
      "         248       0.58      0.21      0.31        66\n",
      "         249       0.62      0.32      0.42        57\n",
      "         250       0.62      0.38      0.47        48\n",
      "         251       0.60      0.32      0.41        57\n",
      "         252       0.20      0.02      0.03        57\n",
      "         253       0.20      0.02      0.04        52\n",
      "         254       0.20      0.02      0.04        50\n",
      "         255       0.59      0.31      0.41        42\n",
      "         256       0.64      0.49      0.55        70\n",
      "         257       0.61      0.48      0.53        42\n",
      "         258       0.62      0.44      0.52        45\n",
      "         259       0.36      0.17      0.23        78\n",
      "         260       0.58      0.31      0.40       135\n",
      "         261       0.62      0.49      0.55        69\n",
      "         262       0.64      0.50      0.56        60\n",
      "         263       0.50      0.21      0.30        38\n",
      "         264       0.73      0.45      0.56        49\n",
      "         265       0.58      0.10      0.17       112\n",
      "         266       0.50      0.16      0.24        51\n",
      "         267       0.83      0.09      0.16        56\n",
      "         268       0.57      0.10      0.17        39\n",
      "         269       0.67      0.09      0.16        68\n",
      "         270       0.62      0.04      0.08       125\n",
      "         271       0.67      0.08      0.15        71\n",
      "         272       0.44      0.19      0.26        37\n",
      "         273       0.55      0.13      0.22       126\n",
      "         274       1.00      0.02      0.03        60\n",
      "         275       0.53      0.24      0.33        42\n",
      "         276       0.56      0.24      0.33        38\n",
      "         277       0.50      0.03      0.05       146\n",
      "         278       0.33      0.05      0.08        63\n",
      "         279       0.70      0.18      0.29        39\n",
      "         280       0.50      0.08      0.13        39\n",
      "         281       0.00      0.00      0.00        76\n",
      "         282       0.71      0.29      0.42        51\n",
      "         283       0.20      0.03      0.06        31\n",
      "         284       0.77      0.55      0.64        55\n",
      "         285       0.70      0.44      0.54        88\n",
      "         286       0.00      0.00      0.00        48\n",
      "         287       0.20      0.01      0.03        74\n",
      "         288       0.00      0.00      0.00        67\n",
      "         289       0.00      0.00      0.00        62\n",
      "         290       0.00      0.00      0.00        57\n",
      "         291       0.60      0.06      0.11        48\n",
      "         292       0.00      0.00      0.00        61\n",
      "         293       1.00      0.02      0.04        46\n",
      "         294       0.50      0.02      0.05        42\n",
      "         295       0.50      0.02      0.04        50\n",
      "         296       0.56      0.13      0.21        39\n",
      "         297       0.69      0.15      0.25        59\n",
      "         298       0.78      0.09      0.17        75\n",
      "         299       0.78      0.09      0.17        74\n",
      "\n",
      "   micro avg       0.77      0.64      0.70    157057\n",
      "   macro avg       0.55      0.27      0.33    157057\n",
      "weighted avg       0.73      0.64      0.66    157057\n",
      " samples avg       0.79      0.70      0.71    157057\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Convert probabilities to binary predictions\n",
    "# new_y_pred_binary = (new_y_pred > 0.5).astype(int)\n",
    "# print(classification_report(y_test, new_y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 567us/step\n"
     ]
    }
   ],
   "source": [
    "### Prediction using sample test\n",
    "y_pred_gt = model.predict(X_test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = generate_submission_df(y_pred_gt, test_ids, pred_columns)\n",
    "submission_df.to_csv('./dataset/prediction/sample_prediction_cc.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_y_pred_gt = post_processing(y_pred_gt, pred_columns, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df = generate_submission_df(new_y_pred_gt, test_ids, pred_columns)\n",
    "# submission_df.to_csv('./dataset/prediction/sample_prediction_cc_propagate.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Dataset - Molecular Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "selected_ids = create_training_ids('./dataset/test/sampled_test.txt', './dataset/train/train_ids.txt', 1.0)\n",
    "df_train_set = filter_train_data(df_train_set_all, selected_ids, 'molecular_function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_ids = sample_protein_ids('./dataset/test/sampled_test.txt', 1.0)\n",
    "X_test_gt = build_test_data(p_embeddings_data, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|█████████████████████████████████| 53/53 [00:18<00:00,  2.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein_ID</th>\n",
       "      <th>GO:0016830</th>\n",
       "      <th>GO:0016829</th>\n",
       "      <th>GO:0016833</th>\n",
       "      <th>GO:0003824</th>\n",
       "      <th>GO:0003674</th>\n",
       "      <th>GO:0005488</th>\n",
       "      <th>GO:0005515</th>\n",
       "      <th>GO:0003676</th>\n",
       "      <th>GO:0003690</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0001091</th>\n",
       "      <th>GO:0042287</th>\n",
       "      <th>GO:0050897</th>\n",
       "      <th>GO:0015923</th>\n",
       "      <th>GO:0016251</th>\n",
       "      <th>GO:0004559</th>\n",
       "      <th>GO:0030515</th>\n",
       "      <th>GO:0016863</th>\n",
       "      <th>GO:0005337</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O81027</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0491, 0.0389, -0.0178, 0.02779, -0.00568, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8IXT2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.02515, -0.01331, 0.00575, 0.004353, -0.069...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9WUC4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.06134, -0.00452, 0.01472, 0.001324, 0.03162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q6P6T4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.02074, 0.09515, 0.0519, 0.00766, -0.02692, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P04014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01222, -0.0453, 0.0269, -0.00953, -0.01057,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 841 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Protein_ID  GO:0016830  GO:0016829  GO:0016833  GO:0003824  GO:0003674  \\\n",
       "0     O81027           1           1           1           1           1   \n",
       "1     Q8IXT2           0           0           0           0           1   \n",
       "2     Q9WUC4           0           0           0           0           1   \n",
       "3     Q6P6T4           0           0           0           0           1   \n",
       "4     P04014           0           0           0           1           1   \n",
       "\n",
       "   GO:0005488  GO:0005515  GO:0003676  GO:0003690  ...  GO:0001091  \\\n",
       "0           1           1           0           0  ...           0   \n",
       "1           1           0           1           1  ...           0   \n",
       "2           1           1           0           0  ...           0   \n",
       "3           1           1           0           0  ...           0   \n",
       "4           1           1           1           0  ...           0   \n",
       "\n",
       "   GO:0042287  GO:0050897  GO:0015923  GO:0016251  GO:0004559  GO:0030515  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   GO:0016863  GO:0005337                                          embedding  \n",
       "0           0           0  [0.0491, 0.0389, -0.0178, 0.02779, -0.00568, 0...  \n",
       "1           0           0  [-0.02515, -0.01331, 0.00575, 0.004353, -0.069...  \n",
       "2           0           0  [0.06134, -0.00452, 0.01472, 0.001324, 0.03162...  \n",
       "3           0           0  [0.02074, 0.09515, 0.0519, 0.00766, -0.02692, ...  \n",
       "4           0           0  [0.01222, -0.0453, 0.0269, -0.00953, -0.01057,...  \n",
       "\n",
       "[5 rows x 841 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = encode_go_terms_sparse(df_train_set)\n",
    "df_encoded = get_embeddings(df_encoded, p_embeddings_data)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N labels, for molecular function we pick top 450\n",
    "freq_df = pd.read_csv('./dataset/train/molecular_function_freq.csv')[:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = df_encoded.iloc[:, 1:-1]\n",
    "pred_columns = get_top_freq(y_columns.columns.tolist(), set(freq_df['id']))\n",
    "y_columns = y_columns[pred_columns]\n",
    "y = create_y(y_columns)\n",
    "X = create_X(df_encoded,'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0671 - f1_score: 0.4472\n",
      "Epoch 2/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0448 - f1_score: 0.5543\n",
      "Epoch 3/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0408 - f1_score: 0.5954\n",
      "Epoch 4/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0385 - f1_score: 0.6212\n",
      "Epoch 5/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0368 - f1_score: 0.6390\n",
      "Epoch 6/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0355 - f1_score: 0.6523\n",
      "Epoch 7/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0345 - f1_score: 0.6624\n",
      "Epoch 8/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0337 - f1_score: 0.6717\n",
      "Epoch 9/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0330 - f1_score: 0.6779\n",
      "Epoch 10/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0323 - f1_score: 0.6843\n",
      "Epoch 11/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0317 - f1_score: 0.6902\n",
      "Epoch 12/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0312 - f1_score: 0.6960\n",
      "Epoch 13/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0307 - f1_score: 0.7014\n",
      "Epoch 14/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0303 - f1_score: 0.7046\n",
      "Epoch 15/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0298 - f1_score: 0.7096\n",
      "Epoch 16/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0295 - f1_score: 0.7138\n",
      "Epoch 17/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0291 - f1_score: 0.7169\n",
      "Epoch 18/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0287 - f1_score: 0.7208\n",
      "Epoch 19/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0284 - f1_score: 0.7239\n",
      "Epoch 20/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0281 - f1_score: 0.7265\n",
      "Epoch 21/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0278 - f1_score: 0.7295\n",
      "Epoch 22/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0275 - f1_score: 0.7323\n",
      "Epoch 23/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0273 - f1_score: 0.7356\n",
      "Epoch 24/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0270 - f1_score: 0.7374\n",
      "Epoch 25/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0268 - f1_score: 0.7395\n",
      "Epoch 26/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0265 - f1_score: 0.7422\n",
      "Epoch 27/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0263 - f1_score: 0.7446\n",
      "Epoch 28/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0261 - f1_score: 0.7468\n",
      "Epoch 29/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0259 - f1_score: 0.7486\n",
      "Epoch 30/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0257 - f1_score: 0.7505\n",
      "Epoch 31/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0255 - f1_score: 0.7531\n",
      "Epoch 32/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0253 - f1_score: 0.7547\n",
      "Epoch 33/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0251 - f1_score: 0.7564\n",
      "Epoch 34/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0250 - f1_score: 0.7580\n",
      "Epoch 35/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0248 - f1_score: 0.7601\n",
      "Epoch 36/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0246 - f1_score: 0.7613\n",
      "Epoch 37/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0245 - f1_score: 0.7628\n",
      "Epoch 38/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0243 - f1_score: 0.7644\n",
      "Epoch 39/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0242 - f1_score: 0.7666\n",
      "Epoch 40/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0240 - f1_score: 0.7673\n",
      "Epoch 41/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0239 - f1_score: 0.7686\n",
      "Epoch 42/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0237 - f1_score: 0.7704\n",
      "Epoch 43/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0236 - f1_score: 0.7719\n",
      "Epoch 44/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0235 - f1_score: 0.7728\n",
      "Epoch 45/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0233 - f1_score: 0.7745\n",
      "Epoch 46/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0232 - f1_score: 0.7749\n",
      "Epoch 47/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0231 - f1_score: 0.7765\n",
      "Epoch 48/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0230 - f1_score: 0.7775\n",
      "Epoch 49/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0229 - f1_score: 0.7781\n",
      "Epoch 50/50\n",
      "1453/1453 [==============================] - 2s 1ms/step - loss: 0.0228 - f1_score: 0.7796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "model = train(X_train, y_train, './model/mf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 575us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Molecular Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.46      0.57        70\n",
      "           1       0.84      0.60      0.70       250\n",
      "           2       0.87      0.92      0.90      3627\n",
      "           3       1.00      1.00      1.00      8205\n",
      "           4       0.85      0.86      0.85      5453\n",
      "           5       0.75      0.73      0.74      4182\n",
      "           6       0.79      0.71      0.75      1431\n",
      "           7       0.63      0.60      0.62       488\n",
      "           8       0.74      0.69      0.71       606\n",
      "           9       0.76      0.59      0.67      1795\n",
      "          10       0.62      0.58      0.60       446\n",
      "          11       0.79      0.72      0.76       845\n",
      "          12       0.49      0.04      0.07       579\n",
      "          13       0.00      0.00      0.00        25\n",
      "          14       0.77      0.42      0.54       414\n",
      "          15       0.66      0.65      0.65       139\n",
      "          16       0.64      0.55      0.59        51\n",
      "          17       0.63      0.67      0.65        33\n",
      "          18       0.61      0.63      0.62        49\n",
      "          19       0.76      0.70      0.73       324\n",
      "          20       0.68      0.56      0.61       253\n",
      "          21       0.00      0.00      0.00       198\n",
      "          22       0.59      0.51      0.54       380\n",
      "          23       0.59      0.51      0.55       377\n",
      "          24       0.60      0.46      0.52       471\n",
      "          25       0.38      0.11      0.17        89\n",
      "          26       0.60      0.67      0.63       347\n",
      "          27       0.50      0.09      0.15        88\n",
      "          28       0.50      0.12      0.19        77\n",
      "          29       0.77      0.41      0.54       293\n",
      "          30       0.81      0.58      0.68       614\n",
      "          31       1.00      0.90      0.95        31\n",
      "          32       0.83      0.71      0.76       143\n",
      "          33       0.78      0.50      0.61       206\n",
      "          34       1.00      0.94      0.97        31\n",
      "          35       0.76      0.60      0.67        68\n",
      "          36       0.62      0.41      0.49       291\n",
      "          37       0.72      0.47      0.57       389\n",
      "          38       0.69      0.47      0.56       589\n",
      "          39       0.65      0.30      0.41       149\n",
      "          40       0.80      0.79      0.79      1004\n",
      "          41       0.68      0.65      0.67        49\n",
      "          42       0.79      0.76      0.77        98\n",
      "          43       0.85      0.63      0.72        27\n",
      "          44       0.89      0.78      0.83       123\n",
      "          45       0.83      0.84      0.83      1379\n",
      "          46       0.86      0.71      0.78       143\n",
      "          47       0.85      0.63      0.72        27\n",
      "          48       0.78      0.73      0.75        44\n",
      "          49       0.33      0.17      0.22       143\n",
      "          50       0.36      0.21      0.26       167\n",
      "          51       0.24      0.10      0.14       119\n",
      "          52       0.56      0.27      0.36       208\n",
      "          53       0.50      0.04      0.07        56\n",
      "          54       0.84      0.61      0.71        75\n",
      "          55       0.60      0.72      0.65        43\n",
      "          56       0.66      0.58      0.62       139\n",
      "          57       0.87      0.78      0.82       254\n",
      "          58       0.80      0.75      0.77      1182\n",
      "          59       0.76      0.48      0.59        27\n",
      "          60       0.43      0.14      0.21       782\n",
      "          61       0.43      0.17      0.24       426\n",
      "          62       0.43      0.18      0.26       390\n",
      "          63       0.47      0.11      0.17       834\n",
      "          64       0.50      0.13      0.21        60\n",
      "          65       0.44      0.15      0.22       310\n",
      "          66       0.85      0.85      0.85       598\n",
      "          67       0.89      0.88      0.88       467\n",
      "          68       0.84      0.88      0.86       410\n",
      "          69       0.84      0.86      0.85        36\n",
      "          70       0.40      0.07      0.12       112\n",
      "          71       0.45      0.19      0.26       134\n",
      "          72       0.60      0.03      0.06       199\n",
      "          73       0.00      0.00      0.00       103\n",
      "          74       0.67      0.03      0.05       150\n",
      "          75       0.73      0.67      0.70        92\n",
      "          76       0.50      0.12      0.20       139\n",
      "          77       0.55      0.15      0.23       233\n",
      "          78       0.00      0.00      0.00        62\n",
      "          79       0.00      0.00      0.00        54\n",
      "          80       0.76      0.62      0.69        72\n",
      "          81       0.85      0.72      0.78        39\n",
      "          82       0.82      0.71      0.76       140\n",
      "          83       0.79      0.72      0.75       299\n",
      "          84       0.84      0.85      0.85       613\n",
      "          85       0.84      0.76      0.80        42\n",
      "          86       0.74      0.67      0.71        52\n",
      "          87       0.36      0.16      0.22       335\n",
      "          88       0.39      0.15      0.22       295\n",
      "          89       0.27      0.07      0.12        40\n",
      "          90       0.39      0.16      0.22       295\n",
      "          91       0.39      0.14      0.20       446\n",
      "          92       0.44      0.12      0.18       420\n",
      "          93       0.86      0.87      0.87       319\n",
      "          94       1.00      0.71      0.83        14\n",
      "          95       0.86      0.63      0.73        57\n",
      "          96       0.58      0.06      0.10       124\n",
      "          97       0.81      0.84      0.82       162\n",
      "          98       0.80      0.79      0.80       216\n",
      "          99       0.83      0.71      0.77       466\n",
      "         100       0.37      0.18      0.24       217\n",
      "         101       0.00      0.00      0.00        45\n",
      "         102       0.00      0.00      0.00        20\n",
      "         103       0.00      0.00      0.00        27\n",
      "         104       0.56      0.07      0.13       507\n",
      "         105       0.46      0.07      0.11       169\n",
      "         106       0.67      0.71      0.69       108\n",
      "         107       0.50      0.36      0.42       182\n",
      "         108       0.50      0.36      0.42       182\n",
      "         109       0.51      0.37      0.43       181\n",
      "         110       0.81      0.79      0.80       104\n",
      "         111       0.83      0.71      0.77       118\n",
      "         112       0.79      0.81      0.80        62\n",
      "         113       0.56      0.76      0.64        25\n",
      "         114       0.79      0.67      0.73       280\n",
      "         115       0.89      0.86      0.87       609\n",
      "         116       0.80      0.78      0.79       209\n",
      "         117       0.80      0.77      0.78       209\n",
      "         118       0.89      0.84      0.86       651\n",
      "         119       0.82      0.67      0.74       347\n",
      "         120       0.84      0.85      0.84       151\n",
      "         121       0.85      0.88      0.87       130\n",
      "         122       0.70      0.59      0.64        27\n",
      "         123       0.82      0.58      0.68       122\n",
      "         124       0.80      0.54      0.64        69\n",
      "         125       0.97      0.76      0.85        41\n",
      "         126       0.88      0.65      0.75        34\n",
      "         127       0.38      0.06      0.10        88\n",
      "         128       0.58      0.03      0.05       262\n",
      "         129       0.55      0.02      0.04       323\n",
      "         130       0.77      0.65      0.71        37\n",
      "         131       1.00      0.11      0.19        28\n",
      "         132       0.75      0.79      0.77        19\n",
      "         133       0.94      0.88      0.91       209\n",
      "         134       0.77      0.81      0.79        21\n",
      "         135       0.81      0.65      0.72        34\n",
      "         136       0.75      0.25      0.38        24\n",
      "         137       0.43      0.27      0.33        33\n",
      "         138       0.43      0.18      0.25        17\n",
      "         139       0.43      0.27      0.33        33\n",
      "         140       0.65      0.55      0.60       136\n",
      "         141       0.90      0.54      0.67        52\n",
      "         142       0.74      0.61      0.67        46\n",
      "         143       0.79      0.65      0.71        68\n",
      "         144       0.62      0.32      0.42        25\n",
      "         145       0.81      0.90      0.85       162\n",
      "         146       0.71      0.38      0.50        26\n",
      "         147       0.70      0.52      0.60       135\n",
      "         148       0.56      0.50      0.53        38\n",
      "         149       0.70      0.55      0.61       126\n",
      "         150       0.70      0.56      0.62       122\n",
      "         151       1.00      0.04      0.08        47\n",
      "         152       0.35      0.08      0.14        83\n",
      "         153       0.83      0.83      0.83        92\n",
      "         154       0.79      0.73      0.76       126\n",
      "         155       0.68      0.66      0.67        29\n",
      "         156       0.32      0.09      0.14        97\n",
      "         157       0.82      0.74      0.78       349\n",
      "         158       0.80      0.81      0.81       117\n",
      "         159       0.83      0.74      0.78       346\n",
      "         160       0.78      0.78      0.78       257\n",
      "         161       0.00      0.00      0.00        37\n",
      "         162       0.75      0.19      0.31        47\n",
      "         163       0.67      0.15      0.25        26\n",
      "         164       0.71      0.16      0.26        31\n",
      "         165       0.72      0.81      0.76        16\n",
      "         166       0.75      0.37      0.49        41\n",
      "         167       0.81      0.81      0.81        27\n",
      "         168       0.80      0.87      0.84        38\n",
      "         169       0.85      0.92      0.88        24\n",
      "         170       0.73      0.85      0.79        41\n",
      "         171       0.79      0.80      0.79        99\n",
      "         172       0.69      0.81      0.75        31\n",
      "         173       0.60      0.33      0.43        45\n",
      "         174       0.58      0.45      0.51        75\n",
      "         175       0.74      0.58      0.65       130\n",
      "         176       0.78      0.45      0.57        77\n",
      "         177       0.79      0.78      0.78        67\n",
      "         178       0.83      0.83      0.83        23\n",
      "         179       0.84      0.76      0.80        34\n",
      "         180       0.75      0.76      0.75        66\n",
      "         181       0.77      0.59      0.67        90\n",
      "         182       0.57      0.50      0.53        26\n",
      "         183       0.55      0.58      0.56        64\n",
      "         184       0.70      0.76      0.73        25\n",
      "         185       0.71      0.88      0.78        72\n",
      "         186       0.85      0.80      0.83        41\n",
      "         187       0.74      0.89      0.81        19\n",
      "         188       0.59      0.83      0.69        47\n",
      "         189       0.70      0.91      0.80        70\n",
      "         190       0.82      0.95      0.88        38\n",
      "         191       0.76      0.79      0.77       122\n",
      "         192       0.72      0.90      0.80        29\n",
      "         193       0.84      0.68      0.75       306\n",
      "         194       0.72      0.90      0.80        29\n",
      "         195       0.83      0.79      0.81       175\n",
      "         196       0.75      0.82      0.78       116\n",
      "         197       0.85      0.71      0.78       365\n",
      "         198       0.80      0.89      0.84        18\n",
      "         199       0.66      0.52      0.58        44\n",
      "         200       0.00      0.00      0.00        16\n",
      "         201       1.00      0.01      0.03        72\n",
      "         202       0.89      0.32      0.47        25\n",
      "         203       0.80      0.46      0.59        52\n",
      "         204       0.42      0.51      0.46        59\n",
      "         205       0.70      0.69      0.69       144\n",
      "         206       0.72      0.69      0.70       136\n",
      "         207       0.44      0.54      0.48        63\n",
      "         208       0.74      0.72      0.73       122\n",
      "         209       0.55      0.16      0.24        38\n",
      "         210       0.77      0.84      0.80        55\n",
      "         211       0.78      0.56      0.65       117\n",
      "         212       0.40      0.29      0.33        14\n",
      "         213       0.72      0.69      0.70       129\n",
      "         214       0.70      0.68      0.69        31\n",
      "         215       0.76      0.58      0.66        33\n",
      "         216       0.72      0.54      0.62       201\n",
      "         217       0.58      0.37      0.45        41\n",
      "         218       0.80      0.53      0.64       118\n",
      "         219       0.74      0.62      0.68        77\n",
      "         220       0.68      0.57      0.62        40\n",
      "         221       0.84      0.67      0.74       282\n",
      "         222       0.75      0.58      0.66        65\n",
      "         223       0.85      0.75      0.80       211\n",
      "         224       0.77      0.75      0.76        40\n",
      "         225       0.86      0.62      0.72        29\n",
      "         226       0.76      0.60      0.67        65\n",
      "         227       0.77      0.52      0.62       141\n",
      "         228       0.85      0.59      0.69        29\n",
      "         229       0.37      0.35      0.36        51\n",
      "         230       0.53      0.36      0.43       113\n",
      "         231       0.60      0.36      0.45       276\n",
      "         232       0.74      0.74      0.74        54\n",
      "         233       0.72      0.79      0.75        33\n",
      "         234       0.80      0.43      0.56       133\n",
      "         235       0.81      0.75      0.78        63\n",
      "         236       0.74      0.68      0.71        59\n",
      "         237       0.78      0.34      0.48       169\n",
      "         238       0.91      0.32      0.48        31\n",
      "         239       0.73      0.35      0.48        31\n",
      "         240       0.75      0.69      0.72       204\n",
      "         241       0.37      0.31      0.33       147\n",
      "         242       0.49      0.51      0.50        68\n",
      "         243       0.69      0.69      0.69        26\n",
      "         244       0.84      0.84      0.84        43\n",
      "         245       0.68      0.73      0.71        49\n",
      "         246       0.69      0.76      0.72        49\n",
      "         247       0.73      0.66      0.69        62\n",
      "         248       0.72      0.78      0.75        40\n",
      "         249       0.67      0.32      0.43        25\n",
      "         250       0.33      0.08      0.13        73\n",
      "         251       0.27      0.06      0.10        97\n",
      "         252       0.50      0.03      0.05        39\n",
      "         253       0.59      0.32      0.41        94\n",
      "         254       0.62      0.38      0.47        69\n",
      "         255       0.00      0.00      0.00        31\n",
      "         256       0.80      0.73      0.76        55\n",
      "         257       0.76      0.69      0.72        90\n",
      "         258       0.72      0.49      0.58        37\n",
      "         259       0.71      0.52      0.60        23\n",
      "         260       0.74      0.68      0.71        88\n",
      "         261       0.83      0.46      0.59        41\n",
      "         262       0.62      0.23      0.33        35\n",
      "         263       0.80      0.65      0.72        37\n",
      "         264       0.63      0.64      0.64        59\n",
      "         265       0.80      0.59      0.68        56\n",
      "         266       0.82      0.62      0.71        66\n",
      "         267       0.00      0.00      0.00        25\n",
      "         268       0.29      0.14      0.19        35\n",
      "         269       0.85      0.58      0.69        19\n",
      "         270       0.72      0.57      0.63        58\n",
      "         271       0.71      0.58      0.64        77\n",
      "         272       0.66      0.47      0.55        40\n",
      "         273       0.64      0.35      0.45        81\n",
      "         274       0.75      0.20      0.32        30\n",
      "         275       1.00      0.08      0.14        13\n",
      "         276       0.90      0.56      0.69        62\n",
      "         277       0.85      0.68      0.76       136\n",
      "         278       0.85      0.68      0.76       136\n",
      "         279       0.63      0.48      0.55        79\n",
      "         280       0.61      0.41      0.49        56\n",
      "         281       0.31      0.10      0.15       177\n",
      "         282       0.20      0.04      0.06        26\n",
      "         283       0.37      0.15      0.22       240\n",
      "         284       0.54      0.17      0.25        42\n",
      "         285       0.55      0.16      0.24        38\n",
      "         286       0.50      0.50      0.50        42\n",
      "         287       0.24      0.10      0.14        48\n",
      "         288       0.78      0.80      0.79        35\n",
      "         289       0.96      1.00      0.98        22\n",
      "         290       0.00      0.00      0.00        40\n",
      "         291       0.64      0.39      0.48        36\n",
      "         292       0.77      0.49      0.60        49\n",
      "         293       0.44      0.32      0.38        37\n",
      "         294       0.78      0.76      0.77        76\n",
      "         295       0.53      0.67      0.59        24\n",
      "         296       0.89      0.83      0.86        29\n",
      "         297       0.89      0.83      0.86        29\n",
      "         298       0.69      0.27      0.39        66\n",
      "         299       0.48      0.12      0.20        97\n",
      "         300       0.83      0.29      0.43        35\n",
      "         301       0.71      0.39      0.50        31\n",
      "         302       0.87      0.59      0.70        22\n",
      "         303       0.81      0.63      0.71        27\n",
      "         304       0.56      0.59      0.57        34\n",
      "         305       0.49      0.65      0.56        26\n",
      "         306       0.67      0.70      0.68        20\n",
      "         307       0.91      0.88      0.89        24\n",
      "         308       0.91      0.77      0.83        65\n",
      "         309       0.82      0.83      0.82        48\n",
      "         310       0.78      0.86      0.82        29\n",
      "         311       0.71      0.77      0.74        35\n",
      "         312       0.76      0.79      0.78        57\n",
      "         313       0.50      0.16      0.24        69\n",
      "         314       0.38      0.17      0.24       197\n",
      "         315       0.39      0.17      0.24       212\n",
      "         316       0.46      0.16      0.24        69\n",
      "         317       0.35      0.11      0.17        64\n",
      "         318       0.35      0.10      0.16        67\n",
      "         319       0.00      0.00      0.00        18\n",
      "         320       0.40      0.11      0.17       108\n",
      "         321       0.37      0.13      0.19       133\n",
      "         322       0.35      0.14      0.20       155\n",
      "         323       0.86      0.26      0.40        23\n",
      "         324       0.71      0.29      0.42        17\n",
      "         325       0.00      0.00      0.00        23\n",
      "         326       0.76      0.52      0.62        92\n",
      "         327       0.76      0.22      0.35        85\n",
      "         328       0.69      0.49      0.57        59\n",
      "         329       0.40      0.25      0.31        16\n",
      "         330       0.45      0.28      0.34        18\n",
      "         331       0.54      0.29      0.37        52\n",
      "         332       0.38      0.02      0.03       177\n",
      "         333       0.25      0.01      0.02       202\n",
      "         334       0.47      0.36      0.41        22\n",
      "         335       0.50      0.15      0.23        27\n",
      "         336       0.78      0.57      0.66        63\n",
      "         337       1.00      0.25      0.40        20\n",
      "         338       0.91      0.50      0.65        20\n",
      "         339       0.50      0.09      0.15        35\n",
      "         340       1.00      0.09      0.16        23\n",
      "         341       0.00      0.00      0.00        36\n",
      "         342       0.50      0.14      0.22        36\n",
      "         343       0.47      0.11      0.18        63\n",
      "         344       0.00      0.00      0.00        39\n",
      "         345       0.87      0.65      0.74        20\n",
      "         346       0.68      0.60      0.64        35\n",
      "         347       0.84      0.70      0.76        37\n",
      "         348       0.50      0.02      0.03        62\n",
      "         349       0.50      0.02      0.04        52\n",
      "         350       0.29      0.06      0.10        35\n",
      "         351       1.00      0.87      0.93        31\n",
      "         352       0.56      0.18      0.27        28\n",
      "         353       0.00      0.00      0.00        24\n",
      "         354       0.33      0.05      0.08        22\n",
      "         355       0.69      0.67      0.68        30\n",
      "         356       0.84      0.81      0.82        26\n",
      "         357       0.78      0.68      0.72        31\n",
      "         358       0.61      0.22      0.32        65\n",
      "         359       0.54      0.18      0.27        76\n",
      "         360       0.90      0.35      0.50        26\n",
      "         361       0.81      0.28      0.42        46\n",
      "         362       0.53      0.10      0.17       157\n",
      "         363       0.50      0.13      0.20       119\n",
      "         364       0.89      0.76      0.82        21\n",
      "         365       0.70      0.41      0.52        17\n",
      "         366       0.28      0.14      0.18        80\n",
      "         367       0.25      0.02      0.03        63\n",
      "         368       0.50      0.05      0.09        41\n",
      "         369       0.67      0.44      0.53        32\n",
      "         370       0.61      0.38      0.47        29\n",
      "         371       0.50      0.04      0.07        25\n",
      "         372       0.20      0.04      0.07        23\n",
      "         373       0.89      0.67      0.76        36\n",
      "         374       0.71      0.46      0.56        26\n",
      "         375       0.58      0.44      0.50        34\n",
      "         376       0.59      0.33      0.43        48\n",
      "         377       0.47      0.17      0.25        48\n",
      "         378       0.77      0.81      0.79        54\n",
      "         379       0.00      0.00      0.00        29\n",
      "         380       0.74      0.59      0.66        34\n",
      "         381       0.72      0.59      0.65        49\n",
      "         382       0.72      0.45      0.55        29\n",
      "         383       0.74      0.47      0.58        36\n",
      "         384       0.78      0.44      0.56        32\n",
      "         385       0.90      0.69      0.78        26\n",
      "         386       0.71      0.50      0.59        30\n",
      "         387       0.79      0.24      0.37        46\n",
      "         388       0.79      0.26      0.39        42\n",
      "         389       0.83      0.91      0.87        22\n",
      "         390       0.52      0.21      0.30        57\n",
      "         391       0.42      0.33      0.37        15\n",
      "         392       0.83      0.26      0.40        19\n",
      "         393       0.00      0.00      0.00        19\n",
      "         394       0.90      0.85      0.88        33\n",
      "         395       0.62      0.62      0.62        24\n",
      "         396       0.77      0.71      0.74        14\n",
      "         397       0.87      0.68      0.76        19\n",
      "         398       0.85      0.55      0.67        20\n",
      "         399       0.00      0.00      0.00        21\n",
      "         400       0.29      0.06      0.10        35\n",
      "         401       0.29      0.06      0.09        36\n",
      "         402       0.38      0.12      0.18        49\n",
      "         403       0.77      1.00      0.87        20\n",
      "         404       1.00      0.25      0.40        24\n",
      "         405       0.83      0.69      0.75        42\n",
      "         406       0.75      0.17      0.27        18\n",
      "         407       1.00      0.03      0.06        35\n",
      "         408       0.25      0.01      0.02       108\n",
      "         409       1.00      0.46      0.63        13\n",
      "         410       0.45      0.07      0.12        71\n",
      "         411       0.75      0.72      0.74        29\n",
      "         412       0.62      0.14      0.23        36\n",
      "         413       0.75      0.06      0.12        48\n",
      "         414       0.00      0.00      0.00        12\n",
      "         415       0.82      0.61      0.70        23\n",
      "         416       0.83      0.76      0.79        33\n",
      "         417       1.00      0.02      0.04        50\n",
      "         418       0.62      0.15      0.24        33\n",
      "         419       0.57      0.25      0.35        32\n",
      "         420       0.00      0.00      0.00        33\n",
      "         421       0.40      0.08      0.14        24\n",
      "         422       0.78      0.30      0.44        23\n",
      "         423       0.47      0.32      0.38        22\n",
      "         424       1.00      0.07      0.13        14\n",
      "         425       0.00      0.00      0.00        46\n",
      "         426       0.43      0.29      0.34        21\n",
      "         427       0.44      0.20      0.28        20\n",
      "         428       0.77      0.53      0.62        19\n",
      "         429       0.83      0.53      0.65        19\n",
      "         430       0.00      0.00      0.00        27\n",
      "         431       0.40      0.40      0.40        20\n",
      "         432       0.77      0.85      0.81        27\n",
      "         433       0.55      0.19      0.29        31\n",
      "         434       0.58      0.47      0.52        15\n",
      "         435       0.58      0.47      0.52        15\n",
      "         436       0.40      0.10      0.15        21\n",
      "         437       1.00      0.13      0.24        15\n",
      "         438       0.27      0.15      0.19        46\n",
      "         439       0.59      0.31      0.41        32\n",
      "         440       0.61      0.79      0.69        14\n",
      "         441       0.57      0.24      0.33        17\n",
      "         442       0.50      0.10      0.17        29\n",
      "         443       0.44      0.16      0.24        25\n",
      "         444       0.00      0.00      0.00        21\n",
      "         445       0.60      0.21      0.32        14\n",
      "         446       0.00      0.00      0.00        20\n",
      "         447       0.00      0.00      0.00        27\n",
      "         448       0.43      0.12      0.19        24\n",
      "         449       0.00      0.00      0.00        15\n",
      "\n",
      "   micro avg       0.79      0.62      0.69     73956\n",
      "   macro avg       0.63      0.45      0.50     73956\n",
      "weighted avg       0.73      0.62      0.65     73956\n",
      " samples avg       0.81      0.67      0.70     73956\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 605us/step\n"
     ]
    }
   ],
   "source": [
    "### Prediction using sample test\n",
    "y_pred_gt = model.predict(X_test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = generate_submission_df(y_pred_gt, test_ids, pred_columns)\n",
    "submission_df.to_csv('./dataset/prediction/sample_prediction_mf.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess - Biological Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "selected_ids = create_training_ids('./dataset/test/sampled_test.txt', './dataset/train/train_ids.txt', 1.0)\n",
    "df_train_set = filter_train_data(df_train_set_all, selected_ids, 'biological_process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "test_ids = sample_protein_ids('./dataset/test/sampled_test.txt', 1.0)\n",
    "X_test_gt = build_test_data(p_embeddings_data, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|███████████████████████████████| 260/260 [03:02<00:00,  1.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein_ID</th>\n",
       "      <th>GO:0090304</th>\n",
       "      <th>GO:0044271</th>\n",
       "      <th>GO:0010467</th>\n",
       "      <th>GO:0034641</th>\n",
       "      <th>GO:0016070</th>\n",
       "      <th>GO:0006366</th>\n",
       "      <th>GO:0044249</th>\n",
       "      <th>GO:0043170</th>\n",
       "      <th>GO:0009058</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0008360</th>\n",
       "      <th>GO:0030522</th>\n",
       "      <th>GO:1901264</th>\n",
       "      <th>GO:0051983</th>\n",
       "      <th>GO:0042129</th>\n",
       "      <th>GO:0050777</th>\n",
       "      <th>GO:0051445</th>\n",
       "      <th>GO:0071322</th>\n",
       "      <th>GO:2000027</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q04418</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.022, -0.06964, -0.007042, 0.0544, -0.04633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q7ZT12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.04028, -0.03357, 0.1046, 0.0669, -0.07935, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q6DBW0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01106, 0.02277, 0.02895, 0.03293, -0.00641,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q9WUC4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.06134, -0.00452, 0.01472, 0.001324, 0.03162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q03370</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.014366, -0.0655, 0.0208, 0.0652, 0.0433, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1489 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Protein_ID  GO:0090304  GO:0044271  GO:0010467  GO:0034641  GO:0016070  \\\n",
       "0     Q04418           1           1           1           1           1   \n",
       "1     Q7ZT12           0           0           0           0           0   \n",
       "2     Q6DBW0           0           0           0           0           0   \n",
       "3     Q9WUC4           0           0           0           0           0   \n",
       "4     Q03370           0           0           0           0           0   \n",
       "\n",
       "   GO:0006366  GO:0044249  GO:0043170  GO:0009058  ...  GO:0008360  \\\n",
       "0           1           1           1           1  ...           0   \n",
       "1           0           0           0           0  ...           0   \n",
       "2           0           0           0           0  ...           0   \n",
       "3           0           0           0           0  ...           0   \n",
       "4           0           0           0           0  ...           0   \n",
       "\n",
       "   GO:0030522  GO:1901264  GO:0051983  GO:0042129  GO:0050777  GO:0051445  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   GO:0071322  GO:2000027                                          embedding  \n",
       "0           0           0  [-0.022, -0.06964, -0.007042, 0.0544, -0.04633...  \n",
       "1           0           0  [0.04028, -0.03357, 0.1046, 0.0669, -0.07935, ...  \n",
       "2           0           0  [0.01106, 0.02277, 0.02895, 0.03293, -0.00641,...  \n",
       "3           0           0  [0.06134, -0.00452, 0.01472, 0.001324, 0.03162...  \n",
       "4           0           0  [-0.014366, -0.0655, 0.0208, 0.0652, 0.0433, 0...  \n",
       "\n",
       "[5 rows x 1489 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = encode_go_terms_sparse(df_train_set)\n",
    "df_encoded = get_embeddings(df_encoded, p_embeddings_data)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top N labels, for biological process 1100\n",
    "freq_df = pd.read_csv('./dataset/train/biological_process_freq.csv')[:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = df_encoded.iloc[:, 1:-1]\n",
    "pred_columns = get_top_freq(y_columns.columns.tolist(), set(freq_df['id']))\n",
    "y_columns = y_columns[pred_columns]\n",
    "y = create_y(y_columns)\n",
    "X = create_X(df_encoded,'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0978 - f1_score: 0.2009\n",
      "Epoch 2/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0833 - f1_score: 0.2762\n",
      "Epoch 3/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0808 - f1_score: 0.2970\n",
      "Epoch 4/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0792 - f1_score: 0.3119\n",
      "Epoch 5/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0780 - f1_score: 0.3242\n",
      "Epoch 6/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0769 - f1_score: 0.3344\n",
      "Epoch 7/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0760 - f1_score: 0.3425\n",
      "Epoch 8/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0753 - f1_score: 0.3513\n",
      "Epoch 9/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0746 - f1_score: 0.3583\n",
      "Epoch 10/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0740 - f1_score: 0.3655\n",
      "Epoch 11/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0734 - f1_score: 0.3715\n",
      "Epoch 12/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0728 - f1_score: 0.3770\n",
      "Epoch 13/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0723 - f1_score: 0.3830\n",
      "Epoch 14/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0718 - f1_score: 0.3887\n",
      "Epoch 15/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0714 - f1_score: 0.3941\n",
      "Epoch 16/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0710 - f1_score: 0.3984\n",
      "Epoch 17/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0706 - f1_score: 0.4026\n",
      "Epoch 18/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0702 - f1_score: 0.4070\n",
      "Epoch 19/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0698 - f1_score: 0.4108\n",
      "Epoch 20/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0695 - f1_score: 0.4154\n",
      "Epoch 21/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0692 - f1_score: 0.4192\n",
      "Epoch 22/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0688 - f1_score: 0.4221\n",
      "Epoch 23/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0686 - f1_score: 0.4253\n",
      "Epoch 24/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0683 - f1_score: 0.4286\n",
      "Epoch 25/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0680 - f1_score: 0.4315\n",
      "Epoch 26/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0677 - f1_score: 0.4341\n",
      "Epoch 27/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0675 - f1_score: 0.4367\n",
      "Epoch 28/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0673 - f1_score: 0.4394\n",
      "Epoch 29/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0670 - f1_score: 0.4420\n",
      "Epoch 30/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0668 - f1_score: 0.4447\n",
      "Epoch 31/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0666 - f1_score: 0.4471\n",
      "Epoch 32/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0664 - f1_score: 0.4491\n",
      "Epoch 33/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0662 - f1_score: 0.4518\n",
      "Epoch 34/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0660 - f1_score: 0.4533\n",
      "Epoch 35/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0658 - f1_score: 0.4561\n",
      "Epoch 36/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0656 - f1_score: 0.4578\n",
      "Epoch 37/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0655 - f1_score: 0.4596\n",
      "Epoch 38/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0653 - f1_score: 0.4613\n",
      "Epoch 39/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0652 - f1_score: 0.4634\n",
      "Epoch 40/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0650 - f1_score: 0.4650\n",
      "Epoch 41/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0649 - f1_score: 0.4666\n",
      "Epoch 42/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0647 - f1_score: 0.4681\n",
      "Epoch 43/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0645 - f1_score: 0.4698\n",
      "Epoch 44/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0644 - f1_score: 0.4720\n",
      "Epoch 45/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0643 - f1_score: 0.4729\n",
      "Epoch 46/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0641 - f1_score: 0.4741\n",
      "Epoch 47/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0640 - f1_score: 0.4765\n",
      "Epoch 48/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0639 - f1_score: 0.4770\n",
      "Epoch 49/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0638 - f1_score: 0.4784\n",
      "Epoch 50/50\n",
      "2180/2180 [==============================] - 4s 2ms/step - loss: 0.0636 - f1_score: 0.4801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "model = train(X_train, y_train, './model/bp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385/385 [==============================] - 0s 697us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Bilogical Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.53      0.57      1023\n",
      "           1       0.60      0.28      0.38       689\n",
      "           2       0.59      0.42      0.49       943\n",
      "           3       0.65      0.50      0.56      1572\n",
      "           4       0.57      0.49      0.52       672\n",
      "           5       0.39      0.07      0.12       100\n",
      "           6       0.59      0.49      0.53      1961\n",
      "           7       0.61      0.52      0.56      2363\n",
      "           8       0.61      0.53      0.57      2090\n",
      "           9       0.66      0.63      0.64      3411\n",
      "          10       0.63      0.54      0.58      2908\n",
      "          11       0.70      0.70      0.70      4173\n",
      "          12       0.51      0.11      0.18       175\n",
      "          13       0.54      0.21      0.30       598\n",
      "          14       0.72      0.92      0.81      8284\n",
      "          15       0.55      0.18      0.28       509\n",
      "          16       1.00      1.00      1.00     12310\n",
      "          17       0.64      0.49      0.56      1251\n",
      "          18       0.58      0.44      0.50      1197\n",
      "          19       0.62      0.48      0.54      1481\n",
      "          20       0.64      0.60      0.62      3319\n",
      "          21       0.51      0.10      0.17       182\n",
      "          22       0.60      0.52      0.56      2046\n",
      "          23       0.65      0.47      0.54      1458\n",
      "          24       0.62      0.47      0.53      1626\n",
      "          25       0.50      0.17      0.26       494\n",
      "          26       0.69      0.67      0.68      3865\n",
      "          27       0.56      0.19      0.29       344\n",
      "          28       0.55      0.05      0.08       631\n",
      "          29       0.67      0.29      0.41      1005\n",
      "          30       1.00      0.02      0.03        60\n",
      "          31       0.69      0.29      0.40       859\n",
      "          32       1.00      0.02      0.04        53\n",
      "          33       1.00      0.02      0.03       116\n",
      "          34       0.70      0.27      0.39       912\n",
      "          35       1.00      0.01      0.03        74\n",
      "          36       0.56      0.09      0.16      1281\n",
      "          37       0.54      0.03      0.06       440\n",
      "          38       0.68      0.19      0.30      1432\n",
      "          39       0.56      0.02      0.04       466\n",
      "          40       0.56      0.06      0.11      1011\n",
      "          41       0.56      0.28      0.37      2292\n",
      "          42       0.57      0.13      0.21      1909\n",
      "          43       0.56      0.04      0.08       934\n",
      "          44       0.62      0.45      0.52      4150\n",
      "          45       0.69      0.28      0.40       858\n",
      "          46       0.76      0.15      0.26       440\n",
      "          47       1.00      0.02      0.04       100\n",
      "          48       0.25      0.00      0.01       206\n",
      "          49       0.53      0.05      0.10       492\n",
      "          50       0.45      0.03      0.05       363\n",
      "          51       0.47      0.02      0.04       399\n",
      "          52       0.43      0.03      0.06       443\n",
      "          53       0.33      0.01      0.01       141\n",
      "          54       0.38      0.03      0.06       164\n",
      "          55       0.67      0.01      0.02       168\n",
      "          56       0.43      0.02      0.03       180\n",
      "          57       0.53      0.06      0.10       703\n",
      "          58       0.62      0.02      0.04       225\n",
      "          59       0.50      0.01      0.02        94\n",
      "          60       0.00      0.00      0.00        75\n",
      "          61       0.56      0.04      0.08       336\n",
      "          62       0.67      0.03      0.05        73\n",
      "          63       0.67      0.03      0.05        73\n",
      "          64       0.67      0.04      0.07       306\n",
      "          65       1.00      0.06      0.12        62\n",
      "          66       0.50      0.01      0.02        86\n",
      "          67       0.57      0.03      0.06       359\n",
      "          68       0.60      0.31      0.41      1991\n",
      "          69       0.65      0.43      0.52      3567\n",
      "          70       1.00      0.04      0.09        67\n",
      "          71       0.66      0.44      0.52      3476\n",
      "          72       0.50      0.01      0.03        76\n",
      "          73       0.64      0.36      0.46      2583\n",
      "          74       0.66      0.42      0.51      3316\n",
      "          75       0.55      0.23      0.32      1389\n",
      "          76       0.00      0.00      0.00        76\n",
      "          77       0.00      0.00      0.00        60\n",
      "          78       0.50      0.10      0.17       882\n",
      "          79       0.80      0.05      0.09       254\n",
      "          80       0.61      0.20      0.31      2073\n",
      "          81       0.69      0.70      0.69      5300\n",
      "          82       0.69      0.71      0.70      5578\n",
      "          83       0.66      0.64      0.65      4726\n",
      "          84       0.50      0.02      0.05       249\n",
      "          85       0.50      0.03      0.05       238\n",
      "          86       0.47      0.26      0.34      1823\n",
      "          87       0.48      0.28      0.36      2042\n",
      "          88       0.53      0.04      0.08       380\n",
      "          89       0.55      0.04      0.08       422\n",
      "          90       0.67      0.47      0.55      1502\n",
      "          91       0.40      0.20      0.27       154\n",
      "          92       0.59      0.34      0.43        77\n",
      "          93       0.38      0.21      0.27       135\n",
      "          94       0.35      0.20      0.25       128\n",
      "          95       0.57      0.42      0.48       262\n",
      "          96       0.43      0.22      0.29       153\n",
      "          97       0.58      0.39      0.46       235\n",
      "          98       0.58      0.42      0.49       310\n",
      "          99       0.52      0.26      0.35       388\n",
      "         100       0.66      0.47      0.55      1606\n",
      "         101       0.66      0.45      0.54      1921\n",
      "         102       0.56      0.33      0.41      2482\n",
      "         103       0.56      0.37      0.44      2616\n",
      "         104       0.56      0.26      0.35      1347\n",
      "         105       0.61      0.38      0.47       143\n",
      "         106       0.53      0.52      0.52       362\n",
      "         107       0.60      0.42      0.49       130\n",
      "         108       0.62      0.43      0.51       128\n",
      "         109       0.58      0.37      0.45       164\n",
      "         110       0.51      0.31      0.39       267\n",
      "         111       0.63      0.40      0.49       125\n",
      "         112       0.56      0.20      0.29       613\n",
      "         113       0.54      0.20      0.29       593\n",
      "         114       0.57      0.16      0.25       190\n",
      "         115       0.50      0.19      0.27       573\n",
      "         116       0.59      0.19      0.28       172\n",
      "         117       0.60      0.38      0.47      1212\n",
      "         118       0.61      0.43      0.51      1905\n",
      "         119       0.47      0.18      0.26       682\n",
      "         120       0.65      0.05      0.09       225\n",
      "         121       0.00      0.00      0.00        59\n",
      "         122       0.00      0.00      0.00        99\n",
      "         123       0.57      0.16      0.25       239\n",
      "         124       0.48      0.14      0.22      1072\n",
      "         125       0.47      0.04      0.07       379\n",
      "         126       0.52      0.11      0.18       912\n",
      "         127       0.00      0.00      0.00        81\n",
      "         128       0.38      0.05      0.08       612\n",
      "         129       0.66      0.25      0.36       123\n",
      "         130       0.69      0.42      0.52       387\n",
      "         131       0.57      0.30      0.39        67\n",
      "         132       0.46      0.29      0.35       772\n",
      "         133       0.61      0.45      0.52      2522\n",
      "         134       0.62      0.46      0.53      1901\n",
      "         135       0.46      0.25      0.32       434\n",
      "         136       0.62      0.46      0.53      1135\n",
      "         137       0.44      0.26      0.33       974\n",
      "         138       0.61      0.46      0.52      2337\n",
      "         139       0.62      0.46      0.53      1982\n",
      "         140       0.60      0.40      0.48      1833\n",
      "         141       0.62      0.46      0.53      1133\n",
      "         142       0.46      0.30      0.36       746\n",
      "         143       0.47      0.22      0.30       681\n",
      "         144       0.60      0.41      0.49      1918\n",
      "         145       0.44      0.27      0.34       889\n",
      "         146       0.62      0.46      0.53      2018\n",
      "         147       0.62      0.47      0.53      1858\n",
      "         148       0.47      0.23      0.31       462\n",
      "         149       0.49      0.26      0.34       407\n",
      "         150       0.61      0.45      0.52      2243\n",
      "         151       0.61      0.44      0.51      1345\n",
      "         152       0.45      0.28      0.35       902\n",
      "         153       0.61      0.46      0.53      1241\n",
      "         154       0.46      0.28      0.35       786\n",
      "         155       0.49      0.26      0.34       407\n",
      "         156       0.00      0.00      0.00       180\n",
      "         157       0.58      0.03      0.05       994\n",
      "         158       0.83      0.02      0.04       255\n",
      "         159       1.00      0.01      0.02        92\n",
      "         160       0.00      0.00      0.00       101\n",
      "         161       0.00      0.00      0.00       110\n",
      "         162       0.49      0.03      0.06      1054\n",
      "         163       0.47      0.03      0.06       231\n",
      "         164       0.59      0.06      0.11       168\n",
      "         165       0.59      0.09      0.15       491\n",
      "         166       0.62      0.06      0.11       257\n",
      "         167       0.61      0.25      0.35      1486\n",
      "         168       0.52      0.05      0.09       299\n",
      "         169       0.75      0.08      0.15        71\n",
      "         170       0.56      0.03      0.06       664\n",
      "         171       0.38      0.01      0.02       306\n",
      "         172       1.00      0.01      0.01       183\n",
      "         173       0.60      0.03      0.05       109\n",
      "         174       0.53      0.16      0.24       527\n",
      "         175       0.64      0.42      0.51       155\n",
      "         176       0.60      0.35      0.44       176\n",
      "         177       0.52      0.14      0.22       541\n",
      "         178       0.00      0.00      0.00       101\n",
      "         179       0.48      0.21      0.29       263\n",
      "         180       0.50      0.21      0.30       241\n",
      "         181       0.52      0.26      0.35       489\n",
      "         182       0.40      0.02      0.04       535\n",
      "         183       0.60      0.01      0.02       245\n",
      "         184       0.50      0.03      0.06        66\n",
      "         185       0.50      0.01      0.02       115\n",
      "         186       0.49      0.12      0.19       206\n",
      "         187       0.47      0.09      0.16        97\n",
      "         188       0.52      0.03      0.07       489\n",
      "         189       0.48      0.08      0.13      1025\n",
      "         190       0.57      0.02      0.04       425\n",
      "         191       0.50      0.01      0.02       350\n",
      "         192       0.00      0.00      0.00        82\n",
      "         193       0.53      0.30      0.38      2475\n",
      "         194       0.00      0.00      0.00       133\n",
      "         195       0.30      0.01      0.03       207\n",
      "         196       0.48      0.06      0.11       995\n",
      "         197       0.88      0.07      0.13        96\n",
      "         198       0.70      0.07      0.12       206\n",
      "         199       0.53      0.05      0.09       173\n",
      "         200       0.50      0.02      0.04        54\n",
      "         201       0.56      0.22      0.32       393\n",
      "         202       0.50      0.02      0.03       243\n",
      "         203       0.55      0.35      0.42        81\n",
      "         204       0.56      0.22      0.32      1587\n",
      "         205       0.51      0.16      0.24       141\n",
      "         206       0.55      0.35      0.42        81\n",
      "         207       0.56      0.22      0.32      1607\n",
      "         208       0.50      0.04      0.08        67\n",
      "         209       0.56      0.28      0.38       341\n",
      "         210       0.40      0.10      0.16       172\n",
      "         211       0.42      0.09      0.14       491\n",
      "         212       0.50      0.25      0.34       832\n",
      "         213       0.33      0.05      0.09       111\n",
      "         214       0.35      0.06      0.11       113\n",
      "         215       0.57      0.08      0.14       154\n",
      "         216       1.00      0.06      0.11        50\n",
      "         217       0.75      0.05      0.10        59\n",
      "         218       0.00      0.00      0.00       123\n",
      "         219       0.00      0.00      0.00        80\n",
      "         220       1.00      0.00      0.01       247\n",
      "         221       0.55      0.22      0.32       930\n",
      "         222       0.25      0.00      0.01       376\n",
      "         223       0.50      0.00      0.01       266\n",
      "         224       0.70      0.25      0.36       650\n",
      "         225       1.00      0.01      0.02        87\n",
      "         226       0.58      0.09      0.15       416\n",
      "         227       0.60      0.23      0.33      1487\n",
      "         228       0.60      0.24      0.34      1302\n",
      "         229       0.42      0.03      0.05       287\n",
      "         230       0.58      0.24      0.34      1127\n",
      "         231       0.56      0.05      0.09       359\n",
      "         232       0.00      0.00      0.00        54\n",
      "         233       0.55      0.14      0.22      1224\n",
      "         234       0.00      0.00      0.00        86\n",
      "         235       1.00      0.01      0.01       151\n",
      "         236       0.52      0.12      0.19       846\n",
      "         237       0.55      0.15      0.23      1164\n",
      "         238       0.62      0.03      0.06       246\n",
      "         239       0.67      0.04      0.08       241\n",
      "         240       0.54      0.04      0.07       181\n",
      "         241       0.58      0.06      0.11       173\n",
      "         242       0.50      0.02      0.04       155\n",
      "         243       0.00      0.00      0.00        45\n",
      "         244       0.59      0.21      0.31       578\n",
      "         245       0.57      0.21      0.30       636\n",
      "         246       0.41      0.04      0.07       320\n",
      "         247       0.44      0.03      0.05       417\n",
      "         248       0.00      0.00      0.00        67\n",
      "         249       0.40      0.15      0.22       208\n",
      "         250       0.14      0.01      0.01       167\n",
      "         251       0.37      0.09      0.15       207\n",
      "         252       0.42      0.09      0.14       268\n",
      "         253       0.00      0.00      0.00       348\n",
      "         254       0.53      0.03      0.06       529\n",
      "         255       0.43      0.07      0.12       333\n",
      "         256       0.41      0.15      0.22       205\n",
      "         257       0.61      0.13      0.22       907\n",
      "         258       0.09      0.00      0.01       221\n",
      "         259       0.34      0.14      0.19       162\n",
      "         260       0.40      0.02      0.03       380\n",
      "         261       0.14      0.01      0.02       200\n",
      "         262       0.39      0.12      0.19       178\n",
      "         263       0.11      0.01      0.01       144\n",
      "         264       0.34      0.13      0.19       163\n",
      "         265       0.40      0.02      0.04       187\n",
      "         266       0.48      0.08      0.14       135\n",
      "         267       0.50      0.12      0.19        86\n",
      "         268       0.33      0.01      0.01       137\n",
      "         269       0.33      0.01      0.01       163\n",
      "         270       0.54      0.27      0.36      2175\n",
      "         271       0.67      0.33      0.44       773\n",
      "         272       0.45      0.15      0.22        68\n",
      "         273       0.70      0.06      0.11       123\n",
      "         274       0.00      0.00      0.00       198\n",
      "         275       0.00      0.00      0.00       297\n",
      "         276       0.59      0.18      0.27       664\n",
      "         277       0.62      0.25      0.36       516\n",
      "         278       0.62      0.26      0.36       562\n",
      "         279       0.45      0.15      0.23       198\n",
      "         280       0.51      0.16      0.25       221\n",
      "         281       0.60      0.18      0.28      1129\n",
      "         282       0.00      0.00      0.00        73\n",
      "         283       0.44      0.12      0.19        65\n",
      "         284       0.36      0.18      0.24        57\n",
      "         285       0.53      0.12      0.20        64\n",
      "         286       0.58      0.18      0.27       369\n",
      "         287       0.46      0.05      0.09       117\n",
      "         288       0.47      0.15      0.22       143\n",
      "         289       0.44      0.03      0.06       242\n",
      "         290       0.64      0.45      0.53       586\n",
      "         291       0.55      0.37      0.44       151\n",
      "         292       0.64      0.44      0.53       610\n",
      "         293       0.66      0.48      0.56      1022\n",
      "         294       0.60      0.47      0.53       206\n",
      "         295       0.64      0.44      0.52       607\n",
      "         296       0.53      0.12      0.20      1359\n",
      "         297       0.82      0.07      0.12       269\n",
      "         298       0.72      0.04      0.08       491\n",
      "         299       0.67      0.09      0.15       140\n",
      "         300       0.69      0.11      0.19       234\n",
      "         301       0.76      0.08      0.15       197\n",
      "         302       0.64      0.13      0.22       405\n",
      "         303       0.55      0.06      0.11       378\n",
      "         304       0.54      0.27      0.36        73\n",
      "         305       0.55      0.27      0.36        64\n",
      "         306       0.33      0.01      0.02       130\n",
      "         307       0.50      0.04      0.08        70\n",
      "         308       0.22      0.02      0.04        92\n",
      "         309       0.74      0.26      0.38       352\n",
      "         310       0.48      0.03      0.06       656\n",
      "         311       0.00      0.00      0.00       224\n",
      "         312       0.00      0.00      0.00        67\n",
      "         313       1.00      0.01      0.01       309\n",
      "         314       1.00      0.01      0.02       252\n",
      "         315       0.00      0.00      0.00        91\n",
      "         316       0.00      0.00      0.00       256\n",
      "         317       0.00      0.00      0.00       152\n",
      "         318       0.59      0.24      0.34       468\n",
      "         319       0.89      0.02      0.04       356\n",
      "         320       0.59      0.23      0.33      1006\n",
      "         321       0.00      0.00      0.00        52\n",
      "         322       0.52      0.05      0.10       875\n",
      "         323       0.58      0.14      0.23       176\n",
      "         324       0.00      0.00      0.00        80\n",
      "         325       0.50      0.00      0.01       353\n",
      "         326       0.00      0.00      0.00       307\n",
      "         327       0.00      0.00      0.00       127\n",
      "         328       0.00      0.00      0.00        82\n",
      "         329       0.00      0.00      0.00       140\n",
      "         330       0.45      0.04      0.08       577\n",
      "         331       0.62      0.03      0.07       286\n",
      "         332       0.59      0.26      0.36       630\n",
      "         333       0.45      0.04      0.07       715\n",
      "         334       0.00      0.00      0.00       147\n",
      "         335       0.00      0.00      0.00        60\n",
      "         336       0.00      0.00      0.00       105\n",
      "         337       1.00      0.03      0.05       111\n",
      "         338       0.48      0.27      0.34      1300\n",
      "         339       0.00      0.00      0.00        71\n",
      "         340       0.47      0.21      0.29       966\n",
      "         341       0.41      0.08      0.13       589\n",
      "         342       0.00      0.00      0.00        78\n",
      "         343       0.32      0.02      0.04       384\n",
      "         344       0.47      0.24      0.32      1154\n",
      "         345       0.30      0.01      0.02       309\n",
      "         346       0.29      0.02      0.04       233\n",
      "         347       0.00      0.00      0.00       143\n",
      "         348       0.00      0.00      0.00       223\n",
      "         349       0.83      0.04      0.07       136\n",
      "         350       0.51      0.12      0.20       529\n",
      "         351       0.00      0.00      0.00        83\n",
      "         352       0.00      0.00      0.00       165\n",
      "         353       0.50      0.02      0.03       505\n",
      "         354       0.53      0.02      0.03       506\n",
      "         355       0.56      0.07      0.13       468\n",
      "         356       0.48      0.07      0.12       914\n",
      "         357       0.75      0.03      0.07       261\n",
      "         358       0.63      0.06      0.11       662\n",
      "         359       0.70      0.02      0.03       430\n",
      "         360       0.46      0.08      0.14      1121\n",
      "         361       0.00      0.00      0.00       184\n",
      "         362       0.47      0.08      0.13      1115\n",
      "         363       0.77      0.48      0.59        69\n",
      "         364       0.69      0.41      0.51        85\n",
      "         365       0.59      0.24      0.35       245\n",
      "         366       0.58      0.36      0.45       129\n",
      "         367       0.69      0.41      0.51       105\n",
      "         368       0.79      0.51      0.62        65\n",
      "         369       0.69      0.41      0.51        85\n",
      "         370       0.73      0.44      0.55        79\n",
      "         371       0.56      0.30      0.39       197\n",
      "         372       0.70      0.40      0.51        88\n",
      "         373       0.70      0.06      0.11       562\n",
      "         374       0.49      0.25      0.33       961\n",
      "         375       0.51      0.26      0.35       676\n",
      "         376       0.51      0.28      0.36       612\n",
      "         377       0.50      0.24      0.33       946\n",
      "         378       0.49      0.24      0.32       893\n",
      "         379       0.49      0.25      0.33      1181\n",
      "         380       0.53      0.39      0.45       599\n",
      "         381       0.52      0.29      0.37       571\n",
      "         382       0.52      0.29      0.37       572\n",
      "         383       0.52      0.31      0.39       381\n",
      "         384       0.00      0.00      0.00        88\n",
      "         385       0.29      0.03      0.06       116\n",
      "         386       0.21      0.03      0.06        92\n",
      "         387       0.46      0.13      0.20       215\n",
      "         388       0.48      0.13      0.21       244\n",
      "         389       0.38      0.12      0.18       233\n",
      "         390       0.61      0.17      0.27       603\n",
      "         391       0.59      0.17      0.27       456\n",
      "         392       0.60      0.16      0.25       506\n",
      "         393       0.48      0.06      0.10       175\n",
      "         394       0.64      0.09      0.16       633\n",
      "         395       0.62      0.08      0.15       772\n",
      "         396       0.47      0.05      0.10       149\n",
      "         397       0.62      0.10      0.17       612\n",
      "         398       0.62      0.06      0.12       124\n",
      "         399       0.00      0.00      0.00       218\n",
      "         400       0.00      0.00      0.00        74\n",
      "         401       0.50      0.01      0.01       180\n",
      "         402       0.00      0.00      0.00        69\n",
      "         403       0.00      0.00      0.00       124\n",
      "         404       0.82      0.09      0.16       343\n",
      "         405       1.00      0.01      0.03        76\n",
      "         406       1.00      0.01      0.01       135\n",
      "         407       0.14      0.00      0.01       393\n",
      "         408       1.00      0.01      0.01       142\n",
      "         409       0.29      0.04      0.07       284\n",
      "         410       0.56      0.09      0.16        53\n",
      "         411       0.36      0.09      0.15        98\n",
      "         412       0.43      0.07      0.12        44\n",
      "         413       0.56      0.20      0.30       374\n",
      "         414       0.43      0.07      0.11        46\n",
      "         415       0.44      0.14      0.21       170\n",
      "         416       0.69      0.45      0.55       651\n",
      "         417       0.75      0.51      0.60        95\n",
      "         418       0.57      0.39      0.46       119\n",
      "         419       0.70      0.38      0.49       154\n",
      "         420       0.64      0.39      0.48       531\n",
      "         421       0.58      0.40      0.47       373\n",
      "         422       0.63      0.37      0.46        71\n",
      "         423       0.00      0.00      0.00        70\n",
      "         424       0.29      0.04      0.07       100\n",
      "         425       0.29      0.07      0.11        57\n",
      "         426       0.17      0.03      0.05        68\n",
      "         427       0.50      0.02      0.04       107\n",
      "         428       0.00      0.00      0.00        81\n",
      "         429       0.40      0.10      0.15       125\n",
      "         430       0.44      0.10      0.17       144\n",
      "         431       0.51      0.20      0.29       196\n",
      "         432       0.52      0.23      0.32       173\n",
      "         433       0.00      0.00      0.00       110\n",
      "         434       1.00      0.00      0.01       281\n",
      "         435       0.65      0.35      0.46       182\n",
      "         436       0.51      0.24      0.32       716\n",
      "         437       0.48      0.28      0.35       379\n",
      "         438       0.57      0.31      0.40       230\n",
      "         439       0.53      0.20      0.29       376\n",
      "         440       0.50      0.02      0.03        58\n",
      "         441       0.50      0.01      0.03        73\n",
      "         442       0.50      0.01      0.03       139\n",
      "         443       0.50      0.02      0.03        61\n",
      "         444       0.00      0.00      0.00        84\n",
      "         445       0.00      0.00      0.00        62\n",
      "         446       0.58      0.05      0.09       137\n",
      "         447       0.76      0.18      0.29       153\n",
      "         448       0.00      0.00      0.00        72\n",
      "         449       1.00      0.02      0.03        58\n",
      "         450       0.00      0.00      0.00       113\n",
      "         451       0.53      0.18      0.27       719\n",
      "         452       0.56      0.14      0.22       406\n",
      "         453       0.00      0.00      0.00       192\n",
      "         454       0.62      0.23      0.33       322\n",
      "         455       0.80      0.05      0.10       223\n",
      "         456       0.53      0.21      0.30       521\n",
      "         457       0.00      0.00      0.00        76\n",
      "         458       0.00      0.00      0.00        98\n",
      "         459       0.62      0.23      0.33       232\n",
      "         460       0.59      0.21      0.31       418\n",
      "         461       0.54      0.23      0.32       540\n",
      "         462       0.57      0.20      0.29       533\n",
      "         463       0.50      0.02      0.04       159\n",
      "         464       0.52      0.03      0.06       321\n",
      "         465       0.45      0.17      0.25       171\n",
      "         466       0.50      0.26      0.35       121\n",
      "         467       0.61      0.34      0.44       137\n",
      "         468       0.50      0.19      0.28       152\n",
      "         469       0.54      0.15      0.24       314\n",
      "         470       1.00      0.02      0.04        50\n",
      "         471       1.00      0.02      0.03        57\n",
      "         472       0.69      0.26      0.38       531\n",
      "         473       0.59      0.07      0.13       178\n",
      "         474       0.62      0.03      0.06       171\n",
      "         475       0.20      0.01      0.03        73\n",
      "         476       0.56      0.02      0.03       330\n",
      "         477       0.27      0.03      0.06       125\n",
      "         478       0.20      0.01      0.02       164\n",
      "         479       0.31      0.05      0.08       269\n",
      "         480       0.26      0.06      0.10       197\n",
      "         481       0.25      0.01      0.02       172\n",
      "         482       0.31      0.05      0.08       270\n",
      "         483       0.20      0.01      0.02       164\n",
      "         484       0.25      0.06      0.09       216\n",
      "         485       0.31      0.03      0.05       135\n",
      "         486       0.33      0.00      0.01       234\n",
      "         487       0.59      0.02      0.04       454\n",
      "         488       0.50      0.04      0.07       307\n",
      "         489       0.00      0.00      0.00       124\n",
      "         490       0.53      0.30      0.38        67\n",
      "         491       0.64      0.20      0.30        80\n",
      "         492       0.66      0.23      0.34       446\n",
      "         493       0.62      0.32      0.43       279\n",
      "         494       0.51      0.17      0.26       111\n",
      "         495       0.44      0.34      0.38       119\n",
      "         496       0.57      0.36      0.44       270\n",
      "         497       0.42      0.24      0.30        76\n",
      "         498       0.56      0.26      0.35       150\n",
      "         499       0.47      0.23      0.31        96\n",
      "         500       0.56      0.30      0.39       103\n",
      "         501       0.50      0.30      0.37       130\n",
      "         502       0.57      0.40      0.47       723\n",
      "         503       0.58      0.26      0.35       363\n",
      "         504       0.50      0.26      0.35       209\n",
      "         505       0.47      0.22      0.30        67\n",
      "         506       0.50      0.04      0.07       101\n",
      "         507       0.45      0.06      0.10       358\n",
      "         508       0.41      0.05      0.08       154\n",
      "         509       0.25      0.01      0.02        78\n",
      "         510       1.00      0.02      0.04       109\n",
      "         511       0.86      0.12      0.21       147\n",
      "         512       0.77      0.12      0.21        84\n",
      "         513       1.00      0.01      0.01       135\n",
      "         514       0.59      0.10      0.17       243\n",
      "         515       0.81      0.09      0.17       139\n",
      "         516       0.81      0.12      0.20       112\n",
      "         517       0.67      0.02      0.05       329\n",
      "         518       0.00      0.00      0.00        67\n",
      "         519       0.88      0.12      0.21       129\n",
      "         520       0.79      0.33      0.46        70\n",
      "         521       0.62      0.19      0.29       408\n",
      "         522       0.54      0.20      0.29       217\n",
      "         523       0.61      0.20      0.30       289\n",
      "         524       0.58      0.24      0.34       180\n",
      "         525       1.00      0.01      0.01       162\n",
      "         526       0.73      0.25      0.38       232\n",
      "         527       0.50      0.09      0.16       108\n",
      "         528       0.00      0.00      0.00        99\n",
      "         529       0.00      0.00      0.00        96\n",
      "         530       0.54      0.06      0.11       237\n",
      "         531       0.57      0.08      0.13       213\n",
      "         532       0.43      0.08      0.14        72\n",
      "         533       0.64      0.26      0.37       171\n",
      "         534       0.79      0.21      0.34       270\n",
      "         535       0.58      0.03      0.06       429\n",
      "         536       0.68      0.03      0.06       390\n",
      "         537       0.50      0.01      0.01       157\n",
      "         538       0.54      0.03      0.05       513\n",
      "         539       0.58      0.03      0.06       425\n",
      "         540       0.50      0.02      0.04       141\n",
      "         541       0.00      0.00      0.00        78\n",
      "         542       0.50      0.00      0.01       379\n",
      "         543       0.75      0.04      0.08       139\n",
      "         544       0.60      0.01      0.02       272\n",
      "         545       0.43      0.05      0.09        63\n",
      "         546       0.50      0.25      0.33        60\n",
      "         547       0.43      0.15      0.22        66\n",
      "         548       0.65      0.35      0.45       199\n",
      "         549       0.53      0.07      0.13       123\n",
      "         550       0.56      0.03      0.06       154\n",
      "         551       0.54      0.08      0.14       353\n",
      "         552       0.50      0.06      0.10       140\n",
      "         553       0.67      0.02      0.04        89\n",
      "         554       0.42      0.07      0.12       254\n",
      "         555       0.33      0.01      0.02       116\n",
      "         556       0.54      0.07      0.12       101\n",
      "         557       0.00      0.00      0.00       165\n",
      "         558       0.00      0.00      0.00        85\n",
      "         559       1.00      0.01      0.03        72\n",
      "         560       0.43      0.01      0.03       217\n",
      "         561       0.00      0.00      0.00       116\n",
      "         562       0.38      0.01      0.03       232\n",
      "         563       0.43      0.01      0.03       216\n",
      "         564       0.00      0.00      0.00        66\n",
      "         565       0.00      0.00      0.00       195\n",
      "         566       0.00      0.00      0.00        73\n",
      "         567       0.00      0.00      0.00        57\n",
      "         568       0.47      0.07      0.12       308\n",
      "         569       0.59      0.08      0.14       212\n",
      "         570       0.31      0.07      0.11        59\n",
      "         571       1.00      0.08      0.15        86\n",
      "         572       1.00      0.11      0.20        82\n",
      "         573       1.00      0.12      0.21        78\n",
      "         574       0.63      0.24      0.35       258\n",
      "         575       0.64      0.22      0.33       204\n",
      "         576       0.62      0.25      0.36       249\n",
      "         577       0.61      0.25      0.35       246\n",
      "         578       0.62      0.22      0.32       180\n",
      "         579       0.61      0.26      0.37       250\n",
      "         580       0.00      0.00      0.00        57\n",
      "         581       0.80      0.08      0.14       106\n",
      "         582       1.00      0.05      0.10       131\n",
      "         583       0.00      0.00      0.00       101\n",
      "         584       1.00      0.00      0.01       228\n",
      "         585       0.00      0.00      0.00        58\n",
      "         586       0.27      0.01      0.02       355\n",
      "         587       0.00      0.00      0.00        45\n",
      "         588       0.00      0.00      0.00       173\n",
      "         589       0.00      0.00      0.00        60\n",
      "         590       0.85      0.11      0.20        96\n",
      "         591       0.40      0.02      0.04       100\n",
      "         592       0.60      0.07      0.13       202\n",
      "         593       0.00      0.00      0.00        62\n",
      "         594       0.00      0.00      0.00        80\n",
      "         595       0.47      0.07      0.13       360\n",
      "         596       0.45      0.07      0.12       363\n",
      "         597       0.67      0.08      0.14       128\n",
      "         598       0.70      0.06      0.11       118\n",
      "         599       0.52      0.09      0.15       135\n",
      "         600       0.53      0.07      0.12       127\n",
      "         601       0.00      0.00      0.00       124\n",
      "         602       0.00      0.00      0.00       103\n",
      "         603       1.00      0.01      0.02       114\n",
      "         604       1.00      0.01      0.02        80\n",
      "         605       0.57      0.03      0.05       151\n",
      "         606       1.00      0.01      0.02       102\n",
      "         607       0.67      0.05      0.10       116\n",
      "         608       0.80      0.03      0.05       146\n",
      "         609       0.67      0.02      0.03       123\n",
      "         610       0.00      0.00      0.00        74\n",
      "         611       0.00      0.00      0.00        96\n",
      "         612       0.00      0.00      0.00       101\n",
      "         613       1.00      0.01      0.01       179\n",
      "         614       1.00      0.01      0.01       193\n",
      "         615       0.00      0.00      0.00        96\n",
      "         616       1.00      0.01      0.01       172\n",
      "         617       1.00      0.00      0.01       204\n",
      "         618       1.00      0.01      0.02        83\n",
      "         619       0.33      0.02      0.03       471\n",
      "         620       0.00      0.00      0.00        72\n",
      "         621       0.63      0.18      0.28       227\n",
      "         622       0.56      0.21      0.31       333\n",
      "         623       0.29      0.06      0.10       132\n",
      "         624       0.61      0.16      0.25       297\n",
      "         625       0.61      0.21      0.32       573\n",
      "         626       0.67      0.08      0.14        51\n",
      "         627       0.50      0.01      0.02       175\n",
      "         628       0.61      0.15      0.24       339\n",
      "         629       0.67      0.09      0.15        47\n",
      "         630       0.50      0.02      0.04       143\n",
      "         631       0.60      0.02      0.03       173\n",
      "         632       0.20      0.01      0.01       158\n",
      "         633       0.60      0.10      0.18        86\n",
      "         634       0.47      0.06      0.11       115\n",
      "         635       0.48      0.05      0.09       233\n",
      "         636       0.50      0.01      0.03        67\n",
      "         637       0.00      0.00      0.00        78\n",
      "         638       0.00      0.00      0.00       200\n",
      "         639       0.00      0.00      0.00        52\n",
      "         640       0.25      0.01      0.01       165\n",
      "         641       0.25      0.01      0.01       166\n",
      "         642       0.00      0.00      0.00        58\n",
      "         643       0.82      0.03      0.05       350\n",
      "         644       0.00      0.00      0.00        86\n",
      "         645       0.67      0.03      0.05       143\n",
      "         646       0.00      0.00      0.00        65\n",
      "         647       0.54      0.18      0.26       228\n",
      "         648       0.68      0.23      0.35       254\n",
      "         649       0.72      0.16      0.27       158\n",
      "         650       0.68      0.13      0.22       188\n",
      "         651       0.47      0.07      0.12       100\n",
      "         652       0.46      0.06      0.11        94\n",
      "         653       0.65      0.39      0.48       158\n",
      "         654       1.00      0.02      0.03        66\n",
      "         655       0.17      0.00      0.01       325\n",
      "         656       0.69      0.26      0.38       156\n",
      "         657       0.69      0.42      0.52       120\n",
      "         658       0.63      0.18      0.28       178\n",
      "         659       0.64      0.39      0.49        99\n",
      "         660       0.67      0.43      0.52       121\n",
      "         661       0.63      0.38      0.48       102\n",
      "         662       0.69      0.26      0.37       129\n",
      "         663       0.65      0.45      0.53       137\n",
      "         664       0.64      0.44      0.52       150\n",
      "         665       0.65      0.38      0.48       119\n",
      "         666       0.40      0.02      0.04        87\n",
      "         667       0.18      0.02      0.03       169\n",
      "         668       0.73      0.11      0.20        71\n",
      "         669       0.00      0.00      0.00        95\n",
      "         670       1.00      0.04      0.08       101\n",
      "         671       0.50      0.03      0.05       120\n",
      "         672       0.67      0.04      0.07        57\n",
      "         673       1.00      0.04      0.07        57\n",
      "         674       0.00      0.00      0.00       178\n",
      "         675       0.59      0.09      0.16       145\n",
      "         676       0.53      0.07      0.12       131\n",
      "         677       0.53      0.07      0.12       131\n",
      "         678       0.56      0.07      0.13       135\n",
      "         679       0.70      0.11      0.20        61\n",
      "         680       0.50      0.06      0.11        81\n",
      "         681       0.64      0.11      0.19        62\n",
      "         682       0.62      0.26      0.37       173\n",
      "         683       0.63      0.23      0.34       308\n",
      "         684       0.48      0.08      0.14       125\n",
      "         685       0.57      0.07      0.13       109\n",
      "         686       0.64      0.08      0.14        90\n",
      "         687       0.50      0.08      0.14       125\n",
      "         688       0.50      0.06      0.11       185\n",
      "         689       0.50      0.03      0.05        77\n",
      "         690       0.33      0.02      0.04        97\n",
      "         691       0.59      0.26      0.36       135\n",
      "         692       0.60      0.30      0.40       135\n",
      "         693       0.57      0.33      0.42       170\n",
      "         694       0.60      0.06      0.10       109\n",
      "         695       1.00      0.02      0.03        57\n",
      "         696       0.00      0.00      0.00        59\n",
      "         697       0.50      0.01      0.02       397\n",
      "         698       0.00      0.00      0.00        81\n",
      "         699       0.60      0.18      0.28       221\n",
      "         700       0.60      0.25      0.35       146\n",
      "         701       0.69      0.13      0.23        67\n",
      "         702       0.60      0.12      0.20       256\n",
      "         703       0.81      0.25      0.38       104\n",
      "         704       0.76      0.25      0.38       104\n",
      "         705       0.33      0.04      0.07        52\n",
      "         706       0.43      0.06      0.10        52\n",
      "         707       0.31      0.06      0.10        70\n",
      "         708       0.00      0.00      0.00        61\n",
      "         709       1.00      0.01      0.02        89\n",
      "         710       0.50      0.00      0.01       229\n",
      "         711       0.00      0.00      0.00        81\n",
      "         712       0.76      0.28      0.41       132\n",
      "         713       0.00      0.00      0.00        60\n",
      "         714       0.00      0.00      0.00        92\n",
      "         715       0.00      0.00      0.00       152\n",
      "         716       0.67      0.01      0.03       285\n",
      "         717       0.00      0.00      0.00        57\n",
      "         718       0.00      0.00      0.00        60\n",
      "         719       0.00      0.00      0.00        92\n",
      "         720       0.52      0.07      0.13       233\n",
      "         721       0.00      0.00      0.00        76\n",
      "         722       0.00      0.00      0.00        77\n",
      "         723       0.67      0.09      0.15        69\n",
      "         724       0.75      0.26      0.39       354\n",
      "         725       0.73      0.47      0.58       200\n",
      "         726       0.74      0.54      0.62       176\n",
      "         727       0.76      0.41      0.53       235\n",
      "         728       0.74      0.37      0.50       259\n",
      "         729       0.70      0.40      0.51        65\n",
      "         730       0.57      0.06      0.11        65\n",
      "         731       0.57      0.06      0.12        62\n",
      "         732       0.53      0.08      0.13       118\n",
      "         733       0.65      0.08      0.14       170\n",
      "         734       1.00      0.01      0.02        80\n",
      "         735       0.57      0.05      0.10        73\n",
      "         736       0.53      0.08      0.14       102\n",
      "         737       0.60      0.04      0.07       171\n",
      "         738       0.52      0.13      0.21       100\n",
      "         739       0.60      0.07      0.13        80\n",
      "         740       0.44      0.02      0.03       227\n",
      "         741       0.55      0.57      0.56       103\n",
      "         742       0.52      0.45      0.48       235\n",
      "         743       0.51      0.42      0.46       179\n",
      "         744       0.56      0.41      0.47        90\n",
      "         745       0.50      0.35      0.41        71\n",
      "         746       0.63      0.39      0.48       109\n",
      "         747       0.53      0.33      0.41        69\n",
      "         748       0.63      0.42      0.50        86\n",
      "         749       0.51      0.34      0.41        59\n",
      "         750       0.49      0.32      0.39        59\n",
      "         751       0.00      0.00      0.00        58\n",
      "         752       0.67      0.03      0.05       153\n",
      "         753       0.78      0.04      0.07       181\n",
      "         754       0.60      0.07      0.12        46\n",
      "         755       0.67      0.01      0.02       230\n",
      "         756       0.00      0.00      0.00        65\n",
      "         757       1.00      0.01      0.02       123\n",
      "         758       0.00      0.00      0.00        80\n",
      "         759       0.00      0.00      0.00        52\n",
      "         760       0.43      0.02      0.04       153\n",
      "         761       0.55      0.07      0.12       169\n",
      "         762       0.44      0.03      0.05       158\n",
      "         763       0.00      0.00      0.00        79\n",
      "         764       0.67      0.05      0.09        79\n",
      "         765       1.00      0.03      0.06        62\n",
      "         766       0.50      0.03      0.06        94\n",
      "         767       0.51      0.11      0.18       248\n",
      "         768       0.57      0.13      0.21       288\n",
      "         769       0.62      0.11      0.19       165\n",
      "         770       0.31      0.02      0.04       165\n",
      "         771       0.20      0.01      0.01       156\n",
      "         772       0.43      0.01      0.01       413\n",
      "         773       0.00      0.00      0.00       104\n",
      "         774       0.00      0.00      0.00       106\n",
      "         775       0.40      0.01      0.03       155\n",
      "         776       0.36      0.03      0.06       131\n",
      "         777       1.00      0.05      0.10        57\n",
      "         778       0.80      0.05      0.10        76\n",
      "         779       0.75      0.05      0.09        62\n",
      "         780       0.50      0.02      0.03        64\n",
      "         781       0.50      0.04      0.08        71\n",
      "         782       0.70      0.22      0.33       139\n",
      "         783       0.70      0.22      0.33       137\n",
      "         784       0.00      0.00      0.00       113\n",
      "         785       0.25      0.06      0.09        70\n",
      "         786       0.33      0.01      0.03        69\n",
      "         787       0.25      0.05      0.09        96\n",
      "         788       0.83      0.04      0.07       132\n",
      "         789       0.50      0.35      0.41       241\n",
      "         790       0.72      0.29      0.41       108\n",
      "         791       0.51      0.36      0.42       240\n",
      "         792       0.47      0.30      0.36       374\n",
      "         793       1.00      0.01      0.03       225\n",
      "         794       0.33      0.02      0.04       214\n",
      "         795       0.44      0.03      0.06       231\n",
      "         796       0.00      0.00      0.00        90\n",
      "         797       0.00      0.00      0.00       119\n",
      "         798       0.00      0.00      0.00        92\n",
      "         799       0.00      0.00      0.00       169\n",
      "         800       0.00      0.00      0.00        72\n",
      "         801       0.00      0.00      0.00       169\n",
      "         802       0.00      0.00      0.00        95\n",
      "         803       0.00      0.00      0.00        71\n",
      "         804       0.00      0.00      0.00        47\n",
      "         805       0.00      0.00      0.00        51\n",
      "         806       1.00      0.01      0.01       146\n",
      "         807       0.00      0.00      0.00        81\n",
      "         808       1.00      0.01      0.01       165\n",
      "         809       1.00      0.01      0.02       126\n",
      "         810       0.25      0.06      0.10        48\n",
      "         811       0.39      0.24      0.30        88\n",
      "         812       0.65      0.23      0.34        74\n",
      "         813       0.33      0.02      0.03        64\n",
      "         814       0.68      0.19      0.29       101\n",
      "         815       0.63      0.24      0.35        70\n",
      "         816       0.55      0.20      0.29        61\n",
      "         817       0.00      0.00      0.00        66\n",
      "         818       0.60      0.15      0.24        60\n",
      "         819       0.91      0.43      0.58        68\n",
      "         820       0.76      0.22      0.34        86\n",
      "         821       0.76      0.20      0.32        93\n",
      "         822       0.67      0.04      0.08        89\n",
      "         823       0.67      0.04      0.08        89\n",
      "         824       0.68      0.29      0.41       102\n",
      "         825       0.69      0.27      0.39       122\n",
      "         826       0.00      0.00      0.00        63\n",
      "         827       0.25      0.01      0.01       159\n",
      "         828       0.00      0.00      0.00        63\n",
      "         829       0.36      0.17      0.23        52\n",
      "         830       0.00      0.00      0.00        64\n",
      "         831       0.00      0.00      0.00        63\n",
      "         832       0.50      0.11      0.18        92\n",
      "         833       0.55      0.04      0.08       136\n",
      "         834       0.00      0.00      0.00        60\n",
      "         835       0.80      0.05      0.09        83\n",
      "         836       0.38      0.18      0.25        71\n",
      "         837       0.63      0.27      0.38       182\n",
      "         838       0.59      0.23      0.33       116\n",
      "         839       0.66      0.22      0.32        88\n",
      "         840       0.67      0.21      0.32        67\n",
      "         841       0.73      0.13      0.22       126\n",
      "         842       0.73      0.13      0.22        85\n",
      "         843       0.71      0.11      0.20        87\n",
      "         844       0.42      0.25      0.31        57\n",
      "         845       0.69      0.36      0.47        70\n",
      "         846       0.00      0.00      0.00        66\n",
      "         847       0.00      0.00      0.00        66\n",
      "         848       0.67      0.02      0.04       104\n",
      "         849       0.33      0.01      0.01       184\n",
      "         850       0.52      0.34      0.41       137\n",
      "         851       0.55      0.31      0.39       172\n",
      "         852       0.45      0.36      0.40        81\n",
      "         853       0.49      0.27      0.35        99\n",
      "         854       0.00      0.00      0.00        79\n",
      "         855       0.67      0.04      0.07       105\n",
      "         856       0.67      0.03      0.06       118\n",
      "         857       0.00      0.00      0.00        62\n",
      "         858       0.00      0.00      0.00        65\n",
      "         859       0.40      0.16      0.22       160\n",
      "         860       0.00      0.00      0.00        53\n",
      "         861       0.33      0.07      0.12        55\n",
      "         862       0.00      0.00      0.00        53\n",
      "         863       0.14      0.02      0.03        64\n",
      "         864       0.50      0.12      0.19       158\n",
      "         865       0.49      0.12      0.19       208\n",
      "         866       0.30      0.05      0.09        55\n",
      "         867       1.00      0.04      0.08        48\n",
      "         868       1.00      0.03      0.05        73\n",
      "         869       0.00      0.00      0.00        92\n",
      "         870       0.75      0.05      0.09        62\n",
      "         871       0.00      0.00      0.00        69\n",
      "         872       1.00      0.02      0.04        53\n",
      "         873       0.00      0.00      0.00       110\n",
      "         874       0.00      0.00      0.00        63\n",
      "         875       0.00      0.00      0.00        56\n",
      "         876       0.00      0.00      0.00       104\n",
      "         877       1.00      0.01      0.02        83\n",
      "         878       0.20      0.02      0.04        52\n",
      "         879       1.00      0.01      0.02       117\n",
      "         880       0.00      0.00      0.00        62\n",
      "         881       0.00      0.00      0.00        66\n",
      "         882       0.40      0.02      0.03       225\n",
      "         883       0.40      0.02      0.04       218\n",
      "         884       0.00      0.00      0.00        55\n",
      "         885       0.50      0.03      0.06        60\n",
      "         886       0.00      0.00      0.00        62\n",
      "         887       0.62      0.05      0.10        94\n",
      "         888       0.33      0.01      0.02       106\n",
      "         889       0.64      0.05      0.09       143\n",
      "         890       0.62      0.06      0.11       132\n",
      "         891       1.00      0.02      0.04        54\n",
      "         892       0.40      0.03      0.05       233\n",
      "         893       0.60      0.02      0.04       258\n",
      "         894       0.53      0.51      0.52        92\n",
      "         895       1.00      0.01      0.02        95\n",
      "         896       1.00      0.02      0.03        63\n",
      "         897       0.00      0.00      0.00        60\n",
      "         898       0.60      0.11      0.19        82\n",
      "         899       0.56      0.35      0.43        57\n",
      "         900       0.00      0.00      0.00       145\n",
      "         901       0.00      0.00      0.00       129\n",
      "         902       0.62      0.07      0.13        69\n",
      "         903       0.56      0.44      0.49       187\n",
      "         904       0.59      0.10      0.17       131\n",
      "         905       0.40      0.04      0.07        53\n",
      "         906       0.56      0.06      0.11       169\n",
      "         907       0.57      0.10      0.18       125\n",
      "         908       0.50      0.07      0.13        82\n",
      "         909       0.56      0.04      0.08       118\n",
      "         910       0.71      0.06      0.11        82\n",
      "         911       0.80      0.05      0.10        74\n",
      "         912       0.00      0.00      0.00        54\n",
      "         913       0.40      0.01      0.02       156\n",
      "         914       0.50      0.03      0.06        65\n",
      "         915       0.53      0.17      0.26       141\n",
      "         916       0.41      0.13      0.19       103\n",
      "         917       0.58      0.20      0.29       213\n",
      "         918       0.44      0.15      0.23        92\n",
      "         919       0.00      0.00      0.00        80\n",
      "         920       0.46      0.25      0.32        68\n",
      "         921       0.49      0.24      0.32        71\n",
      "         922       0.44      0.22      0.29        78\n",
      "         923       0.64      0.25      0.36       177\n",
      "         924       0.48      0.24      0.32        67\n",
      "         925       0.52      0.26      0.35        91\n",
      "         926       0.54      0.19      0.28       141\n",
      "         927       0.51      0.21      0.29       112\n",
      "         928       0.53      0.19      0.28       111\n",
      "         929       0.60      0.18      0.28       160\n",
      "         930       0.51      0.19      0.28       100\n",
      "         931       0.49      0.25      0.33        93\n",
      "         932       0.64      0.25      0.36       173\n",
      "         933       0.46      0.20      0.28        60\n",
      "         934       0.38      0.11      0.17        72\n",
      "         935       0.00      0.00      0.00        60\n",
      "         936       0.60      0.04      0.07        78\n",
      "         937       1.00      0.04      0.08        95\n",
      "         938       0.00      0.00      0.00        55\n",
      "         939       0.00      0.00      0.00        56\n",
      "         940       0.75      0.05      0.09        63\n",
      "         941       0.59      0.28      0.38        67\n",
      "         942       0.08      0.02      0.03        65\n",
      "         943       0.56      0.31      0.40       159\n",
      "         944       0.58      0.33      0.42       183\n",
      "         945       0.55      0.29      0.38       163\n",
      "         946       0.44      0.20      0.28        88\n",
      "         947       0.51      0.25      0.34       102\n",
      "         948       0.56      0.32      0.41       156\n",
      "         949       0.00      0.00      0.00       136\n",
      "         950       0.00      0.00      0.00        75\n",
      "         951       0.00      0.00      0.00       140\n",
      "         952       0.25      0.02      0.04        94\n",
      "         953       0.33      0.04      0.06        57\n",
      "         954       1.00      0.01      0.03        72\n",
      "         955       1.00      0.01      0.03        68\n",
      "         956       0.25      0.01      0.02        93\n",
      "         957       0.33      0.01      0.03        76\n",
      "         958       0.29      0.03      0.05        72\n",
      "         959       0.33      0.03      0.05        74\n",
      "         960       0.50      0.01      0.03        68\n",
      "         961       0.00      0.00      0.00        60\n",
      "         962       0.20      0.01      0.01       129\n",
      "         963       0.00      0.00      0.00        60\n",
      "         964       0.00      0.00      0.00        71\n",
      "         965       0.00      0.00      0.00        66\n",
      "         966       0.00      0.00      0.00        78\n",
      "         967       0.00      0.00      0.00        67\n",
      "         968       1.00      0.01      0.03        73\n",
      "         969       0.20      0.02      0.03        54\n",
      "         970       1.00      0.07      0.13        58\n",
      "         971       0.41      0.13      0.20        53\n",
      "         972       0.49      0.38      0.43        86\n",
      "         973       0.48      0.36      0.41        76\n",
      "         974       0.00      0.00      0.00        66\n",
      "         975       1.00      0.01      0.02       229\n",
      "         976       0.00      0.00      0.00        49\n",
      "         977       0.25      0.02      0.03        60\n",
      "         978       0.70      0.08      0.15        85\n",
      "         979       0.00      0.00      0.00        55\n",
      "         980       0.67      0.04      0.08        45\n",
      "         981       0.00      0.00      0.00        71\n",
      "         982       0.53      0.11      0.18        72\n",
      "         983       0.00      0.00      0.00        61\n",
      "         984       0.67      0.03      0.06        67\n",
      "         985       0.44      0.15      0.23       195\n",
      "         986       0.41      0.16      0.23       178\n",
      "         987       0.41      0.17      0.24       163\n",
      "         988       0.41      0.20      0.26       143\n",
      "         989       0.50      0.02      0.03       171\n",
      "         990       0.30      0.03      0.06        95\n",
      "         991       0.00      0.00      0.00        83\n",
      "         992       0.65      0.25      0.36        53\n",
      "         993       0.42      0.11      0.18       122\n",
      "         994       0.38      0.22      0.28       100\n",
      "         995       0.45      0.21      0.29        72\n",
      "         996       0.50      0.16      0.24        57\n",
      "         997       0.50      0.18      0.27        60\n",
      "         998       0.00      0.00      0.00        98\n",
      "         999       0.00      0.00      0.00        47\n",
      "        1000       0.00      0.00      0.00        57\n",
      "        1001       0.00      0.00      0.00        78\n",
      "        1002       0.17      0.01      0.02        94\n",
      "        1003       0.00      0.00      0.00        89\n",
      "        1004       0.71      0.06      0.11        82\n",
      "        1005       0.33      0.02      0.04        52\n",
      "        1006       0.58      0.18      0.28       169\n",
      "        1007       0.40      0.09      0.14        69\n",
      "        1008       0.58      0.19      0.29       134\n",
      "        1009       0.44      0.10      0.16        84\n",
      "        1010       0.44      0.10      0.16        71\n",
      "        1011       0.43      0.12      0.19        75\n",
      "        1012       0.00      0.00      0.00       109\n",
      "        1013       0.00      0.00      0.00        59\n",
      "        1014       0.00      0.00      0.00        59\n",
      "        1015       1.00      0.03      0.06        69\n",
      "        1016       0.92      0.10      0.19       105\n",
      "        1017       0.00      0.00      0.00        81\n",
      "        1018       0.00      0.00      0.00        98\n",
      "        1019       0.00      0.00      0.00        84\n",
      "        1020       0.00      0.00      0.00        56\n",
      "        1021       0.33      0.02      0.03       113\n",
      "        1022       0.17      0.01      0.02       106\n",
      "        1023       0.36      0.13      0.19        94\n",
      "        1024       0.33      0.02      0.04        48\n",
      "        1025       0.29      0.03      0.06        60\n",
      "        1026       1.00      0.04      0.08        47\n",
      "        1027       0.54      0.09      0.16        75\n",
      "        1028       0.40      0.04      0.07       105\n",
      "        1029       0.00      0.00      0.00        76\n",
      "        1030       0.00      0.00      0.00       117\n",
      "        1031       0.00      0.00      0.00        67\n",
      "        1032       0.00      0.00      0.00        65\n",
      "        1033       0.53      0.10      0.16        94\n",
      "        1034       0.00      0.00      0.00        66\n",
      "        1035       0.00      0.00      0.00        52\n",
      "        1036       0.19      0.05      0.08        55\n",
      "        1037       1.00      0.01      0.03        67\n",
      "        1038       0.56      0.08      0.14        60\n",
      "        1039       1.00      0.04      0.07        57\n",
      "        1040       0.33      0.02      0.04       140\n",
      "        1041       0.00      0.00      0.00        51\n",
      "        1042       0.25      0.02      0.04        52\n",
      "        1043       0.43      0.06      0.11        50\n",
      "        1044       0.00      0.00      0.00        51\n",
      "        1045       0.45      0.25      0.33        55\n",
      "        1046       0.41      0.24      0.31        49\n",
      "        1047       1.00      0.01      0.02       116\n",
      "        1048       0.50      0.08      0.14       108\n",
      "        1049       0.43      0.05      0.08        65\n",
      "        1050       0.36      0.07      0.12        56\n",
      "        1051       0.47      0.12      0.19        59\n",
      "        1052       0.53      0.08      0.14       128\n",
      "        1053       0.47      0.15      0.23        54\n",
      "        1054       0.53      0.10      0.16        93\n",
      "        1055       0.50      0.11      0.18        64\n",
      "        1056       0.00      0.00      0.00        66\n",
      "        1057       0.54      0.16      0.24       134\n",
      "        1058       0.00      0.00      0.00        79\n",
      "        1059       1.00      0.05      0.09        62\n",
      "        1060       0.00      0.00      0.00        67\n",
      "        1061       0.00      0.00      0.00        73\n",
      "        1062       0.48      0.17      0.26        69\n",
      "        1063       0.45      0.19      0.27        69\n",
      "        1064       0.00      0.00      0.00        59\n",
      "        1065       0.00      0.00      0.00        89\n",
      "        1066       0.00      0.00      0.00        66\n",
      "        1067       0.47      0.15      0.23       115\n",
      "        1068       0.12      0.02      0.04        47\n",
      "        1069       0.58      0.17      0.26        65\n",
      "        1070       0.61      0.17      0.27        64\n",
      "        1071       0.67      0.03      0.06        69\n",
      "        1072       0.50      0.03      0.05        76\n",
      "        1073       0.50      0.04      0.08        71\n",
      "        1074       0.00      0.00      0.00        62\n",
      "        1075       0.00      0.00      0.00        70\n",
      "        1076       0.00      0.00      0.00        77\n",
      "        1077       0.00      0.00      0.00        66\n",
      "        1078       0.50      0.01      0.02       106\n",
      "        1079       0.00      0.00      0.00        79\n",
      "        1080       0.50      0.03      0.06        63\n",
      "        1081       0.75      0.05      0.10        56\n",
      "        1082       0.20      0.01      0.03        69\n",
      "        1083       0.33      0.06      0.10        72\n",
      "        1084       0.00      0.00      0.00        62\n",
      "        1085       0.00      0.00      0.00        64\n",
      "        1086       0.36      0.08      0.12        53\n",
      "        1087       0.00      0.00      0.00        80\n",
      "        1088       0.67      0.04      0.07        51\n",
      "        1089       0.60      0.05      0.10        55\n",
      "        1090       0.38      0.08      0.13        62\n",
      "        1091       0.00      0.00      0.00        51\n",
      "        1092       0.56      0.09      0.15        57\n",
      "        1093       0.00      0.00      0.00        75\n",
      "        1094       0.50      0.04      0.07        78\n",
      "        1095       0.43      0.06      0.10        53\n",
      "        1096       0.00      0.00      0.00        50\n",
      "        1097       0.00      0.00      0.00        48\n",
      "        1098       0.00      0.00      0.00        47\n",
      "        1099       0.00      0.00      0.00        65\n",
      "\n",
      "   micro avg       0.64      0.30      0.41    368894\n",
      "   macro avg       0.46      0.12      0.17    368894\n",
      "weighted avg       0.57      0.30      0.35    368894\n",
      " samples avg       0.66      0.35      0.40    368894\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 635us/step\n"
     ]
    }
   ],
   "source": [
    "### Prediction using sample test\n",
    "y_pred_gt = model.predict(X_test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = generate_submission_df(y_pred_gt, test_ids, pred_columns)\n",
    "submission_df.to_csv('./dataset/prediction/sample_prediction_bp.tsv', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non propagated predictions\n",
    "# File paths\n",
    "bp = './dataset/prediction/sample_prediction_bp.tsv'\n",
    "mf = './dataset/prediction/sample_prediction_mf.tsv'\n",
    "cc = './dataset/prediction/sample_prediction_cc.tsv'\n",
    "\n",
    "concat_df = concat_predictions(bp, mf, cc)\n",
    "\n",
    "# Output the result as a TSV file\n",
    "output_file = './dataset/prediction/sample_test_pred_all.tsv'\n",
    "concat_df.to_csv(output_file, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using CAFA Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval, write_results\n",
    "res = cafa_eval(\"./dataset/taxonomy/go-basic.obo\", \"./dataset/prediction/pred_all/\", \"./dataset/test/sampled_gt.tsv\")\n",
    "write_results(*res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
