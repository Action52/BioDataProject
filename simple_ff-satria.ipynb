{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6HwIe3gAKWIy",
    "outputId": "a55c1a5d-f4b2-472d-ebee-f753e1fc5415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iopNSsqKebZ",
    "outputId": "ae0f0a09-7cac-4756-d927-4fc086a89b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Uni/UniPD/BioData/project/biological_data_pfp\n"
     ]
    }
   ],
   "source": [
    "cd 'drive/MyDrive/Uni/UniPD/BioData/project/biological_data_pfp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "r8NK2V60KguN"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx\n",
    "import hashlib\n",
    "import obonet\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Hbjhqs_MKkM6"
   },
   "outputs": [],
   "source": [
    "def readh5_to_dict(file_path):\n",
    "  # Create an empty dictionary to store the data\n",
    "  p_embeddings_data = {}\n",
    "\n",
    "  # Open the HDF5 file\n",
    "  with h5py.File(file_path, 'r') as p_embeddings:\n",
    "    # Store the data in the dictionary\n",
    "    for key in p_embeddings.keys():\n",
    "      p_embeddings_data[key] = p_embeddings[key][...]\n",
    "\n",
    "  return p_embeddings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "RjTDFHv5K2j4"
   },
   "outputs": [],
   "source": [
    "def sample_protein_ids(file_path,percentage):\n",
    "\n",
    "  # Read the IDs from the text file\n",
    "  with open(file_path, 'r') as file:\n",
    "    ids = [line.strip() for line in file]\n",
    "\n",
    "  # Calculate the index to get the first 30% of IDs\n",
    "  split_index = int(len(ids) * percentage)\n",
    "\n",
    "  # Select the first 30% of IDs\n",
    "  selected_ids = ids[:split_index]\n",
    "\n",
    "  return selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Y1i9yPHqLdUh"
   },
   "outputs": [],
   "source": [
    "def read_tsv(tsv_file_path):\n",
    "  # Read the TSV file into a Pandas DataFrame\n",
    "  df_train_set = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "  # Display the DataFrame\n",
    "  return df_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "62-TWs4OOTCJ"
   },
   "outputs": [],
   "source": [
    "def read_dat(file_path):\n",
    "  column_names = ['Protein_ID', 'IPR_ID', 'description', 'domain','dc1','dc2']\n",
    "  df = pd.read_csv(file_path, delimiter='\\t',names=column_names)\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y71QKlmMMHuo"
   },
   "outputs": [],
   "source": [
    "def filter_train_data(df, selected_ids, category):\n",
    "  filtered_df = df[df['Protein_ID'].isin(selected_ids)]\n",
    "  filtered_df = filtered_df[filtered_df['aspect'] == category]\n",
    "\n",
    "  return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-HglxHVZN2dy"
   },
   "outputs": [],
   "source": [
    "def encode_go_terms(train_df):\n",
    "  one_hot_encoding = pd.get_dummies(train_df['GO_term'])\n",
    "\n",
    "  # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "  df_encoded = pd.concat([train_df, one_hot_encoding], axis=1)\n",
    "  df_encoded_grouped = df_encoded.groupby('Protein_ID').sum().reset_index()\n",
    "\n",
    "  return df_encoded_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VL4CJoenW_G6"
   },
   "outputs": [],
   "source": [
    "def encode_ipr_domain(df_ipr):\n",
    "    df_ipr = df_ipr.drop(columns=['IPR_ID', 'description','dc1','dc2'])\n",
    "    one_hot_encoding = pd.get_dummies(df_ipr['domain'],sparse=True)\n",
    "\n",
    "    # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "    df_encoded = pd.concat([df_ipr, one_hot_encoding], axis=1)\n",
    "    df_encoded_grouped = df_encoded.groupby('Protein_ID').sum().reset_index()\n",
    "\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "0hnJggsuL3XU"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(df, embeddings_dict):\n",
    "  df['embedding'] = df['Protein_ID'].map(embeddings_dict)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mSuC-w8ITSSG"
   },
   "outputs": [],
   "source": [
    "def get_ipr(df_ipr,df_train):\n",
    "   isp_dict = df_ipr.set_index('Protein_ID')['domain'].to_dict()\n",
    "   df_train['ipr'] = df_train['Protein_ID'].map(isp_dict)\n",
    "\n",
    "   return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "qcA4jvHAbkQp"
   },
   "outputs": [],
   "source": [
    "def create_y(df):\n",
    "  y = df.to_numpy()\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Z2pra8kYb30E"
   },
   "outputs": [],
   "source": [
    "def create_X(df,variables):\n",
    "  X = np.array(df[variables])\n",
    "  X = np.vstack(X)\n",
    "\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_freq(column, freq):\n",
    "    top_freq = []\n",
    "    for col in column:\n",
    "        if col not in freq:\n",
    "            continue\n",
    "        top_freq.append(col)\n",
    "    return top_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_array_key(array, context):\n",
    "    return (array.tostring(), array.shape, array.dtype, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prot_emd_pair(embeddings):\n",
    "    return {make_array_key(v): k for k, v in embeddings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uxHe-ugrM2m1"
   },
   "outputs": [],
   "source": [
    "p_embeddings_data = readh5_to_dict('./dataset/train/train_embeddings.h5')\n",
    "selected_ids = sample_protein_ids('./dataset/train/train_ids.txt', 1.0)\n",
    "df_train_set = read_tsv('./dataset/train/train_set.tsv')\n",
    "df_ipr = read_dat('./dataset/train/train_protein2ipr.dat')\n",
    "df_train_set_filter = filter_train_data(df_train_set, selected_ids,'cellular_component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19482/3174157729.py:2: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  return (array.tostring(), array.shape, array.dtype)\n"
     ]
    }
   ],
   "source": [
    "test_embeddings_data = readh5_to_dict('./dataset/test/test_embeddings.h5')\n",
    "test_ids = sample_protein_ids('./dataset/test/test_ids.txt', 1.0)\n",
    "emb_prot_dat = get_prot_emd_pair(test_embeddings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for id in test_ids:\n",
    "    X_test.append(test_embeddings_data[id])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "t_RkvYn7X3oo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ipr_encoded = encode_ipr_domain(df_ipr)\n",
    "#df_ipr_encoded.head()\n",
    "len(emb_prot_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "d0Y7tqbPNgU2",
    "outputId": "680b45a5-a649-454f-a1df-a8ba2f6702e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein_ID</th>\n",
       "      <th>aspect</th>\n",
       "      <th>GO_term</th>\n",
       "      <th>GO:0000118</th>\n",
       "      <th>GO:0000123</th>\n",
       "      <th>GO:0000124</th>\n",
       "      <th>GO:0000131</th>\n",
       "      <th>GO:0000137</th>\n",
       "      <th>GO:0000138</th>\n",
       "      <th>GO:0000139</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:1905360</th>\n",
       "      <th>GO:1905368</th>\n",
       "      <th>GO:1905369</th>\n",
       "      <th>GO:1990023</th>\n",
       "      <th>GO:1990204</th>\n",
       "      <th>GO:1990234</th>\n",
       "      <th>GO:1990351</th>\n",
       "      <th>GO:1990752</th>\n",
       "      <th>GO:1990904</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A021WW32</td>\n",
       "      <td>cellular_componentcellular_componentcellular_c...</td>\n",
       "      <td>GO:0005575GO:0110165GO:0000785GO:0032991GO:000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.01643, -0.001583, 0.00389, 0.0734, 0.01243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A021WZA4</td>\n",
       "      <td>cellular_componentcellular_componentcellular_c...</td>\n",
       "      <td>GO:0005575GO:0110165GO:0071944GO:0005886GO:001...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.007904, 0.0877, -0.001715, 0.03766, 0.01788...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A023GPJ3</td>\n",
       "      <td>cellular_componentcellular_componentcellular_c...</td>\n",
       "      <td>GO:0005575GO:0110165GO:0005622GO:0005829GO:000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01512, 0.01102, 0.0217, -0.02512, 0.0396, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A023GUT0</td>\n",
       "      <td>cellular_componentcellular_componentcellular_c...</td>\n",
       "      <td>GO:0005575GO:0110165GO:0005576GO:0005615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.00414, -0.01288, 0.0716, 0.01605, -0.03983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A023IM54</td>\n",
       "      <td>cellular_componentcellular_componentcellular_c...</td>\n",
       "      <td>GO:0005575GO:0005737GO:0042175GO:0032991GO:000...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.01651, 0.02525, 0.04333, 0.01558, -0.01678...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protein_ID                                             aspect  \\\n",
       "0  A0A021WW32  cellular_componentcellular_componentcellular_c...   \n",
       "1  A0A021WZA4  cellular_componentcellular_componentcellular_c...   \n",
       "2  A0A023GPJ3  cellular_componentcellular_componentcellular_c...   \n",
       "3  A0A023GUT0  cellular_componentcellular_componentcellular_c...   \n",
       "4  A0A023IM54  cellular_componentcellular_componentcellular_c...   \n",
       "\n",
       "                                             GO_term  GO:0000118  GO:0000123  \\\n",
       "0  GO:0005575GO:0110165GO:0000785GO:0032991GO:000...           0           0   \n",
       "1  GO:0005575GO:0110165GO:0071944GO:0005886GO:001...           0           0   \n",
       "2  GO:0005575GO:0110165GO:0005622GO:0005829GO:000...           0           0   \n",
       "3           GO:0005575GO:0110165GO:0005576GO:0005615           0           0   \n",
       "4  GO:0005575GO:0005737GO:0042175GO:0032991GO:000...           0           0   \n",
       "\n",
       "   GO:0000124  GO:0000131  GO:0000137  GO:0000138  GO:0000139  ...  \\\n",
       "0           0           0           0           0           0  ...   \n",
       "1           0           0           0           0           0  ...   \n",
       "2           0           0           0           0           0  ...   \n",
       "3           0           0           0           0           0  ...   \n",
       "4           0           0           0           0           0  ...   \n",
       "\n",
       "   GO:1905360  GO:1905368  GO:1905369  GO:1990023  GO:1990204  GO:1990234  \\\n",
       "0           0           0           0           0           0           0   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           0   \n",
       "\n",
       "   GO:1990351  GO:1990752  GO:1990904  \\\n",
       "0           0           0           0   \n",
       "1           0           0           0   \n",
       "2           0           0           0   \n",
       "3           0           0           0   \n",
       "4           0           0           0   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.01643, -0.001583, 0.00389, 0.0734, 0.01243...  \n",
       "1  [0.007904, 0.0877, -0.001715, 0.03766, 0.01788...  \n",
       "2  [0.01512, 0.01102, 0.0217, -0.02512, 0.0396, 0...  \n",
       "3  [-0.00414, -0.01288, 0.0716, 0.01605, -0.03983...  \n",
       "4  [-0.01651, 0.02525, 0.04333, 0.01558, -0.01678...  \n",
       "\n",
       "[5 rows x 682 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = encode_go_terms(df_train_set_filter)\n",
    "df_encoded = get_embeddings(df_encoded, p_embeddings_data)\n",
    "#df_encoded = get_ipr(df_ipr,df_encoded)\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKNV2HygPBYy",
    "outputId": "67a2bf91-619a-41c0-b74e-7a0ddac93a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUyI1vNNkK_G",
    "outputId": "9e144400-cc28-4744-9115-1e4c0a0c965e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GO:0005575', 'GO:0005886', 'GO:0016020', 'GO:0071944', 'GO:0110165'], dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns[3:-1][df_encoded.iloc[1,3:-1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv('./dataset/train/cellular_component_freq.csv')[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Ru3RD7uTbfVi"
   },
   "outputs": [],
   "source": [
    "y_columns = df_encoded.iloc[:, 3:-1]\n",
    "pred_columns = get_top_freq(y_columns.columns.tolist(), set(freq_df['id']))\n",
    "\n",
    "y_columns = y_columns[pred_columns]\n",
    "y = create_y(y_columns)\n",
    "X = create_X(df_encoded,'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lDxa-NGCcVDj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_B42EKPecoCD",
    "outputId": "d617883a-e559-4423-e96d-3c8fe35b9a31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 6s 1ms/step - loss: 0.0872 - accuracy: 0.8932 - f1_score: 0.6182 - val_loss: 0.0728 - val_accuracy: 0.8935 - val_f1_score: 0.6459\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0709 - accuracy: 0.8957 - f1_score: 0.6582 - val_loss: 0.0694 - val_accuracy: 0.8935 - val_f1_score: 0.6637\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0681 - accuracy: 0.8957 - f1_score: 0.6689 - val_loss: 0.0676 - val_accuracy: 0.8935 - val_f1_score: 0.6718\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0662 - accuracy: 0.8957 - f1_score: 0.6770 - val_loss: 0.0663 - val_accuracy: 0.8935 - val_f1_score: 0.6826\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0647 - accuracy: 0.8957 - f1_score: 0.6837 - val_loss: 0.0656 - val_accuracy: 0.8935 - val_f1_score: 0.6881\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0635 - accuracy: 0.8957 - f1_score: 0.6893 - val_loss: 0.0649 - val_accuracy: 0.8935 - val_f1_score: 0.6898\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0624 - accuracy: 0.8957 - f1_score: 0.6944 - val_loss: 0.0645 - val_accuracy: 0.8935 - val_f1_score: 0.6927\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0615 - accuracy: 0.8957 - f1_score: 0.6990 - val_loss: 0.0641 - val_accuracy: 0.8935 - val_f1_score: 0.6924\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0607 - accuracy: 0.8957 - f1_score: 0.7024 - val_loss: 0.0640 - val_accuracy: 0.8935 - val_f1_score: 0.6970\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0599 - accuracy: 0.8957 - f1_score: 0.7067 - val_loss: 0.0638 - val_accuracy: 0.8935 - val_f1_score: 0.6974\n",
      "265/265 [==============================] - 0s 741us/step - loss: 0.0638 - accuracy: 0.8935 - f1_score: 0.6974\n",
      "Test results - Fold 1: loss of 0.06375331431627274; accuracy of 89.35491442680359%\n",
      "Training for fold 2 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0864 - accuracy: 0.8566 - f1_score: 0.6200 - val_loss: 0.0731 - val_accuracy: 0.8935 - val_f1_score: 0.6540\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0709 - accuracy: 0.8957 - f1_score: 0.6579 - val_loss: 0.0697 - val_accuracy: 0.8935 - val_f1_score: 0.6647\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0681 - accuracy: 0.8957 - f1_score: 0.6693 - val_loss: 0.0683 - val_accuracy: 0.8934 - val_f1_score: 0.6681\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0663 - accuracy: 0.8957 - f1_score: 0.6763 - val_loss: 0.0671 - val_accuracy: 0.8935 - val_f1_score: 0.6748\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0649 - accuracy: 0.8957 - f1_score: 0.6828 - val_loss: 0.0664 - val_accuracy: 0.8935 - val_f1_score: 0.6760\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0637 - accuracy: 0.8957 - f1_score: 0.6883 - val_loss: 0.0657 - val_accuracy: 0.8935 - val_f1_score: 0.6816\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0627 - accuracy: 0.8957 - f1_score: 0.6929 - val_loss: 0.0651 - val_accuracy: 0.8935 - val_f1_score: 0.6867\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0618 - accuracy: 0.8957 - f1_score: 0.6970 - val_loss: 0.0654 - val_accuracy: 0.8935 - val_f1_score: 0.6790\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0609 - accuracy: 0.8957 - f1_score: 0.7013 - val_loss: 0.0648 - val_accuracy: 0.8935 - val_f1_score: 0.6918\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0602 - accuracy: 0.8957 - f1_score: 0.7047 - val_loss: 0.0651 - val_accuracy: 0.8935 - val_f1_score: 0.6924\n",
      "265/265 [==============================] - 0s 740us/step - loss: 0.0651 - accuracy: 0.8935 - f1_score: 0.6924\n",
      "Test results - Fold 2: loss of 0.06508242338895798; accuracy of 89.35491442680359%\n",
      "Training for fold 3 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0873 - accuracy: 0.8628 - f1_score: 0.6175 - val_loss: 0.0743 - val_accuracy: 0.8935 - val_f1_score: 0.6433\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0709 - accuracy: 0.8954 - f1_score: 0.6579 - val_loss: 0.0708 - val_accuracy: 0.8934 - val_f1_score: 0.6577\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0680 - accuracy: 0.8956 - f1_score: 0.6692 - val_loss: 0.0695 - val_accuracy: 0.8935 - val_f1_score: 0.6586\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0663 - accuracy: 0.8957 - f1_score: 0.6766 - val_loss: 0.0682 - val_accuracy: 0.8935 - val_f1_score: 0.6741\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0649 - accuracy: 0.8957 - f1_score: 0.6826 - val_loss: 0.0674 - val_accuracy: 0.8935 - val_f1_score: 0.6758\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0638 - accuracy: 0.8957 - f1_score: 0.6878 - val_loss: 0.0667 - val_accuracy: 0.8935 - val_f1_score: 0.6775\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0629 - accuracy: 0.8957 - f1_score: 0.6921 - val_loss: 0.0664 - val_accuracy: 0.8935 - val_f1_score: 0.6818\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0620 - accuracy: 0.8957 - f1_score: 0.6962 - val_loss: 0.0662 - val_accuracy: 0.8935 - val_f1_score: 0.6837\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0612 - accuracy: 0.8957 - f1_score: 0.7004 - val_loss: 0.0660 - val_accuracy: 0.8935 - val_f1_score: 0.6858\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0605 - accuracy: 0.8957 - f1_score: 0.7035 - val_loss: 0.0655 - val_accuracy: 0.8935 - val_f1_score: 0.6862\n",
      "265/265 [==============================] - 0s 745us/step - loss: 0.0655 - accuracy: 0.8935 - f1_score: 0.6862\n",
      "Test results - Fold 3: loss of 0.0654778778553009; accuracy of 89.35491442680359%\n",
      "Training for fold 4 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0872 - accuracy: 0.8882 - f1_score: 0.6190 - val_loss: 0.0733 - val_accuracy: 0.8905 - val_f1_score: 0.6402\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0706 - accuracy: 0.8961 - f1_score: 0.6595 - val_loss: 0.0697 - val_accuracy: 0.8905 - val_f1_score: 0.6674\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0678 - accuracy: 0.8961 - f1_score: 0.6710 - val_loss: 0.0681 - val_accuracy: 0.8905 - val_f1_score: 0.6721\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0661 - accuracy: 0.8961 - f1_score: 0.6777 - val_loss: 0.0671 - val_accuracy: 0.8905 - val_f1_score: 0.6785\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0648 - accuracy: 0.8961 - f1_score: 0.6832 - val_loss: 0.0665 - val_accuracy: 0.8905 - val_f1_score: 0.6782\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0636 - accuracy: 0.8961 - f1_score: 0.6888 - val_loss: 0.0659 - val_accuracy: 0.8905 - val_f1_score: 0.6820\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0627 - accuracy: 0.8961 - f1_score: 0.6928 - val_loss: 0.0657 - val_accuracy: 0.8905 - val_f1_score: 0.6817\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0618 - accuracy: 0.8961 - f1_score: 0.6972 - val_loss: 0.0657 - val_accuracy: 0.8905 - val_f1_score: 0.6854\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0610 - accuracy: 0.8961 - f1_score: 0.7012 - val_loss: 0.0650 - val_accuracy: 0.8905 - val_f1_score: 0.6940\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0603 - accuracy: 0.8961 - f1_score: 0.7052 - val_loss: 0.0648 - val_accuracy: 0.8905 - val_f1_score: 0.6902\n",
      "265/265 [==============================] - 0s 724us/step - loss: 0.0648 - accuracy: 0.8905 - f1_score: 0.6902\n",
      "Test results - Fold 4: loss of 0.06481800228357315; accuracy of 89.04772996902466%\n",
      "Training for fold 5 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0878 - accuracy: 0.8745 - f1_score: 0.6168 - val_loss: 0.0729 - val_accuracy: 0.8921 - val_f1_score: 0.6454\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0711 - accuracy: 0.8959 - f1_score: 0.6574 - val_loss: 0.0694 - val_accuracy: 0.8921 - val_f1_score: 0.6579\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0683 - accuracy: 0.8959 - f1_score: 0.6691 - val_loss: 0.0676 - val_accuracy: 0.8921 - val_f1_score: 0.6739\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0664 - accuracy: 0.8959 - f1_score: 0.6764 - val_loss: 0.0664 - val_accuracy: 0.8921 - val_f1_score: 0.6760\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0650 - accuracy: 0.8959 - f1_score: 0.6828 - val_loss: 0.0656 - val_accuracy: 0.8921 - val_f1_score: 0.6756\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0638 - accuracy: 0.8959 - f1_score: 0.6888 - val_loss: 0.0650 - val_accuracy: 0.8921 - val_f1_score: 0.6808\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0628 - accuracy: 0.8959 - f1_score: 0.6935 - val_loss: 0.0646 - val_accuracy: 0.8921 - val_f1_score: 0.6830\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0619 - accuracy: 0.8959 - f1_score: 0.6975 - val_loss: 0.0645 - val_accuracy: 0.8921 - val_f1_score: 0.6893\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0610 - accuracy: 0.8959 - f1_score: 0.7016 - val_loss: 0.0642 - val_accuracy: 0.8921 - val_f1_score: 0.6889\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0603 - accuracy: 0.8959 - f1_score: 0.7051 - val_loss: 0.0640 - val_accuracy: 0.8921 - val_f1_score: 0.6893\n",
      "265/265 [==============================] - 0s 704us/step - loss: 0.0640 - accuracy: 0.8921 - f1_score: 0.6893\n",
      "Test results - Fold 5: loss of 0.06396473944187164; accuracy of 89.21313881874084%\n",
      "Training for fold 6 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0883 - accuracy: 0.8893 - f1_score: 0.6156 - val_loss: 0.0731 - val_accuracy: 0.8966 - val_f1_score: 0.6557\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0712 - accuracy: 0.8954 - f1_score: 0.6561 - val_loss: 0.0696 - val_accuracy: 0.8966 - val_f1_score: 0.6665\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0685 - accuracy: 0.8954 - f1_score: 0.6673 - val_loss: 0.0681 - val_accuracy: 0.8966 - val_f1_score: 0.6715\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0667 - accuracy: 0.8954 - f1_score: 0.6749 - val_loss: 0.0672 - val_accuracy: 0.8966 - val_f1_score: 0.6743\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0653 - accuracy: 0.8954 - f1_score: 0.6810 - val_loss: 0.0664 - val_accuracy: 0.8966 - val_f1_score: 0.6814\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0641 - accuracy: 0.8954 - f1_score: 0.6860 - val_loss: 0.0658 - val_accuracy: 0.8966 - val_f1_score: 0.6873\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0631 - accuracy: 0.8954 - f1_score: 0.6907 - val_loss: 0.0656 - val_accuracy: 0.8966 - val_f1_score: 0.6905\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0623 - accuracy: 0.8954 - f1_score: 0.6953 - val_loss: 0.0653 - val_accuracy: 0.8966 - val_f1_score: 0.6874\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0615 - accuracy: 0.8954 - f1_score: 0.6989 - val_loss: 0.0652 - val_accuracy: 0.8966 - val_f1_score: 0.6861\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0608 - accuracy: 0.8954 - f1_score: 0.7017 - val_loss: 0.0648 - val_accuracy: 0.8966 - val_f1_score: 0.6867\n",
      "265/265 [==============================] - 0s 710us/step - loss: 0.0648 - accuracy: 0.8966 - f1_score: 0.6867\n",
      "Test results - Fold 6: loss of 0.06483019888401031; accuracy of 89.66209888458252%\n",
      "Training for fold 7 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0888 - accuracy: 0.8735 - f1_score: 0.6139 - val_loss: 0.0725 - val_accuracy: 0.9035 - val_f1_score: 0.6469\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0713 - accuracy: 0.8946 - f1_score: 0.6560 - val_loss: 0.0692 - val_accuracy: 0.9035 - val_f1_score: 0.6579\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0684 - accuracy: 0.8946 - f1_score: 0.6674 - val_loss: 0.0674 - val_accuracy: 0.9035 - val_f1_score: 0.6655\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0666 - accuracy: 0.8946 - f1_score: 0.6754 - val_loss: 0.0663 - val_accuracy: 0.9035 - val_f1_score: 0.6814\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0652 - accuracy: 0.8946 - f1_score: 0.6821 - val_loss: 0.0655 - val_accuracy: 0.9035 - val_f1_score: 0.6799\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0640 - accuracy: 0.8946 - f1_score: 0.6869 - val_loss: 0.0649 - val_accuracy: 0.9035 - val_f1_score: 0.6812\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0630 - accuracy: 0.8946 - f1_score: 0.6914 - val_loss: 0.0646 - val_accuracy: 0.9035 - val_f1_score: 0.6821\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0621 - accuracy: 0.8946 - f1_score: 0.6960 - val_loss: 0.0643 - val_accuracy: 0.9035 - val_f1_score: 0.6964\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0614 - accuracy: 0.8946 - f1_score: 0.7005 - val_loss: 0.0640 - val_accuracy: 0.9035 - val_f1_score: 0.6950\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0606 - accuracy: 0.8946 - f1_score: 0.7031 - val_loss: 0.0637 - val_accuracy: 0.9035 - val_f1_score: 0.6988\n",
      "265/265 [==============================] - 0s 701us/step - loss: 0.0637 - accuracy: 0.9035 - f1_score: 0.6988\n",
      "Test results - Fold 7: loss of 0.06373589485883713; accuracy of 90.34735560417175%\n",
      "Training for fold 8 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0866 - accuracy: 0.8931 - f1_score: 0.6206 - val_loss: 0.0721 - val_accuracy: 0.8989 - val_f1_score: 0.6529\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0706 - accuracy: 0.8951 - f1_score: 0.6586 - val_loss: 0.0692 - val_accuracy: 0.8989 - val_f1_score: 0.6706\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0680 - accuracy: 0.8951 - f1_score: 0.6697 - val_loss: 0.0673 - val_accuracy: 0.8989 - val_f1_score: 0.6762\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0662 - accuracy: 0.8951 - f1_score: 0.6774 - val_loss: 0.0665 - val_accuracy: 0.8989 - val_f1_score: 0.6771\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0648 - accuracy: 0.8951 - f1_score: 0.6837 - val_loss: 0.0659 - val_accuracy: 0.8989 - val_f1_score: 0.6845\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0637 - accuracy: 0.8951 - f1_score: 0.6888 - val_loss: 0.0654 - val_accuracy: 0.8989 - val_f1_score: 0.6942\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0626 - accuracy: 0.8951 - f1_score: 0.6936 - val_loss: 0.0646 - val_accuracy: 0.8989 - val_f1_score: 0.6906\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0617 - accuracy: 0.8951 - f1_score: 0.6978 - val_loss: 0.0640 - val_accuracy: 0.8989 - val_f1_score: 0.6949\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0609 - accuracy: 0.8951 - f1_score: 0.7017 - val_loss: 0.0642 - val_accuracy: 0.8989 - val_f1_score: 0.6951\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0602 - accuracy: 0.8951 - f1_score: 0.7055 - val_loss: 0.0637 - val_accuracy: 0.8989 - val_f1_score: 0.6968\n",
      "265/265 [==============================] - 0s 723us/step - loss: 0.0637 - accuracy: 0.8989 - f1_score: 0.6968\n",
      "Test results - Fold 8: loss of 0.06371843069791794; accuracy of 89.88657593727112%\n",
      "Training for fold 9 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0868 - accuracy: 0.8529 - f1_score: 0.6191 - val_loss: 0.0734 - val_accuracy: 0.8950 - val_f1_score: 0.6459\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0709 - accuracy: 0.8956 - f1_score: 0.6580 - val_loss: 0.0706 - val_accuracy: 0.8950 - val_f1_score: 0.6639\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0681 - accuracy: 0.8956 - f1_score: 0.6691 - val_loss: 0.0687 - val_accuracy: 0.8950 - val_f1_score: 0.6719\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0663 - accuracy: 0.8956 - f1_score: 0.6772 - val_loss: 0.0685 - val_accuracy: 0.8950 - val_f1_score: 0.6731\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0650 - accuracy: 0.8956 - f1_score: 0.6829 - val_loss: 0.0670 - val_accuracy: 0.8950 - val_f1_score: 0.6750\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0639 - accuracy: 0.8956 - f1_score: 0.6876 - val_loss: 0.0664 - val_accuracy: 0.8950 - val_f1_score: 0.6773\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0628 - accuracy: 0.8956 - f1_score: 0.6927 - val_loss: 0.0659 - val_accuracy: 0.8950 - val_f1_score: 0.6794\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0619 - accuracy: 0.8956 - f1_score: 0.6965 - val_loss: 0.0659 - val_accuracy: 0.8950 - val_f1_score: 0.6890\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0611 - accuracy: 0.8956 - f1_score: 0.7006 - val_loss: 0.0653 - val_accuracy: 0.8950 - val_f1_score: 0.6899\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 4s 1ms/step - loss: 0.0603 - accuracy: 0.8956 - f1_score: 0.7043 - val_loss: 0.0653 - val_accuracy: 0.8950 - val_f1_score: 0.6822\n",
      "265/265 [==============================] - 0s 790us/step - loss: 0.0653 - accuracy: 0.8950 - f1_score: 0.6822\n",
      "Test results - Fold 9: loss of 0.06526006758213043; accuracy of 89.495450258255%\n",
      "Training for fold 10 ...\n",
      "Epoch 1/10\n",
      "2381/2381 [==============================] - 4s 2ms/step - loss: 0.0872 - accuracy: 0.8918 - f1_score: 0.6182 - val_loss: 0.0736 - val_accuracy: 0.8978 - val_f1_score: 0.6525\n",
      "Epoch 2/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0709 - accuracy: 0.8952 - f1_score: 0.6579 - val_loss: 0.0699 - val_accuracy: 0.8978 - val_f1_score: 0.6639\n",
      "Epoch 3/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0681 - accuracy: 0.8952 - f1_score: 0.6691 - val_loss: 0.0683 - val_accuracy: 0.8978 - val_f1_score: 0.6709\n",
      "Epoch 4/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0664 - accuracy: 0.8952 - f1_score: 0.6761 - val_loss: 0.0673 - val_accuracy: 0.8978 - val_f1_score: 0.6798\n",
      "Epoch 5/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0650 - accuracy: 0.8952 - f1_score: 0.6822 - val_loss: 0.0671 - val_accuracy: 0.8978 - val_f1_score: 0.6698\n",
      "Epoch 6/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0638 - accuracy: 0.8952 - f1_score: 0.6875 - val_loss: 0.0661 - val_accuracy: 0.8978 - val_f1_score: 0.6898\n",
      "Epoch 7/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0629 - accuracy: 0.8952 - f1_score: 0.6923 - val_loss: 0.0655 - val_accuracy: 0.8978 - val_f1_score: 0.6862\n",
      "Epoch 8/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0620 - accuracy: 0.8952 - f1_score: 0.6962 - val_loss: 0.0653 - val_accuracy: 0.8978 - val_f1_score: 0.6813\n",
      "Epoch 9/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0612 - accuracy: 0.8952 - f1_score: 0.7002 - val_loss: 0.0649 - val_accuracy: 0.8978 - val_f1_score: 0.6904\n",
      "Epoch 10/10\n",
      "2381/2381 [==============================] - 3s 1ms/step - loss: 0.0604 - accuracy: 0.8952 - f1_score: 0.7039 - val_loss: 0.0653 - val_accuracy: 0.8978 - val_f1_score: 0.6803\n",
      "265/265 [==============================] - 0s 694us/step - loss: 0.0653 - accuracy: 0.8978 - f1_score: 0.6803\n",
      "Test results - Fold 10: loss of 0.06526634097099304; accuracy of 89.77903723716736%\n"
     ]
    }
   ],
   "source": [
    "embedding_size = len(X_train[1]) #1024\n",
    "num_classes = len(y_train[1]) #678\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / (K.sum(K.round(K.clip(y_pred, 0, 1))) + K.epsilon())\n",
    "    recall = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / (K.sum(K.round(K.clip(y_true, 0, 1))) + K.epsilon())\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "histories = []\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over each fold\n",
    "fold_no = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Splitting the data into training and testing sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Build a neural network model\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(embedding_size,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='sigmoid')  # Sigmoid for multi-label classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n",
    "\n",
    "    # Fit the model\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "    histories.append(history)\n",
    "    \n",
    "    # Here, you can evaluate the model on the test set, e.g., calculate metrics\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test results - Fold {fold_no}: {model.metrics_names[0]} of {results[0]}; {model.metrics_names[1]} of {results[1]*100}%\")\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2645/2645 [==============================] - 4s 1ms/step - loss: 0.0857 - f1_score: 0.6224\n",
      "Epoch 2/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0704 - f1_score: 0.6600\n",
      "Epoch 3/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0677 - f1_score: 0.6705\n",
      "Epoch 4/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0659 - f1_score: 0.6781\n",
      "Epoch 5/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0645 - f1_score: 0.6842\n",
      "Epoch 6/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0633 - f1_score: 0.6896\n",
      "Epoch 7/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0623 - f1_score: 0.6944\n",
      "Epoch 8/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0614 - f1_score: 0.6989\n",
      "Epoch 9/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0606 - f1_score: 0.7026\n",
      "Epoch 10/10\n",
      "2645/2645 [==============================] - 3s 1ms/step - loss: 0.0599 - f1_score: 0.7058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f10ac308d50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = keras.Sequential([\n",
    "    layers.Input(shape=(embedding_size,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='sigmoid')\n",
    "])\n",
    "\n",
    "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_score])\n",
    "final_model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnDbWCBVcv7X",
    "outputId": "9f95b107-7edf-49fa-8576-4e48c7d55549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 605us/step\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model on the test set\n",
    "y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = pd.read_csv('./dataset/train/main_pairing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.24 s, sys: 95.9 ms, total: 4.34 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "graph = obonet.read_obo('./dataset/taxonomy/go-basic.obo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Propagating the probability of the children to the parent, in this case if the parent has several children, we will take the max probabilty of the children\n",
    "def post_processing(y_pred, pred_scolumns, graph_df):\n",
    "    new_preds = []\n",
    "\n",
    "    # Get parent relation\n",
    "    parent_dict = {}\n",
    "    for _, row in graph_df.iterrows():\n",
    "        parent_dict.setdefault(row['child'], []).append(row['parent'])\n",
    "\n",
    "    for pred in y_pred:\n",
    "        ### Build prediction dict\n",
    "        preds = {k: 0 for k in pred_columns}\n",
    "        new_pred = [0 for i in range(len(pred))]\n",
    "        for i in range(len(pred)):\n",
    "            term = pred_columns[i]\n",
    "            preds[term] = pred[i]\n",
    "\n",
    "        ### Search the probabilty for the parent\n",
    "        pool = set()\n",
    "        for term, prob in preds.items():\n",
    "            for parent, child, key in graph.in_edges(term, keys=True):\n",
    "                if key not in {'is_a', 'part_of'} or parent not in preds:\n",
    "                    continue\n",
    "\n",
    "                probability = max(prob, preds[parent])\n",
    "                preds[parent] = probability\n",
    "                    \n",
    "        ### Build the array for the new preds\n",
    "        for term, prob in preds.items():\n",
    "            idx = pred_columns.index(term)\n",
    "            new_pred[idx] = prob\n",
    "        new_preds.append(new_pred)\n",
    "    return np.array(new_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_pred = post_processing(y_pred, pred_columns, graph_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        46\n",
      "           1       0.67      0.03      0.05        77\n",
      "           2       0.38      0.03      0.05       107\n",
      "           3       1.00      0.01      0.02       117\n",
      "           4       0.69      0.49      0.57        72\n",
      "           5       1.00      0.03      0.05       115\n",
      "           6       0.69      0.04      0.08       271\n",
      "           7       0.75      0.03      0.05       109\n",
      "           8       1.00      0.03      0.07       117\n",
      "           9       0.00      0.00      0.00        58\n",
      "          10       0.63      0.31      0.42        70\n",
      "          11       0.82      0.65      0.73        55\n",
      "          12       0.70      0.17      0.28       122\n",
      "          13       0.65      0.19      0.29        79\n",
      "          14       0.71      0.24      0.36        84\n",
      "          15       0.00      0.00      0.00        42\n",
      "          16       0.73      0.07      0.13       328\n",
      "          17       0.00      0.00      0.00        49\n",
      "          18       0.70      0.17      0.28       134\n",
      "          19       0.00      0.00      0.00        30\n",
      "          20       0.00      0.00      0.00        57\n",
      "          21       1.00      1.00      1.00     12696\n",
      "          22       0.76      0.33      0.46      1020\n",
      "          23       0.70      0.34      0.46       744\n",
      "          24       0.54      0.36      0.43       220\n",
      "          25       0.90      0.95      0.92     10084\n",
      "          26       0.81      0.66      0.73      4062\n",
      "          27       0.91      0.11      0.19       194\n",
      "          28       0.70      0.17      0.28        40\n",
      "          29       0.66      0.42      0.51      1555\n",
      "          30       0.00      0.00      0.00       163\n",
      "          31       0.37      0.25      0.29        61\n",
      "          32       0.70      0.16      0.26       610\n",
      "          33       0.00      0.00      0.00        41\n",
      "          34       0.74      0.18      0.29       489\n",
      "          35       0.77      0.85      0.81      7593\n",
      "          36       0.84      0.61      0.71      1310\n",
      "          37       0.62      0.23      0.34       334\n",
      "          38       0.00      0.00      0.00        73\n",
      "          39       0.62      0.21      0.31       230\n",
      "          40       0.65      0.25      0.36        68\n",
      "          41       0.62      0.22      0.33       181\n",
      "          42       0.69      0.48      0.56        69\n",
      "          43       0.60      0.07      0.13       161\n",
      "          44       0.00      0.00      0.00        83\n",
      "          45       0.75      0.01      0.02       302\n",
      "          46       0.00      0.00      0.00        85\n",
      "          47       0.00      0.00      0.00        71\n",
      "          48       0.77      0.04      0.08       460\n",
      "          49       0.67      0.01      0.02       188\n",
      "          50       1.00      0.02      0.04       146\n",
      "          51       0.69      0.31      0.43       841\n",
      "          52       0.25      0.01      0.03        69\n",
      "          53       0.36      0.11      0.17       237\n",
      "          54       0.81      0.18      0.29       573\n",
      "          55       0.22      0.04      0.07        49\n",
      "          56       0.57      0.04      0.08        89\n",
      "          57       1.00      0.04      0.08        47\n",
      "          58       0.60      0.11      0.19       187\n",
      "          59       0.51      0.11      0.17       295\n",
      "          60       1.00      0.03      0.06        32\n",
      "          61       0.78      0.05      0.09       142\n",
      "          62       0.61      0.34      0.44      2659\n",
      "          63       0.74      0.64      0.69       231\n",
      "          64       0.66      0.32      0.43       793\n",
      "          65       0.69      0.08      0.15       111\n",
      "          66       1.00      0.03      0.05        39\n",
      "          67       0.43      0.07      0.12        43\n",
      "          68       0.76      0.54      0.63      2426\n",
      "          69       0.83      0.03      0.06       305\n",
      "          70       0.00      0.00      0.00        51\n",
      "          71       0.00      0.00      0.00        84\n",
      "          72       0.73      0.20      0.32       392\n",
      "          73       0.57      0.25      0.35        84\n",
      "          74       0.17      0.02      0.04        49\n",
      "          75       0.71      0.12      0.20       201\n",
      "          76       1.00      0.06      0.12        63\n",
      "          77       0.48      0.62      0.54       111\n",
      "          78       0.65      0.46      0.54        37\n",
      "          79       0.82      0.15      0.25        61\n",
      "          80       0.60      0.02      0.04       147\n",
      "          81       0.86      0.68      0.76       334\n",
      "          82       0.70      0.51      0.59       124\n",
      "          83       0.74      0.55      0.63       116\n",
      "          84       0.70      0.50      0.58        74\n",
      "          85       0.65      0.48      0.55        58\n",
      "          86       0.86      0.59      0.70       488\n",
      "          87       0.72      0.54      0.61       114\n",
      "          88       0.80      0.42      0.55       103\n",
      "          89       0.00      0.00      0.00        77\n",
      "          90       0.00      0.00      0.00        56\n",
      "          91       0.00      0.00      0.00        98\n",
      "          92       0.56      0.12      0.19        78\n",
      "          93       0.54      0.03      0.06       221\n",
      "          94       0.00      0.00      0.00        80\n",
      "          95       0.00      0.00      0.00        47\n",
      "          96       0.67      0.44      0.53      1864\n",
      "          97       0.34      0.07      0.11       222\n",
      "          98       0.00      0.00      0.00        72\n",
      "          99       0.87      0.15      0.26       170\n",
      "         100       0.69      0.21      0.32       507\n",
      "         101       0.51      0.60      0.55       113\n",
      "         102       0.62      0.14      0.23        90\n",
      "         103       0.72      0.59      0.65      3613\n",
      "         104       0.00      0.00      0.00        78\n",
      "         105       0.00      0.00      0.00       132\n",
      "         106       0.61      0.29      0.39        48\n",
      "         107       1.00      0.01      0.02       219\n",
      "         108       0.00      0.00      0.00        89\n",
      "         109       0.79      0.55      0.65       127\n",
      "         110       0.63      0.23      0.34       245\n",
      "         111       0.65      0.22      0.33       166\n",
      "         112       0.00      0.00      0.00        62\n",
      "         113       0.60      0.68      0.64        76\n",
      "         114       0.75      0.70      0.73       148\n",
      "         115       0.79      0.20      0.32        54\n",
      "         116       0.79      0.15      0.25       103\n",
      "         117       0.81      0.14      0.25        90\n",
      "         118       1.00      0.05      0.10        40\n",
      "         119       0.65      0.14      0.23       887\n",
      "         120       0.00      0.00      0.00        96\n",
      "         121       0.43      0.03      0.06       100\n",
      "         122       0.80      0.10      0.18        80\n",
      "         123       1.00      0.02      0.04        55\n",
      "         124       0.20      0.01      0.01       139\n",
      "         125       0.00      0.00      0.00       162\n",
      "         126       0.56      0.31      0.40       463\n",
      "         127       0.80      0.65      0.72        49\n",
      "         128       0.00      0.00      0.00       222\n",
      "         129       0.00      0.00      0.00       191\n",
      "         130       0.00      0.00      0.00        49\n",
      "         131       0.50      0.05      0.09       126\n",
      "         132       0.74      0.41      0.52        69\n",
      "         133       0.00      0.00      0.00        41\n",
      "         134       1.00      0.02      0.05        42\n",
      "         135       0.33      0.07      0.12       215\n",
      "         136       0.75      0.07      0.14        40\n",
      "         137       0.00      0.00      0.00        34\n",
      "         138       0.00      0.00      0.00        54\n",
      "         139       0.33      0.06      0.10        33\n",
      "         140       0.55      0.09      0.16        66\n",
      "         141       0.62      0.26      0.37        38\n",
      "         142       0.61      0.28      0.38        71\n",
      "         143       0.46      0.12      0.19        49\n",
      "         144       0.68      0.17      0.28       224\n",
      "         145       0.54      0.20      0.29      1134\n",
      "         146       0.00      0.00      0.00        50\n",
      "         147       0.00      0.00      0.00        96\n",
      "         148       0.00      0.00      0.00        81\n",
      "         149       0.56      0.17      0.26       760\n",
      "         150       0.25      0.01      0.03        67\n",
      "         151       0.00      0.00      0.00        59\n",
      "         152       0.00      0.00      0.00        60\n",
      "         153       0.75      0.07      0.12        46\n",
      "         154       0.75      0.04      0.08        70\n",
      "         155       0.60      0.21      0.31       318\n",
      "         156       0.68      0.24      0.36       649\n",
      "         157       0.60      0.20      0.30       145\n",
      "         158       0.67      0.44      0.53      2343\n",
      "         159       0.66      0.30      0.41       698\n",
      "         160       0.70      0.49      0.58        75\n",
      "         161       0.68      0.46      0.55      2011\n",
      "         162       0.59      0.13      0.22      1029\n",
      "         163       0.00      0.00      0.00        45\n",
      "         164       0.31      0.06      0.10       364\n",
      "         165       0.25      0.05      0.08        40\n",
      "         166       0.53      0.10      0.17        88\n",
      "         167       0.00      0.00      0.00        77\n",
      "         168       0.51      0.17      0.25       173\n",
      "         169       0.64      0.31      0.42      2468\n",
      "         170       0.69      0.09      0.16       375\n",
      "         171       0.78      0.54      0.64       126\n",
      "         172       0.74      0.46      0.57        87\n",
      "         173       0.83      0.33      0.47        57\n",
      "         174       0.70      0.39      0.50        76\n",
      "         175       0.00      0.00      0.00        54\n",
      "         176       0.72      0.33      0.46        93\n",
      "         177       0.75      0.05      0.10        58\n",
      "         178       0.00      0.00      0.00        45\n",
      "         179       0.67      0.04      0.07       109\n",
      "         180       0.00      0.00      0.00        69\n",
      "         181       0.00      0.00      0.00        32\n",
      "         182       1.00      0.03      0.06       103\n",
      "         183       0.00      0.00      0.00       293\n",
      "         184       0.64      0.54      0.58        71\n",
      "         185       0.36      0.11      0.17       248\n",
      "         186       0.00      0.00      0.00        39\n",
      "         187       1.00      0.03      0.05       146\n",
      "         188       0.75      0.44      0.56        34\n",
      "         189       0.67      0.41      0.50        69\n",
      "         190       0.00      0.00      0.00        67\n",
      "         191       0.68      0.08      0.15       940\n",
      "         192       0.57      0.01      0.02       423\n",
      "         193       0.00      0.00      0.00       194\n",
      "         194       0.00      0.00      0.00        37\n",
      "         195       0.00      0.00      0.00        51\n",
      "         196       0.83      0.88      0.86      8719\n",
      "         197       0.81      0.83      0.82      7964\n",
      "         198       0.69      0.37      0.49      2331\n",
      "         199       0.82      0.88      0.85      8381\n",
      "         200       0.00      0.00      0.00       321\n",
      "         201       0.80      0.83      0.82      7530\n",
      "         202       0.69      0.38      0.49      2327\n",
      "         203       0.67      0.44      0.53      2343\n",
      "         204       0.58      0.08      0.13        93\n",
      "         205       0.89      0.21      0.34       121\n",
      "         206       0.00      0.00      0.00        42\n",
      "         207       0.74      0.49      0.59        88\n",
      "         208       0.79      0.55      0.65       127\n",
      "         209       0.00      0.00      0.00        64\n",
      "         210       0.00      0.00      0.00       214\n",
      "         211       0.00      0.00      0.00        64\n",
      "         212       0.00      0.00      0.00        52\n",
      "         213       0.71      0.68      0.70       203\n",
      "         214       0.00      0.00      0.00        71\n",
      "         215       0.00      0.00      0.00       191\n",
      "         216       0.00      0.00      0.00       109\n",
      "         217       0.61      0.14      0.22       462\n",
      "         218       1.00      0.03      0.06        67\n",
      "         219       0.00      0.00      0.00        86\n",
      "         220       0.00      0.00      0.00       176\n",
      "         221       0.64      0.09      0.15        80\n",
      "         222       0.66      0.34      0.45        62\n",
      "         223       0.69      0.48      0.57        60\n",
      "         224       0.00      0.00      0.00        34\n",
      "         225       0.60      0.02      0.04       147\n",
      "         226       0.64      0.09      0.15        82\n",
      "         227       0.00      0.00      0.00        45\n",
      "         228       0.59      0.17      0.27       138\n",
      "         229       0.74      0.24      0.36       116\n",
      "         230       0.00      0.00      0.00        56\n",
      "         231       0.00      0.00      0.00        56\n",
      "         232       0.00      0.00      0.00       320\n",
      "         233       0.67      0.44      0.53      2343\n",
      "         234       0.00      0.00      0.00       283\n",
      "         235       0.00      0.00      0.00        35\n",
      "         236       0.78      0.05      0.09       395\n",
      "         237       1.00      0.03      0.06        68\n",
      "         238       0.63      0.25      0.36        75\n",
      "         239       0.00      0.00      0.00        54\n",
      "         240       0.35      0.17      0.23        35\n",
      "         241       0.00      0.00      0.00        33\n",
      "         242       0.75      0.58      0.65      2933\n",
      "         243       0.00      0.00      0.00        68\n",
      "         244       0.00      0.00      0.00        86\n",
      "         245       0.52      0.17      0.25       166\n",
      "         246       0.55      0.05      0.09       122\n",
      "         247       0.00      0.00      0.00       191\n",
      "         248       0.57      0.17      0.26       761\n",
      "         249       0.00      0.00      0.00        45\n",
      "         250       0.00      0.00      0.00        58\n",
      "         251       0.30      0.02      0.04       154\n",
      "         252       0.00      0.00      0.00        71\n",
      "         253       0.56      0.08      0.15       624\n",
      "         254       0.67      0.02      0.03       454\n",
      "         255       0.87      0.07      0.13       183\n",
      "         256       0.67      0.08      0.14       123\n",
      "         257       0.71      0.06      0.12       190\n",
      "         258       0.58      0.04      0.07       183\n",
      "         259       0.54      0.16      0.25       465\n",
      "         260       0.66      0.10      0.17       193\n",
      "         261       0.56      0.36      0.43       205\n",
      "         262       0.55      0.39      0.45       101\n",
      "         263       0.36      0.08      0.13        61\n",
      "         264       0.62      0.29      0.39        70\n",
      "         265       0.37      0.10      0.16       250\n",
      "         266       0.00      0.00      0.00       141\n",
      "         267       0.00      0.00      0.00        72\n",
      "         268       0.00      0.00      0.00        70\n",
      "         269       0.00      0.00      0.00        42\n",
      "         270       0.00      0.00      0.00       144\n",
      "         271       0.00      0.00      0.00        79\n",
      "         272       0.63      0.12      0.21       472\n",
      "         273       0.64      0.16      0.25       285\n",
      "         274       0.00      0.00      0.00       246\n",
      "         275       0.66      0.17      0.27       279\n",
      "         276       0.42      0.08      0.14       153\n",
      "         277       0.60      0.11      0.19       219\n",
      "         278       0.00      0.00      0.00        85\n",
      "         279       0.00      0.00      0.00        46\n",
      "         280       0.99      1.00      1.00     12575\n",
      "         281       0.73      0.09      0.15       927\n",
      "         282       0.56      0.12      0.20        41\n",
      "         283       0.63      0.23      0.33       568\n",
      "         284       0.10      0.02      0.04        46\n",
      "         285       0.70      0.15      0.24       447\n",
      "         286       0.00      0.00      0.00       110\n",
      "         287       0.00      0.00      0.00        51\n",
      "         288       0.69      0.19      0.29       835\n",
      "         289       0.69      0.20      0.31       172\n",
      "         290       0.25      0.07      0.11        44\n",
      "         291       0.27      0.06      0.10        49\n",
      "         292       0.00      0.00      0.00       298\n",
      "         293       0.50      0.02      0.04        92\n",
      "         294       0.81      0.37      0.50        82\n",
      "         295       0.76      0.47      0.58        62\n",
      "         296       0.78      0.14      0.24       101\n",
      "         297       0.59      0.16      0.25       365\n",
      "         298       0.69      0.19      0.30       183\n",
      "         299       0.63      0.47      0.54       390\n",
      "\n",
      "   micro avg       0.82      0.59      0.69    160944\n",
      "   macro avg       0.47      0.17      0.22    160944\n",
      "weighted avg       0.74      0.59      0.62    160944\n",
      " samples avg       0.83      0.66      0.70    160944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satria/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.24      0.28        46\n",
      "           1       0.01      1.00      0.01        77\n",
      "           2       0.01      1.00      0.02       107\n",
      "           3       0.03      0.91      0.06       117\n",
      "           4       0.01      1.00      0.02        72\n",
      "           5       0.01      0.93      0.02       115\n",
      "           6       0.02      0.92      0.05       271\n",
      "           7       0.38      0.06      0.10       109\n",
      "           8       0.01      0.97      0.02       117\n",
      "           9       0.01      1.00      0.01        58\n",
      "          10       0.63      0.31      0.42        70\n",
      "          11       0.00      1.00      0.01        55\n",
      "          12       0.21      0.25      0.23       122\n",
      "          13       0.01      1.00      0.01        79\n",
      "          14       0.60      0.29      0.39        84\n",
      "          15       0.04      0.14      0.07        42\n",
      "          16       0.03      1.00      0.05       328\n",
      "          17       0.00      0.00      0.00        49\n",
      "          18       0.22      0.25      0.23       134\n",
      "          19       0.00      1.00      0.00        30\n",
      "          20       0.00      1.00      0.01        57\n",
      "          21       1.00      1.00      1.00     12696\n",
      "          22       0.08      1.00      0.15      1020\n",
      "          23       0.06      1.00      0.11       744\n",
      "          24       0.36      0.46      0.40       220\n",
      "          25       0.79      1.00      0.89     10084\n",
      "          26       0.38      0.98      0.54      4062\n",
      "          27       0.03      0.65      0.05       194\n",
      "          28       0.00      1.00      0.01        40\n",
      "          29       0.12      1.00      0.22      1555\n",
      "          30       0.01      1.00      0.03       163\n",
      "          31       0.00      1.00      0.01        61\n",
      "          32       0.06      1.00      0.11       610\n",
      "          33       0.07      0.22      0.10        41\n",
      "          34       0.05      0.99      0.09       489\n",
      "          35       0.60      1.00      0.75      7593\n",
      "          36       0.12      0.99      0.21      1310\n",
      "          37       0.03      0.99      0.06       334\n",
      "          38       0.01      1.00      0.01        73\n",
      "          39       0.02      1.00      0.04       230\n",
      "          40       0.01      0.65      0.03        68\n",
      "          41       0.02      1.00      0.03       181\n",
      "          42       0.01      1.00      0.01        69\n",
      "          43       0.43      0.08      0.14       161\n",
      "          44       0.01      0.92      0.01        83\n",
      "          45       0.02      0.88      0.05       302\n",
      "          46       0.00      0.00      0.00        85\n",
      "          47       0.25      0.01      0.03        71\n",
      "          48       0.04      0.95      0.08       460\n",
      "          49       0.02      1.00      0.03       188\n",
      "          50       0.01      0.99      0.03       146\n",
      "          51       0.07      0.91      0.13       841\n",
      "          52       0.00      0.70      0.01        69\n",
      "          53       0.02      0.99      0.04       237\n",
      "          54       0.05      0.93      0.09       573\n",
      "          55       0.00      0.98      0.01        49\n",
      "          56       0.01      0.93      0.02        89\n",
      "          57       0.00      0.98      0.01        47\n",
      "          58       0.42      0.14      0.21       187\n",
      "          59       0.02      1.00      0.05       295\n",
      "          60       0.05      0.09      0.06        32\n",
      "          61       0.01      0.99      0.03       142\n",
      "          62       0.21      1.00      0.35      2659\n",
      "          63       0.02      1.00      0.04       231\n",
      "          64       0.07      0.98      0.14       793\n",
      "          65       0.13      0.48      0.20       111\n",
      "          66       0.00      1.00      0.01        39\n",
      "          67       0.00      1.00      0.01        43\n",
      "          68       0.53      0.73      0.61      2426\n",
      "          69       0.29      0.18      0.22       305\n",
      "          70       0.42      0.10      0.16        51\n",
      "          71       0.00      0.00      0.00        84\n",
      "          72       0.03      0.83      0.07       392\n",
      "          73       0.01      1.00      0.01        84\n",
      "          74       0.00      1.00      0.01        49\n",
      "          75       0.02      1.00      0.03       201\n",
      "          76       0.03      0.59      0.05        63\n",
      "          77       0.37      0.64      0.47       111\n",
      "          78       0.14      0.57      0.23        37\n",
      "          79       0.08      0.21      0.12        61\n",
      "          80       0.31      0.03      0.06       147\n",
      "          81       0.03      0.99      0.06       334\n",
      "          82       0.01      1.00      0.02       124\n",
      "          83       0.01      1.00      0.02       116\n",
      "          84       0.01      1.00      0.01        74\n",
      "          85       0.00      1.00      0.01        58\n",
      "          86       0.04      0.99      0.09       488\n",
      "          87       0.37      0.86      0.52       114\n",
      "          88       0.01      0.99      0.02       103\n",
      "          89       0.02      0.75      0.04        77\n",
      "          90       0.01      0.57      0.02        56\n",
      "          91       0.03      0.82      0.05        98\n",
      "          92       0.21      0.71      0.32        78\n",
      "          93       0.02      1.00      0.03       221\n",
      "          94       0.02      0.70      0.04        80\n",
      "          95       0.00      0.98      0.01        47\n",
      "          96       0.15      1.00      0.26      1864\n",
      "          97       0.05      0.74      0.10       222\n",
      "          98       0.01      0.69      0.01        72\n",
      "          99       0.15      0.34      0.21       170\n",
      "         100       0.43      0.33      0.37       507\n",
      "         101       0.37      0.71      0.49       113\n",
      "         102       0.34      0.79      0.48        90\n",
      "         103       0.28      1.00      0.44      3613\n",
      "         104       0.02      0.82      0.04        78\n",
      "         105       0.03      0.80      0.07       132\n",
      "         106       0.03      0.69      0.06        48\n",
      "         107       0.02      1.00      0.03       219\n",
      "         108       0.05      0.53      0.09        89\n",
      "         109       0.01      1.00      0.02       127\n",
      "         110       0.04      0.54      0.08       245\n",
      "         111       0.03      0.46      0.05       166\n",
      "         112       0.00      1.00      0.01        62\n",
      "         113       0.01      1.00      0.01        76\n",
      "         114       0.01      1.00      0.03       148\n",
      "         115       0.01      1.00      0.01        54\n",
      "         116       0.01      0.94      0.02       103\n",
      "         117       0.01      1.00      0.01        90\n",
      "         118       0.00      1.00      0.01        40\n",
      "         119       0.07      1.00      0.13       887\n",
      "         120       0.05      0.10      0.07        96\n",
      "         121       0.01      0.93      0.02       100\n",
      "         122       0.01      0.95      0.01        80\n",
      "         123       0.60      0.11      0.18        55\n",
      "         124       0.01      0.86      0.02       139\n",
      "         125       0.01      0.69      0.02       162\n",
      "         126       0.04      1.00      0.07       463\n",
      "         127       0.11      0.71      0.19        49\n",
      "         128       0.14      0.00      0.01       222\n",
      "         129       0.57      0.02      0.04       191\n",
      "         130       0.00      1.00      0.01        49\n",
      "         131       0.01      1.00      0.02       126\n",
      "         132       0.46      0.61      0.53        69\n",
      "         133       0.00      1.00      0.01        41\n",
      "         134       0.01      0.90      0.02        42\n",
      "         135       0.02      0.85      0.03       215\n",
      "         136       0.01      0.75      0.02        40\n",
      "         137       0.01      0.82      0.02        34\n",
      "         138       0.01      0.83      0.02        54\n",
      "         139       0.00      1.00      0.01        33\n",
      "         140       0.01      0.95      0.01        66\n",
      "         141       0.00      0.95      0.01        38\n",
      "         142       0.01      1.00      0.01        71\n",
      "         143       0.00      1.00      0.01        49\n",
      "         144       0.20      0.24      0.22       224\n",
      "         145       0.10      0.97      0.18      1134\n",
      "         146       0.00      1.00      0.01        50\n",
      "         147       0.01      1.00      0.02        96\n",
      "         148       0.02      0.78      0.04        81\n",
      "         149       0.06      0.86      0.11       760\n",
      "         150       0.33      0.04      0.08        67\n",
      "         151       0.16      0.29      0.20        59\n",
      "         152       0.05      0.20      0.08        60\n",
      "         153       0.00      1.00      0.01        46\n",
      "         154       0.01      0.67      0.01        70\n",
      "         155       0.03      1.00      0.05       318\n",
      "         156       0.06      0.97      0.11       649\n",
      "         157       0.02      0.51      0.05       145\n",
      "         158       0.18      1.00      0.31      2343\n",
      "         159       0.05      1.00      0.10       698\n",
      "         160       0.01      1.00      0.01        75\n",
      "         161       0.19      0.98      0.31      2011\n",
      "         162       0.08      0.73      0.14      1029\n",
      "         163       0.00      0.58      0.01        45\n",
      "         164       0.03      1.00      0.06       364\n",
      "         165       0.00      0.97      0.01        40\n",
      "         166       0.01      1.00      0.01        88\n",
      "         167       0.09      0.29      0.14        77\n",
      "         168       0.02      0.99      0.03       173\n",
      "         169       0.19      1.00      0.33      2468\n",
      "         170       0.03      1.00      0.06       375\n",
      "         171       0.76      0.55      0.64       126\n",
      "         172       0.53      0.55      0.54        87\n",
      "         173       0.42      0.67      0.51        57\n",
      "         174       0.01      0.57      0.03        76\n",
      "         175       0.00      1.00      0.01        54\n",
      "         176       0.01      1.00      0.01        93\n",
      "         177       0.23      0.17      0.20        58\n",
      "         178       0.00      0.64      0.01        45\n",
      "         179       0.01      0.99      0.02       109\n",
      "         180       0.13      0.32      0.19        69\n",
      "         181       0.05      0.16      0.07        32\n",
      "         182       0.01      0.99      0.02       103\n",
      "         183       0.02      1.00      0.05       293\n",
      "         184       0.02      0.75      0.03        71\n",
      "         185       0.05      0.79      0.10       248\n",
      "         186       0.01      0.51      0.02        39\n",
      "         187       0.01      0.99      0.03       146\n",
      "         188       0.00      1.00      0.01        34\n",
      "         189       0.01      0.59      0.03        69\n",
      "         190       0.11      0.40      0.17        67\n",
      "         191       0.07      1.00      0.14       940\n",
      "         192       0.10      0.03      0.04       423\n",
      "         193       0.00      0.00      0.00       194\n",
      "         194       0.04      0.27      0.07        37\n",
      "         195       0.04      0.20      0.07        51\n",
      "         196       0.69      1.00      0.81      8719\n",
      "         197       0.76      0.90      0.82      7964\n",
      "         198       0.23      0.92      0.37      2331\n",
      "         199       0.76      0.96      0.85      8381\n",
      "         200       0.03      0.78      0.05       321\n",
      "         201       0.68      0.96      0.79      7530\n",
      "         202       0.21      0.98      0.35      2327\n",
      "         203       0.24      0.93      0.38      2343\n",
      "         204       0.01      1.00      0.01        93\n",
      "         205       0.01      0.94      0.02       121\n",
      "         206       0.42      0.12      0.19        42\n",
      "         207       0.53      0.55      0.54        88\n",
      "         208       0.78      0.55      0.65       127\n",
      "         209       0.01      1.00      0.01        64\n",
      "         210       0.02      1.00      0.03       214\n",
      "         211       0.01      1.00      0.01        64\n",
      "         212       0.29      0.04      0.07        52\n",
      "         213       0.02      1.00      0.03       203\n",
      "         214       0.02      0.68      0.03        71\n",
      "         215       0.02      1.00      0.03       191\n",
      "         216       0.01      1.00      0.02       109\n",
      "         217       0.41      0.21      0.28       462\n",
      "         218       0.10      0.34      0.15        67\n",
      "         219       0.00      0.00      0.00        86\n",
      "         220       0.01      1.00      0.03       176\n",
      "         221       0.67      0.10      0.17        80\n",
      "         222       0.00      1.00      0.01        62\n",
      "         223       0.01      1.00      0.01        60\n",
      "         224       0.25      0.03      0.05        34\n",
      "         225       0.01      1.00      0.02       147\n",
      "         226       0.01      1.00      0.01        82\n",
      "         227       0.00      0.67      0.01        45\n",
      "         228       0.01      1.00      0.02       138\n",
      "         229       0.14      0.31      0.19       116\n",
      "         230       0.03      0.12      0.04        56\n",
      "         231       0.03      0.12      0.04        56\n",
      "         232       0.03      0.79      0.05       320\n",
      "         233       0.21      0.97      0.35      2343\n",
      "         234       0.02      0.84      0.05       283\n",
      "         235       0.17      0.06      0.09        35\n",
      "         236       0.32      0.16      0.21       395\n",
      "         237       0.01      0.94      0.01        68\n",
      "         238       0.01      1.00      0.01        75\n",
      "         239       0.00      1.00      0.01        54\n",
      "         240       0.21      0.26      0.23        35\n",
      "         241       0.00      1.00      0.01        33\n",
      "         242       0.23      1.00      0.38      2933\n",
      "         243       0.33      0.04      0.08        68\n",
      "         244       0.01      1.00      0.01        86\n",
      "         245       0.41      0.29      0.34       166\n",
      "         246       0.03      0.81      0.06       122\n",
      "         247       0.57      0.02      0.04       191\n",
      "         248       0.06      0.85      0.11       761\n",
      "         249       0.10      0.24      0.14        45\n",
      "         250       0.04      0.07      0.05        58\n",
      "         251       0.01      1.00      0.02       154\n",
      "         252       0.01      0.49      0.02        71\n",
      "         253       0.12      0.57      0.19       624\n",
      "         254       0.11      0.75      0.20       454\n",
      "         255       0.01      1.00      0.03       183\n",
      "         256       0.01      0.94      0.02       123\n",
      "         257       0.01      1.00      0.03       190\n",
      "         258       0.01      1.00      0.03       183\n",
      "         259       0.04      1.00      0.07       465\n",
      "         260       0.02      1.00      0.03       193\n",
      "         261       0.02      1.00      0.03       205\n",
      "         262       0.01      1.00      0.02       101\n",
      "         263       0.00      1.00      0.01        61\n",
      "         264       0.01      1.00      0.01        70\n",
      "         265       0.02      0.93      0.04       250\n",
      "         266       0.01      0.95      0.02       141\n",
      "         267       0.02      0.67      0.03        72\n",
      "         268       0.00      0.00      0.00        70\n",
      "         269       0.00      1.00      0.01        42\n",
      "         270       0.16      0.26      0.20       144\n",
      "         271       0.09      0.28      0.14        79\n",
      "         272       0.04      1.00      0.07       472\n",
      "         273       0.60      0.22      0.32       285\n",
      "         274       0.02      0.76      0.03       246\n",
      "         275       0.60      0.23      0.33       279\n",
      "         276       0.17      0.46      0.25       153\n",
      "         277       0.02      0.98      0.04       219\n",
      "         278       0.01      0.67      0.01        85\n",
      "         279       0.00      0.96      0.01        46\n",
      "         280       0.99      1.00      1.00     12575\n",
      "         281       0.69      0.09      0.16       927\n",
      "         282       0.00      1.00      0.01        41\n",
      "         283       0.04      1.00      0.09       568\n",
      "         284       0.00      1.00      0.01        46\n",
      "         285       0.04      1.00      0.07       447\n",
      "         286       0.01      1.00      0.02       110\n",
      "         287       0.00      1.00      0.01        51\n",
      "         288       0.07      1.00      0.12       835\n",
      "         289       0.01      1.00      0.03       172\n",
      "         290       0.14      0.14      0.14        44\n",
      "         291       0.14      0.12      0.13        49\n",
      "         292       0.03      0.83      0.05       298\n",
      "         293       0.01      1.00      0.01        92\n",
      "         294       0.01      1.00      0.01        82\n",
      "         295       0.00      1.00      0.01        62\n",
      "         296       0.01      1.00      0.02       101\n",
      "         297       0.03      1.00      0.06       365\n",
      "         298       0.01      1.00      0.03       183\n",
      "         299       0.03      1.00      0.06       390\n",
      "\n",
      "   micro avg       0.06      0.92      0.12    160944\n",
      "   macro avg       0.12      0.74      0.12    160944\n",
      "weighted avg       0.47      0.92      0.53    160944\n",
      " samples avg       0.06      0.93      0.12    160944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert probabilities to binary predictions\n",
    "new_y_pred_binary = (new_y_pred > 0.5).astype(int)\n",
    "print(classification_report(y_test, new_y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {'id': [], 'term': [], 'score': []}\n",
    "for i in range(len(new_y_pred)):\n",
    "    for j in range(len(new_y_pred[i])):\n",
    "        out['id'].append(test_ids[i])\n",
    "        out['term'].append(pred_columns[j])\n",
    "        out['score'].append(new_y_pred[i][j])\n",
    "\n",
    "out_df = pd.DataFrame(out).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>term</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>197999</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:1990904</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197987</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:1902493</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197842</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0030880</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197843</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0030964</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197846</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0031248</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197908</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0043657</td>\n",
       "      <td>0.000278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197872</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0033646</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197907</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0043656</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197940</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0071011</td>\n",
       "      <td>0.000166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197710</th>\n",
       "      <td>O43865</td>\n",
       "      <td>GO:0000428</td>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        term     score\n",
       "197999  O43865  GO:1990904  1.000000\n",
       "197987  O43865  GO:1902493  1.000000\n",
       "197842  O43865  GO:0030880  1.000000\n",
       "197843  O43865  GO:0030964  1.000000\n",
       "197846  O43865  GO:0031248  1.000000\n",
       "...        ...         ...       ...\n",
       "197908  O43865  GO:0043657  0.000278\n",
       "197872  O43865  GO:0033646  0.000263\n",
       "197907  O43865  GO:0043656  0.000263\n",
       "197940  O43865  GO:0071011  0.000166\n",
       "197710  O43865  GO:0000428  0.000024\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = out_df.groupby('id', group_keys=False)\n",
    "out_df = out_df.apply(lambda x: x.sort_values(by='score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "for el in out_df.to_dict(orient='records'):\n",
    "    out.setdefault(el['id'], {})\n",
    "    out[el['id']][el['term']] = el['score']\n",
    "\n",
    "submission = ''\n",
    "for id in test_ids:\n",
    "    for term, score in out[id].items():\n",
    "        submission += f'{id} {term} {score}\\n'\n",
    "        \n",
    "with open('submission.tsv', 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcafaeval\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcafaeval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cafa_eval, write_results\n\u001b[0;32m----> 3\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mcafa_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/taxonomy/go-basic.obo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./submission.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset/test/test_ids.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m write_results(\u001b[38;5;241m*\u001b[39mres)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/cafaeval/evaluation.py:174\u001b[0m, in \u001b[0;36mcafa_eval\u001b[0;34m(obo_file, pred_dir, gt_file, ia, no_orphans, norm, prop, max_terms, th_step, threads)\u001b[0m\n\u001b[1;32m    171\u001b[0m ontologies \u001b[38;5;241m=\u001b[39m obo_parser(obo_file, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_a\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart_of\u001b[39m\u001b[38;5;124m\"\u001b[39m), ia, \u001b[38;5;129;01mnot\u001b[39;00m no_orphans)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Parse ground truth file\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m gt \u001b[38;5;241m=\u001b[39m \u001b[43mgt_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43montologies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Set prediction files looking recursively in the prediction folder\u001b[39;00m\n\u001b[1;32m    177\u001b[0m pred_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(pred_dir) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# add the tailing \"/\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/cafaeval/parser.py:91\u001b[0m, in \u001b[0;36mgt_parser\u001b[0;34m(gt_file, ontologies)\u001b[0m\n\u001b[1;32m     89\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line:\n\u001b[0;32m---> 91\u001b[0m     p_id, term_id \u001b[38;5;241m=\u001b[39m line[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m ontologies:\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m term_id \u001b[38;5;129;01min\u001b[39;00m ontologies[ns]\u001b[38;5;241m.\u001b[39mterms_dict:\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import cafaeval\n",
    "from cafaeval.evaluation import cafa_eval, write_results\n",
    "res = cafa_eval(\"./dataset/taxonomy/go-basic.obo\", \"./submission.txt\", \"./dataset/test/test_ids.txt\")\n",
    "write_results(*res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
